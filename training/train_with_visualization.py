#!/usr/bin/env python3
"""
å¸¦å®æ—¶å¯è§†åŒ–çš„è·¯å¾„è§„åˆ’è®­ç»ƒè„šæœ¬
"""

import sys
import os
import torch
import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Tuple
import time
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.animation import FuncAnimation

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from envs.simple_road_env import SimpleRoadEnv
from agents.simple_ppo import SimpleActorCritic


class VisualizedTrainer:
    """å¸¦å¯è§†åŒ–çš„è®­ç»ƒå™¨"""
    
    def __init__(self, 
                 dem_size: Tuple[int, int] = (100, 100),
                 max_steps: int = 200,
                 num_episodes: int = 100,
                 save_interval: int = 25,
                 visualize_interval: int = 10):
        
        self.dem_size = dem_size
        self.max_steps = max_steps
        self.num_episodes = num_episodes
        self.save_interval = save_interval
        self.visualize_interval = visualize_interval
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = SimpleRoadEnv(dem_size=dem_size, max_steps=max_steps)
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.actor_critic = SimpleActorCritic()
        self.optimizer = torch.optim.Adam(self.actor_critic.parameters(), lr=5e-4)  # é™ä½å­¦ä¹ ç‡
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.success_rates = []
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.models_dir = Path("models")
        self.models_dir.mkdir(exist_ok=True)
        
        self.results_dir = Path("data/results")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®matplotlib
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå¯è§†åŒ–å›¾å½¢
        self.fig, (self.ax1, self.ax2) = plt.subplots(1, 2, figsize=(16, 8))
        self.fig.suptitle('è·¯å¾„è§„åˆ’æ™ºèƒ½ä½“å®æ—¶è®­ç»ƒå¯è§†åŒ–', fontsize=16, fontweight='bold')
        
    def train_episode(self, episode_num: int) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        start_time = time.time()
        
        # é‡ç½®ç¯å¢ƒ
        obs, _ = self.env.reset()
        
        total_reward = 0
        episode_length = 0
        success = False
        
        # å­˜å‚¨episodeæ•°æ®
        log_probs = []
        rewards = []
        path_x = [obs['position'][0]]
        path_y = [obs['position'][1]]
        
        while episode_length < self.max_steps:
            # è½¬æ¢ä¸ºå¼ é‡
            obs_tensor = {
                'position': torch.FloatTensor(obs['position']).unsqueeze(0),
                'goal': torch.FloatTensor(obs['goal']).unsqueeze(0),
                'heading': torch.FloatTensor(obs['heading']).unsqueeze(0),
                'distance_to_goal': torch.FloatTensor(obs['distance_to_goal']).unsqueeze(0),
                'direction_to_goal': torch.FloatTensor(obs['direction_to_goal']).unsqueeze(0),
                'local_dem': torch.FloatTensor(obs['local_dem']).unsqueeze(0)
            }
            
            # è·å–åŠ¨ä½œ
            action, log_prob, value = self.actor_critic.get_action(obs_tensor)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, done, truncated, info = self.env.step(action.numpy().squeeze())
            
            # å­˜å‚¨æ•°æ®
            log_probs.append(log_prob)
            rewards.append(reward)
            path_x.append(next_obs['position'][0])
            path_y.append(next_obs['position'][1])
            
            # ç´¯ç§¯å¥–åŠ±
            total_reward += reward
            episode_length += 1
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            if done and not truncated and info.get('reason') == 'reached_goal':
                success = True
                break
            
            # æ£€æŸ¥æ˜¯å¦å¤±è´¥
            if done:
                break
            
            obs = next_obs
        
        # æ”¹è¿›çš„ç­–ç•¥æ¢¯åº¦æ›´æ–°
        if len(log_probs) > 0:
            # è®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±
            log_probs_tensor = torch.stack(log_probs)
            rewards_tensor = torch.FloatTensor(rewards)
            
            # è®¡ç®—ç´¯ç§¯å¥–åŠ±ï¼ˆä»åå¾€å‰ï¼‰
            cumulative_rewards = []
            running_reward = 0
            for reward in reversed(rewards):
                running_reward = reward + 0.99 * running_reward  # æŠ˜æ‰£å› å­
                cumulative_rewards.insert(0, running_reward)
            
            cumulative_rewards_tensor = torch.FloatTensor(cumulative_rewards)
            
            # æ ‡å‡†åŒ–å¥–åŠ±
            if len(cumulative_rewards) > 1:
                cumulative_rewards_tensor = (cumulative_rewards_tensor - cumulative_rewards_tensor.mean()) / (cumulative_rewards_tensor.std() + 1e-8)
            
            # ç­–ç•¥æ¢¯åº¦æŸå¤±
            loss = -(log_probs_tensor * cumulative_rewards_tensor).mean()
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)
            self.optimizer.step()
        
        # æ›´æ–°ç»Ÿè®¡
        self.episode_rewards.append(total_reward)
        self.episode_lengths.append(episode_length)
        self.success_rates.append(1.0 if success else 0.0)
        
        # è®¡ç®—è¿è¡Œæ—¶é—´
        episode_time = time.time() - start_time
        
        # è®¡ç®—å¹³å‡ç»Ÿè®¡
        avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)
        avg_length = np.mean(self.episode_lengths[-100:]) if len(self.episode_lengths) >= 100 else np.mean(self.episode_lengths)
        success_rate = np.mean(self.success_rates[-100:]) if len(self.success_rates) >= 100 else np.mean(self.success_rates)
        
        # æ‰“å°è¿›åº¦
        print(f"Episode {episode_num:4d} | "
              f"å¥–åŠ±: {total_reward:6.1f} | "
              f"æ­¥æ•°: {episode_length:3d} | "
              f"æˆåŠŸ: {'âœ…' if success else 'âŒ'} | "
              f"å¹³å‡å¥–åŠ±: {avg_reward:6.1f} | "
              f"æˆåŠŸç‡: {success_rate*100:5.1f}% | "
              f"ç”¨æ—¶: {episode_time:.1f}s")
        
        # å¯è§†åŒ–ï¼ˆå¦‚æœåˆ°äº†å¯è§†åŒ–é—´éš”ï¼‰
        if episode_num % self.visualize_interval == 0:
            self.visualize_episode(episode_num, path_x, path_y, success)
        
        return {
            'episode': episode_num,
            'total_reward': total_reward,
            'episode_length': episode_length,
            'success': success,
            'avg_reward': avg_reward,
            'success_rate': success_rate,
            'time': episode_time,
            'path_x': path_x,
            'path_y': path_y
        }
    
    def visualize_episode(self, episode_num: int, path_x: List[float], path_y: List[float], success: bool):
        """å¯è§†åŒ–å½“å‰episode"""
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)
        success_rate = np.mean(self.success_rates[-100:]) if len(self.success_rates) >= 100 else np.mean(self.success_rates)
        
        # æ¸…ç©ºå›¾å½¢
        self.ax1.clear()
        self.ax2.clear()
        
        # å·¦å›¾ï¼šåœ°å½¢å’Œè·¯å¾„
        self.ax1.imshow(self.env.dem, cmap='terrain', origin='lower', alpha=0.7)
        
        # ç»˜åˆ¶ç†æƒ³è·¯å¾„ï¼ˆç›´çº¿ï¼‰
        ideal_x = [self.env.start_pos[0], self.env.goal_pos[0]]
        ideal_y = [self.env.start_pos[1], self.env.goal_pos[1]]
        self.ax1.plot(ideal_x, ideal_y, 'r--', linewidth=2, alpha=0.6, label='ç†æƒ³è·¯å¾„')
        
        # ç»˜åˆ¶æ™ºèƒ½ä½“è·¯å¾„
        self.ax1.plot(path_x, path_y, 'b-', linewidth=3, alpha=0.8, label='æ™ºèƒ½ä½“è·¯å¾„')
        self.ax1.plot(path_x[0], path_y[0], 'go', markersize=15, label='èµ·ç‚¹', markeredgecolor='black', markeredgewidth=2)
        self.ax1.plot(self.env.goal_pos[0], self.env.goal_pos[1], 'ro', markersize=15, label='ç»ˆç‚¹', markeredgecolor='black', markeredgewidth=2)
        self.ax1.plot(path_x[-1], path_y[-1], 'bo', markersize=10, label='å½“å‰ä½ç½®')
        
        self.ax1.set_title(f'Episode {episode_num} - è·¯å¾„è§„åˆ’', fontsize=14, fontweight='bold')
        self.ax1.set_xlabel('Xåæ ‡', fontsize=12)
        self.ax1.set_ylabel('Yåæ ‡', fontsize=12)
        self.ax1.grid(True, alpha=0.3)
        self.ax1.legend()
        
        # å³å›¾ï¼šè®­ç»ƒç»Ÿè®¡
        if len(self.episode_rewards) > 1:
            # å¥–åŠ±æ›²çº¿
            window = min(50, len(self.episode_rewards))
            recent_rewards = self.episode_rewards[-window:]
            recent_episodes = list(range(len(self.episode_rewards) - window + 1, len(self.episode_rewards) + 1))
            
            self.ax2.plot(recent_episodes, recent_rewards, 'b-', alpha=0.7, label='Episodeå¥–åŠ±')
            self.ax2.axhline(y=avg_reward, color='r', linestyle='--', alpha=0.8, label=f'å¹³å‡å¥–åŠ±: {avg_reward:.1f}')
            
            self.ax2.set_title('è®­ç»ƒè¿›åº¦', fontsize=14, fontweight='bold')
            self.ax2.set_xlabel('Episode', fontsize=12)
            self.ax2.set_ylabel('å¥–åŠ±', fontsize=12)
            self.ax2.grid(True, alpha=0.3)
            self.ax2.legend()
            
            # æ·»åŠ æˆåŠŸç‡ä¿¡æ¯
            success_text = f"æˆåŠŸç‡: {success_rate*100:.1f}%"
            self.ax2.text(0.02, 0.98, success_text, transform=self.ax2.transAxes, 
                         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        # æ›´æ–°æ˜¾ç¤º
        plt.tight_layout()
        plt.pause(0.1)  # çŸ­æš‚æš‚åœä»¥æ˜¾ç¤ºåŠ¨ç”»æ•ˆæœ
    
    def save_model(self, episode_num: int):
        """ä¿å­˜æ¨¡å‹"""
        model_path = self.models_dir / f"visualized_road_{episode_num}.pth"
        torch.save({
            'episode': episode_num,
            'actor_critic_state_dict': self.actor_critic.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'episode_rewards': self.episode_rewards,
            'success_rates': self.success_rates
        }, model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    def save_results(self):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        results = {
            'training_config': {
                'dem_size': self.dem_size,
                'max_steps': self.max_steps,
                'num_episodes': self.num_episodes
            },
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'success_rates': self.success_rates
        }
        
        results_path = self.results_dir / f"visualized_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"ğŸ“Š è®­ç»ƒç»“æœå·²ä¿å­˜: {results_path}")
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹å¸¦å¯è§†åŒ–çš„è·¯å¾„è§„åˆ’è®­ç»ƒ")
        print(f"ğŸ“ DEMå°ºå¯¸: {self.dem_size}")
        print(f"â±ï¸ æœ€å¤§æ­¥æ•°: {self.max_steps}")
        print(f"ğŸ¯ ç›®æ ‡episodes: {self.num_episodes}")
        print(f"ğŸ“º å¯è§†åŒ–é—´éš”: æ¯{self.visualize_interval}ä¸ªepisodes")
        print("=" * 80)
        
        start_time = time.time()
        
        for episode in range(1, self.num_episodes + 1):
            # è®­ç»ƒä¸€ä¸ªepisode
            episode_info = self.train_episode(episode)
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if episode % self.save_interval == 0:
                self.save_model(episode)
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        
        print("=" * 80)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸ æ€»ç”¨æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
        print(f"ğŸ“ˆ æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(self.episode_rewards[-100:]):.2f}")
        print(f"ğŸ¯ æœ€ç»ˆæˆåŠŸç‡: {np.mean(self.success_rates[-100:])*100:.1f}%")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œç»“æœ
        self.save_model(self.num_episodes)
        self.save_results()
        
        # æ˜¾ç¤ºæœ€ç»ˆå¯è§†åŒ–
        plt.show()
        
        print("ğŸ’¾ æœ€ç»ˆæ¨¡å‹å’Œç»“æœå·²ä¿å­˜ï¼")


if __name__ == "__main__":
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = VisualizedTrainer(
        dem_size=(100, 100),
        max_steps=200,
        num_episodes=200,  # å¢åŠ åˆ°200ä¸ªepisodes
        save_interval=50,
        visualize_interval=10  # æ¯10ä¸ªepisodeså¯è§†åŒ–ä¸€æ¬¡
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
