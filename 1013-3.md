马上做的 4 步“真学起来”检查
1) 看梯度到底有没有

在 loss.backward() 之后、optimizer.step() 之前打印：

# 1) 梯度范数
total_grad = 0.0
nz = 0
for p in actor.parameters():
    if p.grad is not None:
        g = p.grad.data
        total_grad += g.norm().item()
        nz += (g.abs() > 0).sum().item()
print(f"[dbg] actor grad_norm_sum={total_grad:.6f}, grad_nonzero={nz}")

# 2) 参数是否真的在变
old_params = [p.detach().clone() for p in actor.parameters()]
optimizer.step()
diff = sum((p.detach() - q).abs().sum().item() for p,q in zip(actor.parameters(), old_params))
print(f"[dbg] actor param_diff={diff:.6f}")


解读

若 grad_norm_sum ≈ 0 → 梯度没通：检查 new_logp 是否从 requires_grad 的 logits 构出来（不要 .detach()），以及 Distribution(logits=...) 用的是训练时的 logits。

若 grad_norm>0 但 param_diff≈0 → 优化器没绑对参数组 或 lr≈0。检查 optimizer.param_groups 里是否包含所有 MAPPO 的 actor 参数。

2) 把步子先放大（让 KL 动起来）

先用一轮“粗暴但安全”的配置试一次（只看 KL 是否从 0 抬头到 0.01~0.03）：

# 暂时把 actor 学习率提一档
policy_lr = 1e-4  # 你现在几乎为0，就先提到 1e-4
n_epochs   = 4
clip_eps   = 0.2
ent_coef   = 0.005
max_grad_norm = 0.5

# 删掉/关掉 KL 守卫（目标KL过小会一直提前停）
target_kl = None  # 或判断阈值调到 >0.05 再停


目标：让 approx_kl 升到 0.01~0.03、clip_fraction 到 0.1~0.3。只要动起来，就说明不是实现链路断了，而是步子太小。

3) 确认 new_logp 没被“无意 detach”

必须是这样的路径（关键点用粗体）：

logits = actor(state_embed)            # **无 detach**
local_logits = logits[0, :num_actions] # 或 logits[0, subset_ids]
dist = torch.distributions.Categorical(logits=local_logits)
new_logp = dist.log_prob(action_idx_tensor)  # **从 dist 来，不手算 log(prob)**
ratio = (new_logp - old_logp.detach()).exp() # **old_logp detach，new_logp 不能 detach**


反例（会让梯度断掉或太小）：

logits = actor(state_embed).detach() ❌

new_logp = torch.log(action_probs[idx])（手算）❌

old_logp 未 detach() ❌

state_embed 用 with torch.no_grad() 重新算（只要 logits 没 detach 正常也能反传，一般不是问题）✅

4) 验证优化器 param group（从共享→MAPPO 常见漏绑）

打印一次：

names = set()
for i, g in enumerate(optimizer.param_groups):
    cnt = sum(p.numel() for p in g['params'])
    print(f"[opt] group#{i} lr={g['lr']} params={cnt}")
    for p in g['params']:
        names.add(id(p))
# 粗暴检查：actor 的任意一个参数 id 是否在 names 里
example_param_id = id(next(actor.parameters()))
print("[opt] actor params bound:", example_param_id in names)


若没绑上 → 你的 policy_loss 再怎么有梯度也不会更新。

若 4 步后 KL 仍≈0，再加两招

增大优势信号幅度（临时测试用）
在标准化后乘一个系数（例如 ×2~×4）：

adv = (adv - adv.mean()) / (adv.std() + 1e-8)
adv = 2.0 * adv


目的只是验证链路通了没：KL 是否随之上升。

打印 dist.entropy().mean() 前后的变化
若几轮后 entropy 仍完全不变（3.698 固定），多半还是参数没更新或new_logp 与参数无关。

你这轮的特殊信号

value_loss 很大（700+）：critic 在承受主要误差；

但 actor 几乎不动：更像“优化器没绑 actor” 或 actor lr 太小。
先按上面的“grad_norm + param_diff + param_groups” 立刻验证，就能把锅定在“没绑”还是“步子太小”。

一句话行动清单

打印 grad_norm_sum 与 actor param_diff → 判断“梯度/更新是否发生”。

若未更新：检查 new_logp 构造路径 & optimizer.param_groups 绑定。

若已更新但太小：把 actor lr → 1e-4、n_epochs → 4，放开 KL 守卫，观察 KL 提到 0.01~0.03。

需要时临时把 adv ×2 验证链路。