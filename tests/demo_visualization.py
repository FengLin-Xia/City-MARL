#!/usr/bin/env python3
"""
åœ°å½¢é“è·¯å¯»è·¯å¯è§†åŒ–æ¼”ç¤º
å±•ç¤ºå®æ—¶å¯»è·¯è¿‡ç¨‹çš„å¯è§†åŒ–æ•ˆæœ
"""

import numpy as np
import time
from envs.terrain_road_env import TerrainRoadEnvironment, RoadAction

def random_agent_policy(obs):
    """éšæœºæ™ºèƒ½ä½“ç­–ç•¥ - ç”¨äºæ¼”ç¤º"""
    return np.random.randint(0, 7)

def simple_pathfinding_policy(obs):
    """ç®€å•å¯»è·¯ç­–ç•¥ - æœç›®æ ‡æ–¹å‘ç§»åŠ¨"""
    agent_pos = obs['agent_pos']
    target_pos = obs['target_pos']
    
    # è®¡ç®—æ–¹å‘
    direction = target_pos - agent_pos
    
    # é€‰æ‹©ç§»åŠ¨æ–¹å‘
    if abs(direction[0]) > abs(direction[1]):
        # å‚ç›´ç§»åŠ¨
        if direction[0] > 0:
            return RoadAction.MOVE_SOUTH.value
        else:
            return RoadAction.MOVE_NORTH.value
    else:
        # æ°´å¹³ç§»åŠ¨
        if direction[1] > 0:
            return RoadAction.MOVE_EAST.value
        else:
            return RoadAction.MOVE_WEST.value

def smart_pathfinding_policy(obs):
    """æ™ºèƒ½å¯»è·¯ç­–ç•¥ - è€ƒè™‘åœ°å½¢å’Œé“è·¯"""
    agent_pos = obs['agent_pos']
    target_pos = obs['target_pos']
    terrain_map = obs['terrain_map']
    road_map = obs['road_map']
    resources = obs['resources']
    
    # å¦‚æœèµ„æºå……è¶³ä¸”å½“å‰ä½ç½®æ²¡æœ‰é“è·¯ï¼Œè€ƒè™‘å»ºè®¾é“è·¯
    if resources[0] >= 10 and road_map[agent_pos[0], agent_pos[1]] == 0:
        # æ£€æŸ¥å‘¨å›´æ˜¯å¦æœ‰é“è·¯
        for dx, dy in [(-1,0), (1,0), (0,1), (0,-1)]:
            nx, ny = agent_pos[0] + dx, agent_pos[1] + dy
            if (0 <= nx < terrain_map.shape[0] and 
                0 <= ny < terrain_map.shape[1] and 
                road_map[nx, ny] > 0):
                return RoadAction.BUILD_ROAD.value
    
    # ä¼˜å…ˆåœ¨æœ‰é“è·¯çš„åœ°æ–¹ç§»åŠ¨
    best_action = None
    best_score = float('-inf')
    
    for action in [RoadAction.MOVE_NORTH.value, RoadAction.MOVE_SOUTH.value, 
                   RoadAction.MOVE_EAST.value, RoadAction.MOVE_WEST.value]:
        
        if action == RoadAction.MOVE_NORTH.value:
            new_pos = agent_pos + np.array([-1, 0])
        elif action == RoadAction.MOVE_SOUTH.value:
            new_pos = agent_pos + np.array([1, 0])
        elif action == RoadAction.MOVE_EAST.value:
            new_pos = agent_pos + np.array([0, 1])
        else:  # MOVE_WEST
            new_pos = agent_pos + np.array([0, -1])
        
        # æ£€æŸ¥è¾¹ç•Œ
        if (0 <= new_pos[0] < terrain_map.shape[0] and 
            0 <= new_pos[1] < terrain_map.shape[1]):
            
            # è®¡ç®—åˆ†æ•°
            score = 0
            
            # è·ç¦»ç›®æ ‡çš„å¥–åŠ±
            distance_to_target = np.linalg.norm(new_pos - target_pos)
            score -= distance_to_target * 2
            
            # é“è·¯å¥–åŠ±
            if road_map[new_pos[0], new_pos[1]] > 0:
                score += 10
            
            # åœ°å½¢æƒ©ç½š
            terrain_type = terrain_map[new_pos[0], new_pos[1]]
            if terrain_type == 0:  # æ°´åŸŸ
                score -= 100
            elif terrain_type == 2:  # æ£®æ—
                score -= 5
            elif terrain_type == 3:  # å±±åœ°
                score -= 10
            
            if score > best_score:
                best_score = score
                best_action = action
    
    if best_action is not None:
        return best_action
    
    # å¦‚æœæ‰¾ä¸åˆ°å¥½çš„ç§»åŠ¨æ–¹å‘ï¼Œéšæœºç§»åŠ¨
    return np.random.randint(0, 4)

def demo_visualization():
    """æ¼”ç¤ºå¯è§†åŒ–æ•ˆæœ"""
    print("ğŸ® åœ°å½¢é“è·¯å¯»è·¯å¯è§†åŒ–æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç¯å¢ƒ
    env = TerrainRoadEnvironment(
        grid_size=(30, 30),  # è¾ƒå°çš„ç½‘æ ¼ä¾¿äºè§‚å¯Ÿ
        max_steps=200,
        render_mode='human'  # å¯ç”¨å¯è§†åŒ–
    )
    
    print("âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"ğŸ“Š ç½‘æ ¼å¤§å°: {env.grid_size}")
    print(f"ğŸ¯ ç›®æ ‡ä½ç½®: {env.target_pos}")
    print(f"ğŸ¤– æ™ºèƒ½ä½“ä½ç½®: {env.agent_pos}")
    print()
    
    # é€‰æ‹©ç­–ç•¥
    strategies = {
        '1': ('éšæœºç­–ç•¥', random_agent_policy),
        '2': ('ç®€å•å¯»è·¯', simple_pathfinding_policy),
        '3': ('æ™ºèƒ½å¯»è·¯', smart_pathfinding_policy)
    }
    
    print("è¯·é€‰æ‹©æ™ºèƒ½ä½“ç­–ç•¥:")
    for key, (name, _) in strategies.items():
        print(f"  {key}. {name}")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
    if choice not in strategies:
        choice = '1'
        print("ä½¿ç”¨é»˜è®¤ç­–ç•¥: éšæœºç­–ç•¥")
    
    strategy_name, policy = strategies[choice]
    print(f"ğŸ¯ ä½¿ç”¨ç­–ç•¥: {strategy_name}")
    print()
    
    # è¿è¡Œæ¼”ç¤º
    print("ğŸš€ å¼€å§‹æ¼”ç¤º...")
    print("ğŸ’¡ æç¤º: å…³é—­å›¾å½¢çª—å£å¯ä»¥åœæ­¢æ¼”ç¤º")
    
    obs, _ = env.reset()
    total_reward = 0
    step_count = 0
    
    try:
        while True:
            # é€‰æ‹©åŠ¨ä½œ
            action = policy(obs)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, truncated, info = env.step(action)
            total_reward += reward
            step_count += 1
            
            # æ‰“å°ä¿¡æ¯
            if step_count % 10 == 0:
                distance = np.linalg.norm(obs['agent_pos'] - obs['target_pos'])
                print(f"æ­¥éª¤ {step_count}: å¥–åŠ±={reward:.2f}, æ€»å¥–åŠ±={total_reward:.2f}, "
                      f"è·ç¦»ç›®æ ‡={distance:.1f}, èµ„æº={obs['resources']}")
            
            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            if done or truncated:
                break
            
            # æ§åˆ¶é€Ÿåº¦
            time.sleep(0.2)  # æ¯æ­¥æš‚åœ0.2ç§’
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ“Š æ¼”ç¤ºç»“æœ:")
    print(f"  æ€»æ­¥æ•°: {step_count}")
    print(f"  æ€»å¥–åŠ±: {total_reward:.2f}")
    print(f"  å¹³å‡å¥–åŠ±: {total_reward/step_count:.2f}" if step_count > 0 else "  å¹³å‡å¥–åŠ±: 0.00")
    print(f"  æœ€ç»ˆä½ç½®: {obs['agent_pos']}")
    print(f"  ç›®æ ‡ä½ç½®: {obs['target_pos']}")
    print(f"  æ˜¯å¦åˆ°è¾¾ç›®æ ‡: {np.array_equal(obs['agent_pos'], obs['target_pos'])}")
    
    # å…³é—­ç¯å¢ƒ
    env.close()
    print("âœ… æ¼”ç¤ºå®Œæˆ!")

def demo_multiple_episodes():
    """æ¼”ç¤ºå¤šä¸ªepisodeçš„æ•ˆæœ"""
    print("ğŸ® å¤šè½®æ¼”ç¤º - è§‚å¯Ÿä¸åŒç­–ç•¥çš„æ•ˆæœ")
    print("=" * 50)
    
    strategies = [
        ('éšæœºç­–ç•¥', random_agent_policy),
        ('ç®€å•å¯»è·¯', simple_pathfinding_policy),
        ('æ™ºèƒ½å¯»è·¯', smart_pathfinding_policy)
    ]
    
    results = {}
    
    for strategy_name, policy in strategies:
        print(f"\nğŸ¯ æµ‹è¯•ç­–ç•¥: {strategy_name}")
        
        env = TerrainRoadEnvironment(
            grid_size=(20, 20),
            max_steps=100,
            render_mode='human'
        )
        
        episode_rewards = []
        success_count = 0
        
        for episode in range(3):  # æ¯ä¸ªç­–ç•¥æµ‹è¯•3ä¸ªepisode
            obs, _ = env.reset()
            total_reward = 0
            step_count = 0
            
            print(f"  Episode {episode + 1}: ", end="")
            
            while step_count < env.max_steps:
                action = policy(obs)
                obs, reward, done, truncated, info = env.step(action)
                total_reward += reward
                step_count += 1
                
                if done or truncated:
                    break
                
                time.sleep(0.1)
            
            episode_rewards.append(total_reward)
            if np.array_equal(obs['agent_pos'], obs['target_pos']):
                success_count += 1
            
            print(f"å¥–åŠ±={total_reward:.1f}, æ­¥æ•°={step_count}")
        
        env.close()
        
        results[strategy_name] = {
            'avg_reward': np.mean(episode_rewards),
            'success_rate': success_count / 3,
            'rewards': episode_rewards
        }
    
    # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ“Š ç­–ç•¥æ¯”è¾ƒç»“æœ:")
    for strategy_name, result in results.items():
        print(f"\n{strategy_name}:")
        print(f"  å¹³å‡å¥–åŠ±: {result['avg_reward']:.2f}")
        print(f"  æˆåŠŸç‡: {result['success_rate']:.1%}")
        print(f"  å„è½®å¥–åŠ±: {[f'{r:.1f}' for r in result['rewards']]}")

if __name__ == "__main__":
    print("ğŸ® åœ°å½¢é“è·¯å¯»è·¯å¯è§†åŒ–æ¼”ç¤º")
    print("=" * 50)
    print("é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
    print("  1. å•è½®è¯¦ç»†æ¼”ç¤º")
    print("  2. å¤šè½®ç­–ç•¥æ¯”è¾ƒ")
    
    mode = input("è¯·é€‰æ‹©æ¨¡å¼ (1-2): ").strip()
    
    if mode == '2':
        demo_multiple_episodes()
    else:
        demo_visualization()
