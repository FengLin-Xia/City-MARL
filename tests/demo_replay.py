#!/usr/bin/env python3
"""
åœ°å½¢é“è·¯å¯»è·¯å›æ”¾æ¼”ç¤º
å±•ç¤ºè®­ç»ƒåçš„episodeå›æ”¾åŠŸèƒ½
"""

import numpy as np
import time
import os
from envs.terrain_road_env import TerrainRoadEnvironment, RoadAction

def smart_pathfinding_policy(obs):
    """æ™ºèƒ½å¯»è·¯ç­–ç•¥ - è€ƒè™‘åœ°å½¢å’Œé“è·¯"""
    agent_pos = obs['agent_pos']
    target_pos = obs['target_pos']
    terrain_map = obs['terrain_map']
    road_map = obs['road_map']
    resources = obs['resources']
    
    # å¦‚æœèµ„æºå……è¶³ä¸”å½“å‰ä½ç½®æ²¡æœ‰é“è·¯ï¼Œè€ƒè™‘å»ºè®¾é“è·¯
    if resources[0] >= 10 and road_map[agent_pos[0], agent_pos[1]] == 0:
        # æ£€æŸ¥å‘¨å›´æ˜¯å¦æœ‰é“è·¯
        for dx, dy in [(-1,0), (1,0), (0,1), (0,-1)]:
            nx, ny = agent_pos[0] + dx, agent_pos[1] + dy
            if (0 <= nx < terrain_map.shape[0] and 
                0 <= ny < terrain_map.shape[1] and 
                road_map[nx, ny] > 0):
                return RoadAction.BUILD_ROAD.value
    
    # ä¼˜å…ˆåœ¨æœ‰é“è·¯çš„åœ°æ–¹ç§»åŠ¨
    best_action = None
    best_score = float('-inf')
    
    for action in [RoadAction.MOVE_NORTH.value, RoadAction.MOVE_SOUTH.value, 
                   RoadAction.MOVE_EAST.value, RoadAction.MOVE_WEST.value]:
        
        if action == RoadAction.MOVE_NORTH.value:
            new_pos = agent_pos + np.array([-1, 0])
        elif action == RoadAction.MOVE_SOUTH.value:
            new_pos = agent_pos + np.array([1, 0])
        elif action == RoadAction.MOVE_EAST.value:
            new_pos = agent_pos + np.array([0, 1])
        else:  # MOVE_WEST
            new_pos = agent_pos + np.array([0, -1])
        
        # æ£€æŸ¥è¾¹ç•Œ
        if (0 <= new_pos[0] < terrain_map.shape[0] and 
            0 <= new_pos[1] < terrain_map.shape[1]):
            
            # è®¡ç®—åˆ†æ•°
            score = 0
            
            # è·ç¦»ç›®æ ‡çš„å¥–åŠ±
            distance_to_target = np.linalg.norm(new_pos - target_pos)
            score -= distance_to_target * 2
            
            # é“è·¯å¥–åŠ±
            if road_map[new_pos[0], new_pos[1]] > 0:
                score += 10
            
            # åœ°å½¢æƒ©ç½š
            terrain_type = terrain_map[new_pos[0], new_pos[1]]
            if terrain_type == 0:  # æ°´åŸŸ
                score -= 100
            elif terrain_type == 2:  # æ£®æ—
                score -= 5
            elif terrain_type == 3:  # å±±åœ°
                score -= 10
            
            if score > best_score:
                best_score = score
                best_action = action
    
    if best_action is not None:
        return best_action
    
    # å¦‚æœæ‰¾ä¸åˆ°å¥½çš„ç§»åŠ¨æ–¹å‘ï¼Œéšæœºç§»åŠ¨
    return np.random.randint(0, 4)

def record_episode():
    """è®°å½•ä¸€ä¸ªepisode"""
    print("ğŸ¬ å¼€å§‹è®°å½•episode...")
    
    # åˆ›å»ºç¯å¢ƒï¼ˆå…³é—­å®æ—¶æ¸²æŸ“ä»¥æé«˜æ€§èƒ½ï¼‰
    env = TerrainRoadEnvironment(
        grid_size=(25, 25),
        max_steps=150,
        render_mode=None  # å…³é—­å®æ—¶æ¸²æŸ“
    )
    
    # å¼€å§‹è®°å½•
    env.start_recording()
    
    # è¿è¡Œepisode
    obs, _ = env.reset()
    total_reward = 0
    step_count = 0
    
    while step_count < env.max_steps:
        action = smart_pathfinding_policy(obs)
        obs, reward, done, truncated, info = env.step(action)
        total_reward += reward
        step_count += 1
        
        if done or truncated:
            break
    
    # åœæ­¢è®°å½•
    env.stop_recording()
    
    print(f"ğŸ“Š Episodeå®Œæˆ:")
    print(f"  æ€»æ­¥æ•°: {step_count}")
    print(f"  æ€»å¥–åŠ±: {total_reward:.2f}")
    print(f"  æ˜¯å¦åˆ°è¾¾ç›®æ ‡: {np.array_equal(obs['agent_pos'], obs['target_pos'])}")
    
    return env

def demo_replay():
    """æ¼”ç¤ºå›æ”¾åŠŸèƒ½"""
    print("ğŸ® åœ°å½¢é“è·¯å¯»è·¯å›æ”¾æ¼”ç¤º")
    print("=" * 50)
    
    # ç›´æ¥åŠ è½½ç°æœ‰çš„episodeæ–‡ä»¶
    episode_file = "episode_1755203784.json"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(episode_file):
        print(f"ğŸ“‚ åŠ è½½episodeæ–‡ä»¶: {episode_file}")
        env = TerrainRoadEnvironment()
        env.load_episode(episode_file)
    else:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {episode_file}")
        print("ğŸ”„ å°è¯•å½•åˆ¶æ–°çš„episode...")
        env = record_episode()
    
    # è¯¢é—®æ˜¯å¦ä¿å­˜
    save_choice = input("æ˜¯å¦ä¿å­˜è¿™ä¸ªepisode? (y/n): ").strip().lower()
    if save_choice == 'y':
        env.save_episode()
    
    # å¼€å§‹å›æ”¾
    print("\nğŸ¬ å¼€å§‹å›æ”¾...")
    print("ğŸ’¡ æ§åˆ¶è¯´æ˜:")
    print("  - ä½¿ç”¨æ»‘å—è°ƒæ•´æ’­æ”¾é€Ÿåº¦")
    print("  - ç‚¹å‡»æŒ‰é’®æ§åˆ¶æ’­æ”¾/æš‚åœ/é‡ç½®")
    print("  - ç‚¹å‡»Saveä¿å­˜å½“å‰å¸§")
    print("  - æŒ‰Ctrl+Cé€€å‡ºå›æ”¾")
    
    env.replay_episode(speed=1.0)
    
    env.close()

def demo_multiple_episodes():
    """æ¼”ç¤ºå¤šä¸ªepisodeçš„å¯¹æ¯”"""
    print("ğŸ® å¤šepisodeå¯¹æ¯”æ¼”ç¤º")
    print("=" * 50)
    
    episodes = []
    
    # å½•åˆ¶å¤šä¸ªepisode
    for i in range(3):
        print(f"\nğŸ“¹ å½•åˆ¶ç¬¬ {i+1} ä¸ªepisode...")
        env = record_episode()
        env.save_episode(f"episode_{i+1}.json")
        episodes.append(env)
    
    # æ˜¾ç¤ºå¯¹æ¯”ä¿¡æ¯
    print("\nğŸ“Š Episodeå¯¹æ¯”:")
    for i, env in enumerate(episodes):
        metadata = env.episode_history[-1] if env.episode_history else {}
        print(f"Episode {i+1}:")
        print(f"  æ­¥æ•°: {len(env.episode_history)}")
        print(f"  æœ€ç»ˆå¥–åŠ±: {metadata.get('reward', 0):.2f}")
        print(f"  æ˜¯å¦æˆåŠŸ: {metadata.get('done', False)}")
    
    # é€‰æ‹©è¦å›æ”¾çš„episode
    choice = input("\nè¯·é€‰æ‹©è¦å›æ”¾çš„episode (1-3): ").strip()
    if choice.isdigit() and 1 <= int(choice) <= 3:
        env = episodes[int(choice) - 1]
        env.replay_episode(speed=1.0)
    
    # æ¸…ç†
    for env in episodes:
        env.close()

def demo_analysis():
    """æ¼”ç¤ºepisodeåˆ†æåŠŸèƒ½"""
    print("ğŸ“Š Episodeåˆ†ææ¼”ç¤º")
    print("=" * 50)
    
    # å½•åˆ¶ä¸€ä¸ªepisode
    env = record_episode()
    
    if not env.episode_history:
        print("âŒ æ²¡æœ‰episodeæ•°æ®å¯åˆ†æ")
        return
    
    # åˆ†ææ•°æ®
    steps = [frame['step'] for frame in env.episode_history]
    rewards = [frame['reward'] for frame in env.episode_history]
    distances = []
    road_builds = 0
    
    for frame in env.episode_history:
        # è®¡ç®—åˆ°ç›®æ ‡çš„è·ç¦»
        agent_pos = frame['agent_pos']
        distance = np.linalg.norm(agent_pos - env.target_pos)
        distances.append(distance)
        
        # ç»Ÿè®¡é“è·¯å»ºè®¾
        if frame['action'] == RoadAction.BUILD_ROAD.value:
            road_builds += 1
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    print("\nğŸ“ˆ Episodeåˆ†æç»“æœ:")
    print(f"  æ€»æ­¥æ•°: {len(steps)}")
    print(f"  æ€»å¥–åŠ±: {sum(rewards):.2f}")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(rewards):.2f}")
    print(f"  æœ€å¤§å¥–åŠ±: {max(rewards):.2f}")
    print(f"  æœ€å°å¥–åŠ±: {min(rewards):.2f}")
    print(f"  é“è·¯å»ºè®¾æ¬¡æ•°: {road_builds}")
    print(f"  æœ€ç»ˆè·ç¦»ç›®æ ‡: {distances[-1]:.2f}")
    print(f"  æ˜¯å¦åˆ°è¾¾ç›®æ ‡: {env.episode_history[-1]['done']}")
    
    # ç»˜åˆ¶åˆ†æå›¾è¡¨
    import matplotlib.pyplot as plt
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('Episode Analysis', fontsize=16)
    
    # å¥–åŠ±æ›²çº¿
    ax1.plot(steps, rewards, 'b-', linewidth=2)
    ax1.set_title('Reward over Time')
    ax1.set_xlabel('Step')
    ax1.set_ylabel('Reward')
    ax1.grid(True, alpha=0.3)
    
    # è·ç¦»æ›²çº¿
    ax2.plot(steps, distances, 'r-', linewidth=2)
    ax2.set_title('Distance to Target')
    ax2.set_xlabel('Step')
    ax2.set_ylabel('Distance')
    ax2.grid(True, alpha=0.3)
    
    # ç´¯ç§¯å¥–åŠ±
    cumulative_rewards = np.cumsum(rewards)
    ax3.plot(steps, cumulative_rewards, 'g-', linewidth=2)
    ax3.set_title('Cumulative Reward')
    ax3.set_xlabel('Step')
    ax3.set_ylabel('Cumulative Reward')
    ax3.grid(True, alpha=0.3)
    
    # åŠ¨ä½œåˆ†å¸ƒ
    actions = [frame['action'] for frame in env.episode_history]
    action_names = ['North', 'South', 'East', 'West', 'Build', 'Upgrade', 'Wait']
    action_counts = [actions.count(i) for i in range(7)]
    
    ax4.bar(action_names, action_counts, color='orange', alpha=0.7)
    ax4.set_title('Action Distribution')
    ax4.set_xlabel('Action')
    ax4.set_ylabel('Count')
    ax4.tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    env.close()

if __name__ == "__main__":
    print("ğŸ® åœ°å½¢é“è·¯å¯»è·¯å›æ”¾æ¼”ç¤º")
    print("=" * 50)
    print("é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
    print("  1. å•episodeå›æ”¾")
    print("  2. å¤šepisodeå¯¹æ¯”")
    print("  3. Episodeåˆ†æ")
    
    mode = input("è¯·é€‰æ‹©æ¨¡å¼ (1-3): ").strip()
    
    if mode == '2':
        demo_multiple_episodes()
    elif mode == '3':
        demo_analysis()
    else:
        demo_replay()
