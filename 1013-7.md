按优先级给你一套最短动作（做 1–2 条就该看到 KL 抬头）：

关熵 5 个更新
ent_coef = 0（先让策略敢偏离均匀；等 KL 到 0.01–0.03 再开回 0.001–0.003）。

让 logits 先尖一点（训练期临时）
在算分布前做温度缩放：local_logits /= 0.5（若还不动试 0.25）。

预期：KL 立刻从 ~1e-6 抬到 1e-3~1e-2，clip_fraction > 0。

检查并拉高 logits 的起伏
打印 local_logits.std()；如果仍接近 0，重置最后一层初始化（正交 + 小增益，bias=0），并确保最后一层没有激活/BN/LN/Dropout。

增大有效 batch（小样本会把微小变化“平均成 0”）
把每次 PPO 更新的样本累计到 ≥512（多 rollout 再更新，或梯度累积 4–8 次）。

让 KL 自调（用“更新后”的 KL 做反馈）
目标 target_kl=0.02：

若 <0.2×target_kl → actor lr ×1.5

若 >2×target_kl → actor lr ×0.5

如果还是 0，做一次“因果性”快测
同一状态枚举几种合法动作跑短回报，若 Δreturn≈0：说明动作几乎不影响回报 → 加一点即时 shaping（如 α*(score - λ*cost + β*rent_gain)）或让预算/规则改变可行动作集（mask），否则策略没有可学信号。

判定点：一旦看到 KL 升到 1e-3~1e-2、clip_fraction 出现非零、entropy 微降，就说明策略真的开始动了。