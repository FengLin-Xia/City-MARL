#!/usr/bin/env python3
"""
PPOç½‘æ ¼å¯¼èˆªæ™ºèƒ½ä½“
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from typing import Tuple, Dict, List
import random


class PPONetwork(nn.Module):
    """PPOç½‘ç»œï¼šActor-Criticæ¶æ„"""
    
    def __init__(self, state_dim: int = 5, action_dim: int = 4, hidden_dim: int = 64):
        super().__init__()
        
        # å…±äº«ç‰¹å¾æå–å™¨
        self.feature_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Actorç½‘ç»œï¼ˆç­–ç•¥ç½‘ç»œï¼‰
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
        # Criticç½‘ç»œï¼ˆä»·å€¼ç½‘ç»œï¼‰
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
    def forward(self, state):
        """å‰å‘ä¼ æ’­"""
        features = self.feature_net(state)
        action_logits = self.actor(features)
        value = self.critic(features)
        return action_logits, value
    
    def get_action_and_value(self, state, action=None):
        """è·å–åŠ¨ä½œå’Œå€¼"""
        action_logits, value = self.forward(state)
        probs = F.softmax(action_logits, dim=-1)
        log_probs = F.log_softmax(action_logits, dim=-1)
        
        if action is None:
            action = torch.multinomial(probs, 1)
        
        action_log_prob = log_probs.gather(1, action)
        entropy = -(probs * log_probs).sum(dim=-1, keepdim=True)
        
        return action, action_log_prob, entropy, value


class PPOGridNavAgent:
    """PPOç½‘æ ¼å¯¼èˆªæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int = 5, action_dim: int = 4, lr: float = 3e-4):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ç½‘ç»œ
        self.network = PPONetwork(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
        
        # PPOå‚æ•°
        self.clip_epsilon = 0.2
        self.value_coef = 0.5
        self.entropy_coef = 0.01
        self.max_grad_norm = 0.5
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.success_rates = []
        
    def get_state_tensor(self, obs):
        """å°†è§‚æµ‹è½¬æ¢ä¸ºçŠ¶æ€å¼ é‡"""
        state = np.array([
            obs['position'][0],  # å½“å‰x
            obs['position'][1],  # å½“å‰y
            obs['goal'][0],      # ç›®æ ‡x
            obs['goal'][1],      # ç›®æ ‡y
            obs['distance_to_goal'][0]  # è·ç¦»
        ], dtype=np.float32)
        return torch.FloatTensor(state).unsqueeze(0).to(self.device)
    
    def get_action(self, obs, training=True):
        """è·å–åŠ¨ä½œ"""
        state = self.get_state_tensor(obs)
        
        with torch.no_grad():
            action, _, _, _ = self.network.get_action_and_value(state)
            return int(action.cpu().numpy()[0])  # ç¡®ä¿è¿”å›æ•´æ•°
    
    def collect_episode(self, env, max_steps=200):
        """æ”¶é›†ä¸€ä¸ªepisodeçš„æ•°æ®"""
        obs, _ = env.reset()
        
        states = []
        actions = []
        rewards = []
        values = []
        log_probs = []
        dones = []
        
        total_reward = 0
        success = False
        
        for step in range(max_steps):
            state = self.get_state_tensor(obs)
            
            # è·å–åŠ¨ä½œ
            action, log_prob, _, value = self.network.get_action_and_value(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, done, truncated, info = env.step(action.item())
            
            # å­˜å‚¨æ•°æ®
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            values.append(value)
            log_probs.append(log_prob)
            dones.append(done or truncated)
            
            total_reward += reward
            
            if done and not truncated and info.get('reason') == 'reached_goal':
                success = True
                break
            elif done or truncated:
                break
                
            obs = next_obs
        
        # è½¬æ¢ä¸ºå¼ é‡å¹¶åˆ†ç¦»è®¡ç®—å›¾
        states = torch.cat(states).detach()
        actions = torch.cat(actions).squeeze().detach()  # ç¡®ä¿æ˜¯1Då¼ é‡
        rewards = torch.FloatTensor(rewards).to(self.device)
        values = torch.cat(values).squeeze().detach()  # ç¡®ä¿æ˜¯1Då¼ é‡
        log_probs = torch.cat(log_probs).squeeze().detach()  # ç¡®ä¿æ˜¯1Då¼ é‡
        dones = torch.FloatTensor(dones).to(self.device)  # è½¬æ¢ä¸ºæµ®ç‚¹å¼ é‡
        
        return {
            'states': states,
            'actions': actions,
            'rewards': rewards,
            'values': values,
            'log_probs': log_probs,
            'dones': dones,
            'total_reward': total_reward,
            'success': success,
            'episode_length': len(rewards)
        }
    
    def compute_returns_and_advantages(self, rewards, values, dones, gamma=0.99, gae_lambda=0.95):
        """è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿å‡½æ•°"""
        returns = []
        advantages = []
        
        # è®¡ç®—GAE
        gae = 0
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[i + 1]
            
            delta = rewards[i] + gamma * next_value * (1 - dones[i]) - values[i]
            gae = delta + gamma * gae_lambda * (1 - dones[i]) * gae
            advantages.insert(0, gae)
            returns.insert(0, gae + values[i])
        
        returns = torch.FloatTensor(returns).to(self.device)
        advantages = torch.FloatTensor(advantages).to(self.device)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        return returns, advantages
    
    def update_policy(self, episode_data, num_epochs=4):
        """æ›´æ–°ç­–ç•¥"""
        states = episode_data['states']
        actions = episode_data['actions']
        old_log_probs = episode_data['log_probs']
        returns, advantages = self.compute_returns_and_advantages(
            episode_data['rewards'], 
            episode_data['values'], 
            episode_data['dones']
        )
        
        # å¤šè½®æ›´æ–°
        for epoch in range(num_epochs):
            # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„åŠ¨ä½œæ¦‚ç‡å’Œå€¼
            action_logits, values = self.network(states)
            probs = F.softmax(action_logits, dim=-1)
            log_probs = F.log_softmax(action_logits, dim=-1)
            action_log_probs = log_probs.gather(1, actions.unsqueeze(1)).squeeze(1)
            entropy = -(probs * log_probs).sum(dim=-1).mean()
            
            # è®¡ç®—æ¯”ç‡
            ratio = torch.exp(action_log_probs - old_log_probs)
            
            # PPOæŸå¤±
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            
            # ä»·å€¼æŸå¤±
            value_loss = F.mse_loss(values.squeeze(), returns)
            
            # æ€»æŸå¤±
            loss = actor_loss + self.value_coef * value_loss - self.entropy_coef * entropy
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)
            self.optimizer.step()
    
    def train_episode(self, env, episode_num):
        """è®­ç»ƒä¸€ä¸ªepisode"""
        # æ”¶é›†æ•°æ®
        episode_data = self.collect_episode(env)
        
        # æ›´æ–°ç­–ç•¥
        self.update_policy(episode_data)
        
        # æ›´æ–°ç»Ÿè®¡
        self.episode_rewards.append(episode_data['total_reward'])
        self.success_rates.append(1.0 if episode_data['success'] else 0.0)
        
        # è®¡ç®—å¹³å‡ç»Ÿè®¡
        avg_reward = np.mean(self.episode_rewards[-50:]) if len(self.episode_rewards) >= 50 else np.mean(self.episode_rewards)
        success_rate = np.mean(self.success_rates[-50:]) if len(self.success_rates) >= 50 else np.mean(self.success_rates)
        
        # æ‰“å°è¿›åº¦
        print(f"Episode {episode_num:4d} | "
              f"å¥–åŠ±: {episode_data['total_reward']:6.1f} | "
              f"æ­¥æ•°: {episode_data['episode_length']:3d} | "
              f"æˆåŠŸ: {'âœ…' if episode_data['success'] else 'âŒ'} | "
              f"å¹³å‡å¥–åŠ±: {avg_reward:6.1f} | "
              f"æˆåŠŸç‡: {success_rate*100:5.1f}%")
        
        return {
            'episode': episode_num,
            'total_reward': episode_data['total_reward'],
            'episode_length': episode_data['episode_length'],
            'success': episode_data['success'],
            'avg_reward': avg_reward,
            'success_rate': success_rate
        }


def test_ppo_agent(env, agent, num_tests=10):
    """æµ‹è¯•PPOæ™ºèƒ½ä½“"""
    print("\nğŸ§ª æµ‹è¯•PPOæ™ºèƒ½ä½“æ€§èƒ½...")
    
    successes = 0
    total_steps = 0
    
    for test in range(num_tests):
        obs, _ = env.reset()
        steps = 0
        
        while steps < env.max_steps:
            action = agent.get_action(obs, training=False)
            obs, reward, done, truncated, info = env.step(action)
            steps += 1
            
            if done and not truncated and info.get('reason') == 'reached_goal':
                successes += 1
                break
            elif done or truncated:
                break
        
        total_steps += steps
    
    success_rate = successes / num_tests
    avg_steps = total_steps / num_tests
    
    print(f"âœ… æµ‹è¯•ç»“æœ: æˆåŠŸç‡ {success_rate*100:.1f}% | å¹³å‡æ­¥æ•° {avg_steps:.1f}")
    return success_rate, avg_steps
