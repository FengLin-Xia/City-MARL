# KLæ•£åº¦çˆ†ç‚¸çš„æ ¹æœ¬åŽŸå› ä¸Žå®Œæ•´è§£å†³æ–¹æ¡ˆ

## ðŸ”´ é—®é¢˜å›žé¡¾

**æŒç»­çš„KLé«˜é—®é¢˜**ï¼š
```
è°ƒæ•´å‰: KL = 5.24
è°ƒæ•´EntropyåŽ: KL = 4.93  
å®žæ–½MAPPOåŽ: KL = 5.69
```

**æ‰€æœ‰è°ƒæ•´éƒ½æ— æ•ˆï¼** ä¸ºä»€ä¹ˆï¼Ÿ

---

## ðŸ’¡ çœŸæ­£çš„æ ¹æœ¬åŽŸå› ï¼š**Advantageæ²¡æœ‰å½’ä¸€åŒ–**

### é—®é¢˜é“¾æ¡

```python
1. RewardèŒƒå›´å¤§ä¸”åŠ¨æ€
   total_reward = NPV + progress - budget_penalty
                = (-1500~+3000) + (0~15) - (0~500)
                = -2000 ~ +3000

2. å³ä½¿ç¼©æ”¾ï¼ˆÃ·3000ï¼‰ï¼ŒValueä»ç„¶éš¾å­¦
   scaled_reward = -0.67 ~ +1.0
   ä½†V(s)é¢„æµ‹ä»ç„¶ä¸å‡†ï¼ˆValue Loss=567ï¼‰

3. TD errorå¤§
   Î´ = r + Î³V(s') - V(s)
   å¦‚æžœVä¸å‡†ï¼ŒÎ´å¯èƒ½æ˜¯-1000~+1000

4. Advantageå¤§
   advantage = Î£(Î´)
   å¯èƒ½èŒƒå›´ï¼š-2000~+2000
   
5. Ratioè¶…èŒƒå›´
   ratio = exp(new_log_prob - old_log_prob) âˆ advantage
   å¦‚æžœadvantage=2000:
     â†’ ratioå¯èƒ½>>1.2 æˆ– <<0.8
     â†’ å…¨éƒ¨è¢«clip
     â†’ Clip=100%

6. Policyå¤§å¹…å˜åŒ–
   å³ä½¿è¢«clipï¼Œç´¯ç§¯æ›´æ–°ä»ç„¶å¤§
   â†’ KLé«˜
```

---

## âœ… è§£å†³æ–¹æ¡ˆï¼šAdvantageå½’ä¸€åŒ–

### æ ‡å‡†PPOå®žè·µ

```python
# åœ¨compute_gaeä¹‹åŽ
advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
advantages = torch.clamp(advantages, -10.0, 10.0)
```

**æ•ˆæžœ**ï¼š
```
åŽŸå§‹advantage: [-2000, +1000, -500, +800, ...]
  - å‡å€¼ä¸ä¸º0
  - æ ‡å‡†å·®å¾ˆå¤§
  - èŒƒå›´å¾ˆå®½

å½’ä¸€åŒ–åŽ: [-1.2, +0.5, -0.3, +0.4, ...]
  - å‡å€¼=0
  - æ ‡å‡†å·®=1
  - èŒƒå›´æ”¶çª„

â†’ ratioä¸ä¼šè¶…å‡º[0.8, 1.2]å¤ªå¤š
â†’ ClipçŽ‡ä¸‹é™
â†’ KLä¸‹é™
```

---

## ðŸ“Š ä¸ºä»€ä¹ˆä¹‹å‰çš„ä¿®æ”¹éƒ½æ²¡ç”¨

### ä¿®æ”¹1ï¼šè°ƒæ•´reward_scale
```
reward_scale: 500 â†’ 2000 â†’ 3000

æ•ˆæžœï¼š
  âœ… Value Lossä»Ž14000é™åˆ°567ï¼ˆæœ‰ç”¨ï¼ï¼‰
  âŒ ä½†KLä»ç„¶é«˜ï¼ˆå› ä¸ºadvantageæ²¡å½’ä¸€åŒ–ï¼‰
```

### ä¿®æ”¹2ï¼šé™ä½Žlearning rate
```
actor_lr: 1e-4 â†’ 5e-5

æ•ˆæžœï¼š
  âœ… æ›´æ–°æ­¥é•¿å°äº†
  âŒ ä½†advantageä»ç„¶å¤§ï¼Œratioä»ç„¶è¶…èŒƒå›´
```

### ä¿®æ”¹3ï¼šè°ƒæ•´Entropy
```
ent_coef: 0.15 â†’ 0.08

æ•ˆæžœï¼š
  âš ï¸ å¯èƒ½ç•¥æœ‰å¸®åŠ©
  âŒ ä½†ä¸è§£å†³æ ¹æœ¬é—®é¢˜ï¼ˆadvantageå°ºåº¦ï¼‰
```

### ä¿®æ”¹4ï¼šMAPPOç‹¬ç«‹ç½‘ç»œ
```
å…±äº«ç½‘ç»œ â†’ ç‹¬ç«‹ç½‘ç»œ

æ•ˆæžœï¼š
  âœ… æ¶ˆé™¤ç­–ç•¥å†²çªï¼ˆé•¿æœŸæœ‰ç›Šï¼‰
  âŒ ä½†ä¸è§£å†³advantageå°ºåº¦é—®é¢˜
```

**éƒ½æ²¡å‡»ä¸­è¦å®³ï¼**

---

## ðŸŽ¯ Advantageå½’ä¸€åŒ–æ‰æ˜¯æ ¸å¿ƒ

### ä¸ºä»€ä¹ˆè¿™ä¸ªæœ€å…³é”®ï¼Ÿ

**PPOçš„æ ¸å¿ƒæœºåˆ¶**ï¼š
```python
ratio = exp(new_log_prob - old_log_prob)
surr1 = ratio * advantage
surr2 = clip(ratio, 0.8, 1.2) * advantage

policy_loss = -min(surr1, surr2)
```

**å¦‚æžœadvantageå°ºåº¦ä¸å¯¹**ï¼š
```
advantage = 2000 (æœªå½’ä¸€åŒ–)

å³ä½¿ratioæŽ¥è¿‘1.0:
  surr1 = 1.0 * 2000 = 2000
  surr2 = 1.0 * 2000 = 2000
  loss = -2000

gradient âˆ loss = å·¨å¤§
â†’ Policyæ›´æ–°å·¨å¤§
â†’ ä¸‹æ¬¡ratioå°±ä¼šè¶…å‡ºèŒƒå›´
â†’ KLçˆ†ç‚¸
```

**å¦‚æžœadvantageå½’ä¸€åŒ–**ï¼š
```
advantage = 1.0 (å½’ä¸€åŒ–åŽ)

ratioæŽ¥è¿‘1.0:
  surr1 = 1.0 * 1.0 = 1.0
  surr2 = 1.0 * 1.0 = 1.0
  loss = -1.0

gradient âˆ 1.0 = åˆç†
â†’ Policyæ›´æ–°æ¸©å’Œ
â†’ ratioä¿æŒåœ¨èŒƒå›´å†…
â†’ KLä½Ž
```

---

## ðŸ“ˆ é¢„æœŸæ•ˆæžœï¼ˆåŠ äº†Advantageå½’ä¸€åŒ–ï¼‰

### ç«‹å³æ•ˆæžœï¼ˆEpisode 1-5ï¼‰

```
AdvantageèŒƒå›´ï¼ˆå½’ä¸€åŒ–åŽï¼‰: [-2, +1.5]
â†’ æ ‡å‡†å·®=1ï¼Œå¯æŽ§

RatioèŒƒå›´: 
  å¤§éƒ¨åˆ†åœ¨[0.9, 1.1]
  å°‘æ•°åœ¨[0.7, 1.3]
  
ClipçŽ‡: 30-60% (vs ä¹‹å‰100%)

KL: 0.5-1.5 (vs ä¹‹å‰5.0+)
```

### Episode 10
```
KL: 0.3-0.8
Clip: 20-40%
Value Loss: 300-500
```

---

## ðŸ”„ æ‰€æœ‰ä¿®æ”¹çš„ç»¼åˆæ•ˆæžœ

### **çŽ°åœ¨æˆ‘ä»¬æœ‰äº†**ï¼š

1. âœ… **MAPPOç‹¬ç«‹ç½‘ç»œ**ï¼šæ¶ˆé™¤ç­–ç•¥å†²çª
2. âœ… **å›ºå®šNPV Reward**ï¼šè§£å†³èººå¹³é—®é¢˜
3. âœ… **Rewardç¼©æ”¾**ï¼šValue Lossä»Ž14000é™åˆ°567
4. âœ… **Advantageå½’ä¸€åŒ–**ï¼šè§£å†³KLçˆ†ç‚¸ï¼ˆåˆšåŠ ï¼‰

**è¿™4ä¸ªä¸€èµ·**ï¼š
```
MAPPO: å„agentç‹¬ç«‹ä¼˜åŒ–
NPV: æ¸…æ™°çš„å»ºé€ æ¿€åŠ±
Reward Scale: Valueå®¹æ˜“å­¦
Advantage Norm: Ratioç¨³å®š

â†’ åº”è¯¥èƒ½å½»åº•è§£å†³KLå’Œæ”¶æ•›é—®é¢˜ï¼
```

---

## ðŸš€ çŽ°åœ¨åº”è¯¥workäº†

**å·²ä¿®æ”¹**ï¼š
- âœ… `trainers/v4_1/ppo_trainer.py` - æ·»åŠ äº†Advantageå½’ä¸€åŒ–

**éœ€è¦**ï¼š
- é‡æ–°è®­ç»ƒï¼ˆåˆ é™¤æ—§æ¨¡åž‹ï¼‰
- è§‚å¯ŸEpisode 5-10

**é¢„æœŸ**ï¼š
- KLä»Ž5.7é™åˆ°<1.5
- Clipä»Ž100%é™åˆ°<60%
- ReturnæŒç»­æå‡
- å‡ºçŽ°M/Låž‹

---

## ðŸ“ åæ€

**æˆ‘ä¹‹å‰çŠ¯çš„é”™è¯¯**ï¼š
1. è¿‡åº¦å…³æ³¨ç½‘ç»œæž¶æž„ï¼ˆå…±äº«vsç‹¬ç«‹ï¼‰
2. è¿‡åº¦è°ƒæ•´reward_scale
3. **å¿½ç•¥äº†æœ€åŸºç¡€çš„Advantageå½’ä¸€åŒ–**

**æ•™è®­**ï¼š
- PPOçš„æ ‡å‡†å®žè·µï¼ˆAdvantageå½’ä¸€åŒ–ï¼‰ä¸èƒ½å¿½ç•¥
- åº”è¯¥å…ˆæ£€æŸ¥åŸºç¡€å®žçŽ°ï¼Œå†è°ƒæž¶æž„
- ä¸“ä¸šå»ºè®®è¦ä»”ç»†çœ‹ï¼ˆæœ‰æåˆ°ä½†æˆ‘æ²¡é‡è§†ï¼‰

---

**çŽ°åœ¨æ‰€æœ‰å…³é”®ä¿®å¤éƒ½å·²å°±ä½ï¼** åº”è¯¥èƒ½workäº†ï¼

---

**ä¿®å¤æ—¥æœŸ**: 2025-10-12  
**æœ€å…³é”®ä¿®å¤**: Advantageå½’ä¸€åŒ–  
**çŠ¶æ€**: âœ… å·²å®žæ–½ï¼Œå‡†å¤‡é‡æ–°è®­ç»ƒ



