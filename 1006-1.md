# 1006-1.md

## PPOTrainer 改进建议（Rain - 2025/10/06）

这份文件对 `ppo_trainer.py` 的当前实现进行改进建议，使其从“可跑但学不会”提升为“能真正训练”的 PPO 框架。

---

### 🧠 总体目标
在不破坏当前架构（`RLPolicySelector` + `CityEnvironment`）的前提下，修复核心逻辑，使 PPO 算法能产生有效梯度更新。

---

## 1️⃣ 正确计算动作概率（log_prob）

**问题：**
当前实现直接使用
```python
log_prob = torch.log(torch.clamp(torch.tensor(action_score), min=1e-8))
```
这并非策略分布的对数概率，导致 ratio 始终为 1，策略无法学习。

**修改建议：**
使用 `torch.distributions.Categorical` 生成分布：
```python
logits, value = self.selector.actor(state_embed)
dist = torch.distributions.Categorical(logits=logits)

# 假设 Sequence 存储动作索引
action_idx = torch.tensor(sequence.action_index).to(self.device)
log_prob = dist.log_prob(action_idx)
entropy = dist.entropy()
```

---

## 2️⃣ 分离 Actor / Critic 网络

**问题：**
`selector.actor` 同时返回 logits 与 value，不利于梯度分离。

**修改建议：**
在 `RLPolicySelector` 中：
```python
self.actor = ActorNet(...)
self.critic = CriticNet(...)
self.optimizer = optim.Adam(
    list(self.actor.parameters()) + list(self.critic.parameters()),
    lr=self.lr
)
```

在 `_compute_values()` 中：
```python
with torch.no_grad():
    value = self.selector.critic(state_embed)
```

---

## 3️⃣ 修正熵正则项符号

**问题：**
当前实现使用：
```python
entropy_loss = -current_entropies.mean()
total_loss = policy_loss + value_coef * value_loss + entropy_coef * entropy_loss
```
实际上熵项方向反了，模型被鼓励“更确定”。

**正确写法：**
```python
entropy = current_entropies.mean()
total_loss = policy_loss + value_loss_coef * value_loss - entropy_coef * entropy
```

---

## 4️⃣ 确认 GAE 与 returns 对齐

确保 `returns` 与 `advantages` 与 `experiences` 等长：
```python
assert len(returns) == len(experiences)
assert len(advantages) == len(experiences)
```

若不一致，应在 `compute_gae()` 调用时剪裁。

---

## 5️⃣ 统一经验结构

明确 `Sequence` 对象的核心字段：
```python
class Sequence:
    def __init__(self, actions, score, action_index):
        self.actions = actions
        self.score = score
        self.action_index = action_index
```

这样在更新阶段能正确索引 `action_index`。

---

## ✅ 最终目标
- **从伪概率 → 真分布更新**  
- **从单网络 → 双网络结构**  
- **从负熵惩罚 → 正熵奖励**  
- **从演示流程 → 可收敛训练**

---

> 📅 版本：1006-1  
> 📘 作者：ChatGPT (GPT-5) 为 Rain 提供  
> 🧩 用途：PPOTrainer 修复与训练逻辑完善草案
