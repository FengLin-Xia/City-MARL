#!/usr/bin/env python3
"""
GPUåŠ é€Ÿ1000 Episodesè®­ç»ƒ
ä¸€åƒé›¶ä¸€å¤œçš„å¼ºåŒ–å­¦ä¹ ä¹‹æ—…
"""

import sys
import os
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from pathlib import Path
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from envs.terrain_road_env import TerrainRoadEnvironment
from agents.terrain_policy import TerrainPolicyNetwork

class GPUTrainer:
    """GPUåŠ é€Ÿè®­ç»ƒå™¨"""
    
    def __init__(self, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æŸ¥æ‰¾åœ°å½¢æ•°æ®
        terrain_dir = Path("data/terrain")
        terrain_files = list(terrain_dir.glob("terrain_continuity_boundary_*.json"))
        
        if not terrain_files:
            print("âŒ æœªæ‰¾åˆ°åœ°å½¢æ•°æ®æ–‡ä»¶")
            return
        
        latest_file = max(terrain_files, key=lambda x: x.stat().st_mtime)
        print(f"ğŸ—ºï¸ ä½¿ç”¨åœ°å½¢æ–‡ä»¶: {latest_file}")
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = TerrainRoadEnvironment(mesh_file=str(latest_file))
        print(f"ğŸ“ ç¯å¢ƒç½‘æ ¼å°ºå¯¸: {self.env.grid_size}")
        
        # åˆ›å»ºç­–ç•¥ç½‘ç»œ
        self.policy_net = TerrainPolicyNetwork(
            grid_size=self.env.grid_size, 
            action_space=self.env.action_space
        ).to(self.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.success_count = 0
        self.total_episodes = 0
        
        print(f"ğŸ§  ç­–ç•¥ç½‘ç»œå‚æ•°æ•°é‡: {sum(p.numel() for p in self.policy_net.parameters()):,}")
    
    def preprocess_observation(self, obs):
        """é¢„å¤„ç†è§‚å¯Ÿæ•°æ®åˆ°GPU"""
        obs_tensor = {}
        for key, value in obs.items():
            if isinstance(value, np.ndarray):
                obs_tensor[key] = torch.from_numpy(value).unsqueeze(0).to(self.device)
            else:
                obs_tensor[key] = torch.tensor([value]).to(self.device)
        return obs_tensor
    
    def get_action(self, obs_tensor, epsilon=0.1):
        """è·å–åŠ¨ä½œï¼ˆepsilon-greedyï¼‰"""
        if np.random.random() < epsilon:
            return np.random.randint(self.env.action_space.n)
        
        with torch.no_grad():
            action_logits, _ = self.policy_net(obs_tensor)
            action_probs = F.softmax(action_logits, dim=-1)
            action = torch.multinomial(action_probs, 1).item()
            return action
    
    def train_episode(self, episode_num):
        """è®­ç»ƒä¸€ä¸ªepisode"""
        # å…ˆé‡ç½®ç¯å¢ƒ
        obs, _ = self.env.reset()
        total_reward = 0
        step_count = 0
        max_steps = 200
        
        # ç„¶åå¼€å§‹è®°å½•episode
        self.env.start_recording()
        
        while step_count < max_steps:
            # é¢„å¤„ç†è§‚å¯Ÿ
            obs_tensor = self.preprocess_observation(obs)
            
            # è·å–åŠ¨ä½œ
            epsilon = max(0.01, 0.1 * (0.95 ** (episode_num // 100)))  # è¡°å‡çš„epsilon
            action = self.get_action(obs_tensor, epsilon)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, done, truncated, info = self.env.step(action)
            
            # è®¡ç®—æŸå¤±ï¼ˆç®€å•çš„ç­–ç•¥æ¢¯åº¦ï¼‰
            action_logits, value = self.policy_net(obs_tensor)
            action_probs = F.softmax(action_logits, dim=-1)
            action_dist = torch.distributions.Categorical(action_probs)
            
            # ç­–ç•¥æ¢¯åº¦æŸå¤±
            loss = -action_dist.log_prob(torch.tensor([action], device=self.device)) * torch.tensor([reward], device=self.device)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            total_reward += reward
            step_count += 1
            obs = next_obs
            
            if done or truncated:
                break
        
        # åœæ­¢è®°å½•
        episode_data = self.env.stop_recording()
        
        # æ›´æ–°ç»Ÿè®¡
        self.episode_rewards.append(total_reward)
        self.episode_lengths.append(step_count)
        self.total_episodes += 1
        
        if info.get('reached_target', False):
            self.success_count += 1
        
        # ä¿å­˜episodeï¼ˆæ¯10ä¸ªepisodeä¿å­˜ä¸€æ¬¡ï¼‰
        if episode_num % 10 == 0:
            if episode_data:  # åªæœ‰episode_dataä¸ä¸ºNoneæ—¶æ‰ä¿å­˜
                self.save_episode(episode_data, episode_num)
            else:
                print(f"âš ï¸ Episode {episode_num} å½•åˆ¶æ•°æ®ä¸ºç©ºï¼Œæœªä¿å­˜JSONã€‚")
        
        return total_reward, step_count, info.get('reached_target', False)
    
    def save_episode(self, episode_data, episode_num):
        """ä¿å­˜episodeæ•°æ®"""
        episodes_dir = Path("data/episodes")
        episodes_dir.mkdir(exist_ok=True)
        
        filename = episodes_dir / f"episode_{episode_num}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(episode_data, f, ensure_ascii=False, indent=2)
    
    def print_progress(self, episode_num, reward, steps, reached_target, start_time):
        """æ‰“å°è®­ç»ƒè¿›åº¦"""
        elapsed_time = time.time() - start_time
        avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)
        success_rate = self.success_count / self.total_episodes * 100
        
        print(f"ğŸ“Š Episode {episode_num:4d} | "
              f"å¥–åŠ±: {reward:6.2f} | "
              f"æ­¥æ•°: {steps:3d} | "
              f"åˆ°è¾¾: {'âœ…' if reached_target else 'âŒ'} | "
              f"å¹³å‡å¥–åŠ±: {avg_reward:6.2f} | "
              f"æˆåŠŸç‡: {success_rate:5.1f}% | "
              f"ç”¨æ—¶: {elapsed_time/60:.1f}åˆ†é’Ÿ")
    
    def train_1000_episodes(self):
        """è®­ç»ƒ1000ä¸ªepisodes"""
        print("ğŸš€ å¼€å§‹ä¸€åƒé›¶ä¸€å¤œçš„å¼ºåŒ–å­¦ä¹ ä¹‹æ—…ï¼")
        print("=" * 80)
        
        start_time = time.time()
        
        for episode in range(1, 1001):
            reward, steps, reached_target = self.train_episode(episode)
            
            # æ¯10ä¸ªepisodeæ‰“å°ä¸€æ¬¡è¿›åº¦
            if episode % 10 == 0:
                self.print_progress(episode, reward, steps, reached_target, start_time)
            
            # æ¯100ä¸ªepisodeä¿å­˜ä¸€æ¬¡æ¨¡å‹
            if episode % 100 == 0:
                self.save_model(episode)
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print("\n" + "=" * 80)
        print("ğŸ‰ ä¸€åƒé›¶ä¸€å¤œè®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸ æ€»ç”¨æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
        print(f"ğŸ“ˆ æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(self.episode_rewards[-100:]):.2f}")
        print(f"ğŸ¯ æœ€ç»ˆæˆåŠŸç‡: {self.success_count/self.total_episodes*100:.1f}%")
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: models/terrain_policy_1000.pth")
    
    def save_model(self, episode_num):
        """ä¿å­˜æ¨¡å‹"""
        models_dir = Path("models")
        models_dir.mkdir(exist_ok=True)
        
        model_path = models_dir / f"terrain_policy_{episode_num}.pth"
        torch.save({
            'episode': episode_num,
            'model_state_dict': self.policy_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'success_count': self.success_count,
            'total_episodes': self.total_episodes
        }, model_path)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ ä¸€åƒé›¶ä¸€å¤œ GPUåŠ é€Ÿå¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
    print("=" * 80)
    
    # æ£€æŸ¥CUDA
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print("âš ï¸ ä½¿ç”¨CPUè®­ç»ƒï¼ˆå»ºè®®ä½¿ç”¨GPUä»¥è·å¾—æ›´å¥½æ€§èƒ½ï¼‰")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = GPUTrainer()
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train_1000_episodes()
    
    print("\nğŸ¬ è®­ç»ƒå®Œæˆï¼ç°åœ¨å¯ä»¥è§‚çœ‹å›æ”¾äº†ï¼")
    print("è¿è¡Œ: python tests/quick_replay.py")

if __name__ == "__main__":
    main()
