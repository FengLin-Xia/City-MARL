#!/usr/bin/env python3
"""
KLæ•£åº¦ä¿®å¤è°ƒè¯•è„šæœ¬ - æŒ‰ç…§1013-7.mdçš„ä¼˜å…ˆçº§æ‰§è¡Œä¿®å¤
"""

import torch
import torch.nn as nn
import numpy as np
import json
import os
from typing import Dict, List

def fix_kl_divergence():
    """æŒ‰ä¼˜å…ˆçº§ä¿®å¤KLæ•£åº¦é—®é¢˜"""
    
    print("å¼€å§‹KLæ•£åº¦ä¿®å¤è°ƒè¯•...")
    
    # 1. å…³ç†µ 5 ä¸ªæ›´æ–°
    print("\n1. å…³é—­ç†µå¥–åŠ± (ent_coef = 0)")
    fix_entropy_coef()
    
    # 2. è®©logitså…ˆå°–ä¸€ç‚¹ï¼ˆè®­ç»ƒæœŸä¸´æ—¶ï¼‰
    print("\n2. åº”ç”¨æ¸©åº¦ç¼©æ”¾ (tau = 0.5)")
    # è¿™ä¸ªå·²ç»åœ¨ppo_trainer.pyä¸­å®ç°äº†
    
    # 3. æ£€æŸ¥å¹¶æ‹‰é«˜logitsçš„èµ·ä¼
    print("\n3. é‡ç½®æœ€åä¸€å±‚åˆå§‹åŒ–")
    reset_actor_last_layer()
    
    # 4. å¢å¤§æœ‰æ•ˆbatch
    print("\n4. å¢å¤§æœ‰æ•ˆbatch")
    increase_batch_size()
    
    # 5. è®©KLè‡ªè°ƒ
    print("\n5. å®ç°è‡ªé€‚åº”KLè°ƒæ•´")
    implement_adaptive_kl()
    
    print("\næ‰€æœ‰ä¿®å¤å®Œæˆï¼ç°åœ¨è¿è¡Œæµ‹è¯•...")

def fix_entropy_coef():
    """å…³é—­ç†µå¥–åŠ±"""
    config_path = "configs/city_config_v4_1.json"
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # å…³é—­ç†µå¥–åŠ±
    config['solver']['rl']['ent_coef'] = 0.0
    
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"  å·²è®¾ç½® ent_coef = 0.0")

def reset_actor_last_layer():
    """é‡ç½®actorç½‘ç»œæœ€åä¸€å±‚åˆå§‹åŒ–"""
    
    # è¯»å–ç°æœ‰çš„actorç½‘ç»œå®šä¹‰
    selector_path = "solvers/v4_1/rl_selector.py"
    
    with open(selector_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ›¿æ¢åˆå§‹åŒ–ä»£ç 
    old_init = """        # æŒ‰ç…§1013-6.mdå»ºè®®ï¼šé‡åˆå§‹åŒ–æœ€åä¸€å±‚ï¼ˆæé«˜gainåˆ°0.5ï¼‰
        torch.nn.init.orthogonal_(self.network[-1].weight, gain=0.5)
        torch.nn.init.zeros_(self.network[-1].bias)"""
    
    new_init = """        # æŒ‰ç…§1013-7.mdå»ºè®®ï¼šé‡åˆå§‹åŒ–æœ€åä¸€å±‚ï¼ˆæ­£äº¤+å°å¢ç›Šï¼‰
        torch.nn.init.orthogonal_(self.network[-1].weight, gain=0.1)
        torch.nn.init.zeros_(self.network[-1].bias)"""
    
    content = content.replace(old_init, new_init)
    
    with open(selector_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"  å·²é‡ç½®actoræœ€åä¸€å±‚åˆå§‹åŒ– (gain=0.1)")

def increase_batch_size():
    """å¢å¤§æœ‰æ•ˆbatch"""
    config_path = "configs/city_config_v4_1.json"
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # å¢å¤§batch size
    config['solver']['rl']['mini_batch_size'] = 32  # ä»10å¢åŠ åˆ°32
    config['solver']['rl']['rollout_steps'] = 20    # ä»10å¢åŠ åˆ°20
    
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"  å·²å¢å¤§batch size: mini_batch_size=32, rollout_steps=20")

def implement_adaptive_kl():
    """å®ç°è‡ªé€‚åº”KLè°ƒæ•´"""
    
    trainer_path = "trainers/v4_1/ppo_trainer.py"
    
    with open(trainer_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # åœ¨PPOTrainerç±»ä¸­æ·»åŠ è‡ªé€‚åº”KLè°ƒæ•´æ–¹æ³•
    adaptive_kl_method = '''
    def _adaptive_kl_adjustment(self, kl_after: float):
        """è‡ªé€‚åº”KLè°ƒæ•´ï¼ˆæŒ‰ç…§1013-7.mdå»ºè®®ï¼‰"""
        target_kl = 0.02
        
        if kl_after < 0.2 * target_kl:  # å¤ªä¿å®ˆ
            # å¢å¤§å­¦ä¹ ç‡
            for agent in self.selector.actor_optimizers.keys():
                optimizer = self.selector.actor_optimizers[agent]
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= 1.5
            print(f"[adaptive] KL too low ({kl_after:.4f} < {0.2 * target_kl:.4f}), increased lr")
            
        elif kl_after > 2.0 * target_kl:  # å¤ªçŒ›
            # å‡å°å­¦ä¹ ç‡
            for agent in self.selector.actor_optimizers.keys():
                optimizer = self.selector.actor_optimizers[agent]
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= 0.5
            print(f"[adaptive] KL too high ({kl_after:.4f} > {2.0 * target_kl:.4f}), decreased lr")
'''
    
    # æ‰¾åˆ°ç±»å®šä¹‰çš„ç»“æŸä½ç½®å¹¶æ’å…¥æ–¹æ³•
    if "_adaptive_kl_adjustment" not in content:
        # åœ¨set_seedæ–¹æ³•åæ’å…¥
        insert_pos = content.find("def set_seed(self, seed: int):")
        if insert_pos != -1:
            # æ‰¾åˆ°set_seedæ–¹æ³•çš„ç»“æŸä½ç½®
            lines = content.split('\n')
            for i, line in enumerate(lines):
                if "def set_seed(self, seed: int):" in line:
                    # æ‰¾åˆ°ä¸‹ä¸€ä¸ªæ–¹æ³•çš„å¼€å§‹
                    for j in range(i+1, len(lines)):
                        if lines[j].strip().startswith("def ") and not lines[j].strip().startswith("    "):
                            # åœ¨set_seedæ–¹æ³•åæ’å…¥æ–°æ–¹æ³•
                            lines.insert(j, adaptive_kl_method)
                            break
                    break
            
            content = '\n'.join(lines)
            
            with open(trainer_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"  å·²æ·»åŠ è‡ªé€‚åº”KLè°ƒæ•´æ–¹æ³•")
    
    # åœ¨AFTERæµ‹é‡éƒ¨åˆ†è°ƒç”¨è‡ªé€‚åº”è°ƒæ•´
    if "self._adaptive_kl_adjustment(kl_after)" not in content:
        # æ‰¾åˆ°KL_afteræ‰“å°çš„ä½ç½®
        kl_after_line = "print(f\"[probe] Î”loc_L2={dL2:.4g} | KL_before={kl_before:.3g} | KL_after={kl_after:.3g}\")"
        replacement = f"{kl_after_line}\n                    \n                    # è‡ªé€‚åº”KLè°ƒæ•´\n                    self._adaptive_kl_adjustment(kl_after)"
        
        content = content.replace(kl_after_line, replacement)
        
        with open(trainer_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"  å·²é›†æˆè‡ªé€‚åº”KLè°ƒæ•´åˆ°è®­ç»ƒå¾ªç¯")

def create_causality_test():
    """åˆ›å»ºå› æœæ€§æµ‹è¯•ï¼ˆå¦‚æœKLè¿˜æ˜¯0çš„è¯ï¼‰"""
    
    test_code = '''#!/usr/bin/env python3
"""
å› æœæ€§æµ‹è¯• - æ£€æŸ¥åŠ¨ä½œæ˜¯å¦çœŸçš„å½±å“å›æŠ¥
"""

import torch
import numpy as np
from enhanced_city_simulation_v4_1 import CityEnvironment
from solvers.v4_1.rl_selector import RLPolicySelector
import json

def test_action_causality():
    """æµ‹è¯•åŒä¸€çŠ¶æ€ä¸‹ä¸åŒåŠ¨ä½œçš„å›æŠ¥å·®å¼‚"""
    print("ğŸ” å¼€å§‹å› æœæ€§æµ‹è¯•...")
    
    # åŠ è½½é…ç½®
    with open("configs/city_config_v4_1.json", 'r') as f:
        cfg = json.load(f)
    
    # åˆ›å»ºç¯å¢ƒå’Œé€‰æ‹©å™¨
    env = CityEnvironment(cfg)
    selector = RLPolicySelector(cfg)
    
    # å›ºå®šéšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # è·å–ä¸€ä¸ªçŠ¶æ€
    state = env.get_state()
    actions = selector.enumerate_actions(state)
    
    if len(actions) < 5:
        print(f"âŒ å¯ç”¨åŠ¨ä½œå¤ªå°‘: {len(actions)}")
        return
    
    # é€‰æ‹©å‰5ä¸ªåŠ¨ä½œè¿›è¡Œæµ‹è¯•
    test_actions = actions[:5]
    returns = []
    
    for i, action in enumerate(test_actions):
        print(f"æµ‹è¯•åŠ¨ä½œ {i+1}: {action}")
        
        # é‡ç½®ç¯å¢ƒåˆ°ç›¸åŒçŠ¶æ€
        env.reset()
        env.set_state(state)
        
        # æ‰§è¡Œå•ä¸ªåŠ¨ä½œ
        reward = env.step(action)
        
        # è¿è¡Œå‡ æ­¥çœ‹å›æŠ¥
        total_reward = reward
        for _ in range(5):  # è¿è¡Œ5æ­¥
            if env.is_done():
                break
            step_reward = env.step_random()  # éšæœºåŠ¨ä½œ
            total_reward += step_reward
        
        returns.append(total_reward)
        print(f"  æ€»å›æŠ¥: {total_reward:.3f}")
    
    # è®¡ç®—å›æŠ¥å·®å¼‚
    min_return = min(returns)
    max_return = max(returns)
    delta_return = max_return - min_return
    
    print(f"\\nğŸ“Š å› æœæ€§æµ‹è¯•ç»“æœ:")
    print(f"  å›æŠ¥èŒƒå›´: [{min_return:.3f}, {max_return:.3f}]")
    print(f"  å·®å¼‚ Î”return: {delta_return:.3f}")
    
    if delta_return < 0.01:
        print("âš ï¸  åŠ¨ä½œå¯¹å›æŠ¥å½±å“å¾ˆå°ï¼Œå¯èƒ½éœ€è¦shapingå¥–åŠ±")
        print("å»ºè®®: æ·»åŠ å³æ—¶shapingå¥–åŠ± Î±*(score - Î»*cost + Î²*rent_gain)")
    else:
        print("âœ… åŠ¨ä½œå¯¹å›æŠ¥æœ‰æ˜æ˜¾å½±å“ï¼Œé—®é¢˜å¯èƒ½åœ¨æ¢¯åº¦ä¼ æ’­")
    
    return delta_return

if __name__ == "__main__":
    test_action_causality()
'''
    
    with open("test_causality.py", 'w', encoding='utf-8') as f:
        f.write(test_code)
    
    print(f"  å·²åˆ›å»ºå› æœæ€§æµ‹è¯•æ–‡ä»¶: test_causality.py")

def run_test():
    """è¿è¡Œæµ‹è¯•"""
    print("\nè¿è¡Œä¿®å¤åçš„æµ‹è¯•...")
    
    # è¿è¡Œä¸»ç¨‹åº
    os.system("python enhanced_city_simulation_v4_1.py --mode rl")
    
    print("\nè§‚å¯ŸæŒ‡æ ‡:")
    print("  - KLåº”è¯¥ä» ~1e-6 å‡åˆ° 1e-3~1e-2")
    print("  - clip_fraction åº”è¯¥ > 0")
    print("  - entropy åº”è¯¥å¼€å§‹ç¼“æ…¢ä¸‹é™")
    print("  - loc.std åº”è¯¥ä» ~3e-05 å‡åˆ° ~1e-3~1e-1")

if __name__ == "__main__":
    fix_kl_divergence()
    create_causality_test()
    run_test()
