下面给你一套不靠猜的定位+点火方案，直接把 KL 拉到 1e-2 等级（健康区），按顺序做就行：

1) 先量两件关键指标（10 行代码，马上能定位）

在每个 mini-batch 的 step 前后各量一次：

# === BEFORE ===
logits_before = actor(state_embed)                # [1, A]
local_before  = logits_before[0, :K]              # 或 logits_before[0, subset_ids]
std_embed     = state_embed.std(dim=-1).mean().item()
std_logits    = local_before.std().item()
dist_before   = torch.distributions.Categorical(logits=local_before)
newlp_before  = dist_before.log_prob(action_idx_tensor)
kl_before     = (old_logp - newlp_before).mean().item()

# 反传更新
loss.backward()
actor_optim.step()

# === AFTER ===
with torch.no_grad():
    logits_after = actor(state_embed)
    local_after  = logits_after[0, :K]
    dist_after   = torch.distributions.Categorical(logits=local_after)
    newlp_after  = dist_after.log_prob(action_idx_tensor)
    kl_after     = (old_logp - newlp_after).mean().item()
    dL2          = (local_after - local_before).pow(2).mean().sqrt().item()

print(f"[probe] embed.std={std_embed:.4g}  loc.std={std_logits:.4g}  Δloc_L2={dL2:.4g}  KL bef/after={kl_before:.3g}/{kl_after:.3g}")


解读一行就够：

embed.std ≪ 1e-3 → 状态编码无信息；

embed.std 有、但 loc.std ≪ 1e-3 或 Δloc_L2≈0 → actor 头/初始化把起伏压没了；

Δloc_L2>0 但 KL after 仍≈0 → 熵/温度太高 or 优势太弱（策略动了，softmax 还是均匀）。

2) 对应打补丁（就两三行改动，立刻让 KL 动起来）
A. 若 embed.std 很小 → 给输入“起伏 + 归一化”
# 训练期维护 running mean/std
state_embed = (state_embed - m) / (s + 1e-5)
state_embed = state_embed.clamp(-5, 5)
# 临时放大一倍验证链路（看到 KL↑ 就回退）
state_embed = 2.0 * state_embed

B. 若 loc.std/Δloc_L2 很小 → 让 logits 先“尖一点”

确保 最后一层没有 tanh/sigmoid（Logits 直出 softmax）。

重新初始化最后一层（一次性做）：

torch.nn.init.orthogonal_(actor_last.weight, gain=0.01)
torch.nn.init.zeros_(actor_last.bias)


温度缩放 τ<1（只在训练时）：

tau = 0.5               # 先 0.5；看 KL 起色后可回到 1.0
local_logits = local_logits / tau

C. 若 logits 在动但 KL 仍≈0 → 先把“回拉均匀”的力关掉

暂时关熵：ent_coef = 0（连跑 5 个 update）

放大优势（只是验证链路，不是长期配置）：

adv = (adv - adv.mean()) / (adv.std() + 1e-8)
adv = 2.0 * adv


提升 policy 步子（你已到 1e-3，可以保留），增加 policy epochs 到 8–10，max_grad_norm = 1.0。

目标健康区：KL after ≈ 0.01~0.03、clip_fraction ≈ 0.1~0.3、ratio.mean ≈ 1.0 ± 0.1、entropy 开始缓慢下降。

3) 如果还是不动：用“强力自适应 KL”把它拉起来（防飞）

把“after 的 KL”作为反馈，自适应调 lr：

target_kl = 0.02
if kl_after < 0.2 * target_kl:              # 太保守
    for g in actor_optim.param_groups: g['lr'] *= 1.5
elif kl_after > 2.0 * target_kl:            # 太猛
    for g in actor_optim.param_groups: g['lr'] *= 0.5

4) 最关键的“因果”检查（一次即可）

如果做完 1–3，KL 还是钉死 0，那基本就是 动作对回报几乎没影响（Cursor 第3点）：

同一状态，挑 5 个合法动作各 rollout 一下月度回报，算 Δreturn = max - min。

若 Δreturn ≈ 0：给即时 shaping（让选择当下就有差异）：

r_t += α * (action.score - λ*action.cost + β*expected_rent_gain)   # α 先 0.3、λ=1.0、β=0.5


或者让 budget/规则直接改变可行动作集合（mask），让不同选择立刻影响 K 与可选项质量。

你这轮日志的特别信号

grad_norm≈7e-6 但 param_diff≈2~3.8：参数确实在动，但打不到被用到的 local_logits（或被 softmax 温度/熵压平）。

entropy≈log(K) 始终稳定：最像 logits“太冷/太平” 的症状。
→ 优先尝试 B+C：最后层初始化 + τ=0.5 + ent_coef=0 + n_epochs=10。
跑一轮你应该能看到：KL after 从 ~1e-6 抬到 1e-3~1e-2，entropy 有轻微下降，clip_fraction 出现非零。