# v5.0 增强城市模拟系统架构分析报告

**生成时间**: 2024年10月22日  
**版本**: v5.0.0  
**分析范围**: 动作空间、PPO框架、动态约束机制

---

## 1. 动作空间分析

### 1.1 动作空间大小 N

v5.0 架构采用**动态动作空间设计**，不是传统的固定大小：

#### 总体动作空间
- **总动作数**: 9个离散动作 (ID: 0-8)
- **动作类型**: 建筑建设动作，按智能体类型分组

#### 各智能体动作空间
```json
"agents": {
    "defs": {
        "EDU": {
            "action_ids": [0, 1, 2],     // 3个动作：EDU_S, EDU_M, EDU_L
            "constraints": {
                "max_actions_per_turn": 1,
                "budget_limit": 0,
                "budget_sharing": "EDU_COUNCIL_SHARED"
            }
        },
        "IND": {
            "action_ids": [3, 4, 5],     // 3个动作：IND_S, IND_M, IND_L
            "constraints": {
                "max_actions_per_turn": 2,
                "budget_limit": 15000
            }
        },
        "COUNCIL": {
            "action_ids": [6, 7, 8],     // 3个动作：COUNCIL_A, COUNCIL_B, COUNCIL_C
            "constraints": {
                "max_actions_per_turn": 1,
                "budget_limit": 0,
                "budget_sharing": "EDU_COUNCIL_SHARED",
                "special_rules": {
                    "start_after_month": 6
                }
            }
        }
    }
}
```

### 1.2 每步可用集合不同

**是的，每步可用动作集合确实不同**，这是 v5.0 的核心特性：

#### 动态约束机制
1. **智能体类型约束**: 每个智能体只能选择自己对应的动作ID
2. **预算约束**: 不同智能体有不同的预算限制
3. **时间约束**: COUNCIL 智能体在第6个月后才开始行动
4. **地理约束**: 河流限制影响 EDU 和 IND 的动作选择
5. **槽位约束**: 已占用的槽位不能重复使用

#### 动作掩码实现
```python
def get_action_mask(self) -> np.ndarray:
    """获取动作掩膜"""
    mask = np.zeros(9, dtype=np.int32)  # 9个动作的掩码
    
    # 根据当前智能体类型设置可用动作
    if current_agent == "EDU":
        mask[0:3] = 1  # 只允许动作 0,1,2
    elif current_agent == "IND":
        mask[3:6] = 1  # 只允许动作 3,4,5
    elif current_agent == "COUNCIL":
        mask[6:9] = 1  # 只允许动作 6,7,8
    
    # 应用其他约束（预算、槽位等）
    mask = self._apply_budget_constraints(mask)
    mask = self._apply_slot_constraints(mask)
    
    return mask
```

---

## 2. PPO 框架实现

### 2.1 框架选择

v5.0 使用**自写的 PPO 框架**，不是 CleanRL 或 Stable-Baselines：

#### 实现特点
- **自写实现**: `V5PPOTrainer` 类
- **多智能体支持**: 每个智能体独立的 Actor-Critic 网络
- **配置驱动**: 所有超参数通过配置文件设置
- **动作掩码支持**: 内置动作掩码机制

### 2.2 网络架构

```json
"mappo": {
    "net": {
        "obs_dim": 512,                    // 观察空间维度
        "share_encoder": true,            // 共享编码器
        "actor_hidden": [256, 128],       // Actor 网络隐藏层
        "critic_hidden": [256, 128],      // Critic 网络隐藏层
        "use_lstm": false                 // 不使用 LSTM
    },
    "ppo": {
        "clip_eps": 0.25,                 // PPO 裁剪参数
        "gamma": 0.99,                    // 折扣因子
        "gae_lambda": 0.8,                // GAE 参数
        "entropy_coef": 0.1,              // 熵系数
        "value_coef": 0.5,                // 价值损失系数
        "target_kl": 0.02,                // KL 散度目标
        "max_grad_norm": 0.5,             // 梯度裁剪
        "lr": 0.0003,                     // 学习率
        "lr_schedule": "cosine"           // 学习率调度
    }
}
```

### 2.3 训练流程

#### 经验收集
```python
def collect_experience(self, num_steps: int) -> List[Dict]:
    """收集经验数据"""
    all_experiences = []
    
    while steps_collected < num_steps:
        # 重置环境
        state = self.env.reset()
        
        # 获取当前阶段的所有智能体
        phase_agents = self.env.get_phase_agents()
        
        # 为每个智能体获取动作候选
        for agent in phase_agents:
            candidates = self.env.get_action_candidates(agent)
            if candidates:
                # 使用策略选择动作
                action = self.selector.select_action(agent, candidates, state)
```

#### 策略更新
```python
def update_policy(self, experiences: List[Dict]):
    """更新策略网络"""
    for agent in self.agents:
        # 计算优势函数
        advantages = self.compute_advantages(experiences[agent])
        
        # PPO 损失计算
        policy_loss = self.compute_policy_loss(experiences[agent], advantages)
        value_loss = self.compute_value_loss(experiences[agent])
        
        # 更新网络
        self.optimizers[agent]['actor'].zero_grad()
        policy_loss.backward()
        self.optimizers[agent]['actor'].step()
```

---

## 3. 动态约束系统

### 3.1 约束类型

#### 预算约束
```json
"budget_pools": {
    "EDU_COUNCIL_SHARED": {
        "members": ["EDU", "COUNCIL"],
        "total_budget": 20000,
        "allocation_strategy": "dynamic"
    }
}
```

#### 地理约束
```json
"river_restrictions": {
    "enabled": true,
    "affects_agents": ["IND", "EDU"],
    "council_bypass": true,
    "river_side_assignment": {
        "method": "hub_based",
        "fallback": "random"
    }
}
```

#### 时间约束
```json
"special_rules": {
    "river_bypass": true,
    "start_after_month": 6,  // COUNCIL 第6个月后开始
    "other_side_bonus": {
        "A": 70.0,
        "B": 90.0,
        "C": 110.0
    }
}
```

### 3.2 中间件系统

v5.0 使用中间件模式处理约束：

```json
"action_mw": [
    "conflict.drop_late",           // 冲突处理
    "budget.shared_ledger",         // 预算管理
    "legality.env",                 // 合法性检查
    "river_restriction",            // 河流限制
    "candidate_range",             // 候选范围
    "sequence.trim_to_max_len"     // 序列裁剪
]
```

---

## 4. 调度系统

### 4.1 阶段调度

```json
"scheduler": {
    "name": "phase_cycle",
    "params": {
        "step_unit": "month",
        "period": 1,
        "offset": 0,
        "phases": [
            {
                "agents": ["IND"],
                "mode": "sequential"
            },
            {
                "agents": ["EDU", "COUNCIL"],
                "mode": "concurrent"
            }
        ]
    }
}
```

### 4.2 执行模式

- **Sequential**: 智能体按顺序执行
- **Concurrent**: 智能体并发执行
- **Dynamic**: 根据状态动态调整

---

## 5. 性能优化

### 5.1 配置优化

```json
"performance": {
    "device": "cuda",
    "amp": true,                    // 自动混合精度
    "num_workers": 4,               // 并行工作进程
    "pin_memory": true              // 内存固定
}
```

### 5.2 训练优化

```json
"rollout": {
    "num_envs": 8,                  // 并行环境数
    "horizon": 20,                  // 轨迹长度
    "minibatch_size": 32,           // 小批量大小
    "updates_per_iter": 4,          // 每次迭代更新次数
    "max_updates": 10              // 最大更新次数
}
```

---

## 6. 总结

### 6.1 核心特性

1. **动态动作空间**: 9个动作，按智能体类型分组
2. **每步可用集合不同**: 受预算、地理、时间等约束影响
3. **自写PPO框架**: 支持多智能体、动作掩码、配置驱动
4. **中间件约束**: 模块化的约束处理系统
5. **阶段调度**: 支持顺序和并发执行模式

### 6.2 技术优势

- **配置驱动**: 所有参数可配置，支持不同场景
- **模块化设计**: 各组件独立，易于扩展
- **性能优化**: 支持GPU加速、并行处理
- **错误处理**: 完善的错误处理和恢复机制
- **可扩展性**: 支持新智能体类型和约束规则

### 6.3 适用场景

- 城市规划和建设模拟
- 多智能体协作决策
- 资源分配优化
- 政策影响评估
- 复杂约束下的决策学习

---

**报告生成时间**: 2024年10月22日  
**分析工具**: 基于 v5.0 源码和配置文件的深度分析  
**数据来源**: `configs/city_config_v5_0.json`, `enhanced_city_simulation_v5_0.py`, 相关模块源码
