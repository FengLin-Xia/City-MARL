# v5.0 强化学习动作选择逻辑详细分析

**生成时间**: 2024年10月22日  
**分析范围**: 策略网络、动作空间、掩码机制、环境执行

---

## 1. 策略网络输出动作空间结构

### 1.1 动作空间设计
**v5.0 采用单一离散动作 ID 设计**：

```python
# 动作空间结构
action_space = {
    "EDU": [0, 1, 2],      # EDU_S, EDU_M, EDU_L
    "IND": [3, 4, 5],      # IND_S, IND_M, IND_L  
    "COUNCIL": [6, 7, 8]   # COUNCIL_A, COUNCIL_B, COUNCIL_C
}
```

**关键实现** (`solvers/v5_0/rl_selector.py:119-149`):
```python
def select_action(self, agent: str, candidates: List[ActionCandidate], 
                 state: EnvironmentState, greedy: bool = False) -> Optional[Dict[str, Any]]:
    """基于策略从候选中选择动作"""
    if not candidates:
        return None
    
    # 候选内索引化：仅对当前候选集合建分布
    obs = self._encode_state(state)
    logits_full = self.actor_networks[agent](obs)
    
    # 提取对应候选的 logit，按候选顺序组成向量
    cand_ids_list = [c.id for c in candidates]
    logits = logits_full[cand_ids_list]
    
    # 温度采样
    temp = float(self.config.get('mappo', {}).get('exploration', {}).get('temperature', 1.0))
    logits = logits / max(temp, 1e-6)
    
    # 采样动作
    if greedy:
        idx = int(torch.argmax(probs_vec).item())
    else:
        dist = D.Categorical(probs_vec)
        idx = int(dist.sample().item())
    
    action_id = cand_ids_list[idx]
    return {"sequence": Sequence(agent=agent, actions=[action_id]), 
            "logprob": chosen_logprob, "value": value}
```

### 1.2 动作ID与实体映射
**动作ID直接映射到建筑类型和尺寸**：

```python
# 配置文件中的映射 (configs/city_config_v5_0.json:217-271)
"action_params": {
    "0": {"desc": "EDU_S", "cost": 650, "reward": 160, "prestige": 0.2},
    "1": {"desc": "EDU_M", "cost": 1150, "reward": 530, "prestige": 0.6},
    "2": {"desc": "EDU_L", "cost": 2700, "reward": 360, "prestige": 1.0},
    "3": {"desc": "IND_S", "cost": 900, "reward": 150, "prestige": 0.2},
    "4": {"desc": "IND_M", "cost": 1500, "reward": 280, "prestige": 0.1},
    "5": {"desc": "IND_L", "cost": 2400, "reward": 450, "prestige": -0.1},
    "6": {"desc": "COUNCIL_A", "cost": 570, "reward": 570, "prestige": 0.3},
    "7": {"desc": "COUNCIL_B", "cost": 870, "reward": 870, "prestige": 0.7},
    "8": {"desc": "COUNCIL_C", "cost": 1150, "reward": 1150, "prestige": 1.2}
}
```

---

## 2. 每时间步动作选择机制

### 2.1 支持多动作选择
**v5.0 支持序列化多动作选择**：

```python
# 序列结构 (contracts.py)
@dataclass
class Sequence:
    agent: str
    actions: List[int]  # 支持多个动作ID
    meta: Dict[str, Any] = field(default_factory=dict)
```

**关键实现** (`envs/v5_0/city_env.py:334-346`):
```python
def _execute_agent_sequence(self, agent: str, sequence: Sequence) -> Tuple[float, Dict[str, float]]:
    """执行单个agent的序列"""
    reward = 0.0
    reward_terms = {}
    
    if sequence and sequence.actions:
        for action_id in sequence.actions:  # 遍历多个动作
            action_reward, action_terms = self._execute_action(agent, action_id)
            reward += action_reward
            reward_terms.update(action_terms)
    
    return reward, reward_terms
```

### 2.2 多动作约束机制
**通过配置限制每轮动作数量**：

```json
"constraints": {
    "max_actions_per_turn": 1,  // EDU/COUNCIL: 1个动作
    "max_actions_per_turn": 2,  // IND: 2个动作
}
```

### 2.3 无STOP动作设计
**v5.0 没有显式STOP动作**，通过以下机制控制：
- **预算耗尽**: 自动停止选择
- **候选为空**: 无可用动作时停止
- **时间限制**: 达到最大月份时结束

---

## 3. 动作掩码逻辑

### 3.1 掩码应用阶段
**掩码在网络输出后、采样前应用**：

```python
# 关键实现 (agents/ppo_terrain_agent.py:139-147)
def get_action(self, obs: Dict) -> Tuple[int, torch.Tensor, torch.Tensor]:
    with torch.no_grad():
        action_logits, value = self.network(state_features, terrain_features)
        
        # 应用动作掩膜 - 在采样前应用
        action_mask = torch.FloatTensor(obs['action_mask']).to(self.device)
        masked_logits = action_logits - (1 - action_mask) * 1e8
        
        # 采样动作
        action_probs = F.softmax(masked_logits, dim=-1)
        action_dist = torch.distributions.Categorical(action_probs)
        action = action_dist.sample()
```

### 3.2 掩码维度与动态变化
**掩码维度固定为9，但内容动态变化**：

```python
# 掩码生成逻辑 (envs/grid_nav_env.py:156-161)
def get_action_mask(self) -> np.ndarray:
    """获取动作掩膜"""
    mask = np.zeros(4, dtype=np.int32)  # 固定维度
    for a in range(4):
        mask[a] = 1 if self._legal(self.pos, a) else 0  # 动态内容
    return mask
```

### 3.3 独立掩码设计
**v5.0 使用统一的动作掩码**，不是针对"点"和"动作类型"的独立掩码：

```python
# 统一掩码实现 (solvers/v5_0/rl_selector.py:110-117)
def _masked_logits(self, agent: str, state: EnvironmentState, allowed_ids: List[int]) -> torch.Tensor:
    obs = self._encode_state(state)
    logits = self.actor_networks[agent](obs)
    mask = torch.full((self.action_size,), float('-inf'), device=self.device)
    if allowed_ids:
        mask[allowed_ids] = 0.0  # 只允许特定动作ID
    masked_logits = logits + mask
    return masked_logits
```

---

## 4. 环境执行部分

### 4.1 动作传递格式
**v5.0 使用动作列表（Sequence）作为输入**：

```python
# 环境执行接口 (envs/v5_0/city_env.py:221-263)
def step(self, agent: str, sequence: Optional[Sequence]) -> Tuple[EnvironmentState, float, bool, Dict[str, Any]]:
    """执行一步"""
    if sequence and sequence.actions:
        for action_id in sequence.actions:  # 处理动作列表
            action_reward, action_terms = self._execute_action(agent, action_id)
            reward += action_reward
            reward_terms.update(action_terms)
```

### 4.2 多动作聚合机制
**奖励和状态聚合**：

```python
# 奖励聚合 (envs/v5_0/city_env.py:334-346)
def _execute_agent_sequence(self, agent: str, sequence: Sequence) -> Tuple[float, Dict[str, float]]:
    reward = 0.0
    reward_terms = {}
    
    if sequence and sequence.actions:
        for action_id in sequence.actions:
            action_reward, action_terms = self._execute_action(agent, action_id)
            reward += action_reward  # 累加奖励
            reward_terms.update(action_terms)  # 合并奖励项
    
    return reward, reward_terms
```

### 4.3 状态更新机制
**槽位占用动态更新**：

```python
# 槽位更新 (envs/v5_0/city_env.py:348-359)
def _update_occupied_slots_from_snapshot(self, agent: str, sequence: Sequence) -> None:
    """使用候选快照更新已占用槽位"""
    if not sequence or not sequence.actions:
        return
    for action_id in sequence.actions:
        cand = self._get_candidate_from_snapshot(agent, action_id)
        if not cand:
            continue
        for slot_id in cand.meta.get("slots", []):
            self.occupied_slots.add(slot_id)  # 动态更新占用状态
```

---

## 5. 核心架构特点

### 5.1 动作空间设计
- **单一离散动作ID**: 9个固定动作，按智能体分组
- **直接映射**: 动作ID直接对应建筑类型和尺寸
- **配置驱动**: 所有动作参数通过配置文件定义

### 5.2 多动作支持
- **序列化选择**: 支持每轮选择1-2个动作的序列
- **约束机制**: 通过配置限制每轮动作数量
- **无STOP动作**: 通过预算、候选、时间等机制自然停止

### 5.3 掩码机制
- **动态掩码**: 基于预算、槽位、时间等约束的动态掩码
- **统一设计**: 不是独立的"点×动作类型"掩码
- **采样前应用**: 在网络输出后、采样前应用掩码

### 5.4 环境执行
- **列表输入**: 环境接受Sequence对象，包含动作列表
- **累加聚合**: 多动作的奖励和状态变化累加处理
- **动态更新**: 槽位占用状态实时更新

---

## 6. 技术优势

1. **简洁性**: 单一离散动作ID设计，易于理解和实现
2. **灵活性**: 支持序列化多动作选择，适应复杂场景
3. **可扩展性**: 配置驱动的设计，易于添加新动作类型
4. **高效性**: 动态掩码机制，避免无效动作选择
5. **一致性**: 统一的动作处理流程，降低系统复杂度

---

**分析完成时间**: 2024年10月22日  
**技术栈**: 基于 v5.0 源码的深度分析  
**数据来源**: `solvers/v5_0/rl_selector.py`, `envs/v5_0/city_env.py`, `logic/v5_enumeration.py`, `configs/city_config_v5_0.json`
