# v5.1 å¤šåŠ¨ä½œé‡‡æ ·æœºåˆ¶ï¼ˆMulti-Action Sampling Logicï¼‰

**ç”Ÿæˆæ—¶é—´**: 2025-10-22  
**ç‰ˆæœ¬**: v5.1.0  
**åˆ†æèŒƒå›´**: PPO æ¡†æ¶ä¸‹çš„å¤šåŠ¨ä½œé‡‡æ ·ä¸è®­ç»ƒå…¼å®¹æ€§

---

## 1. èƒŒæ™¯

v5.0 æ¡†æ¶ä¸­æ¯ä¸ªæ—¶é—´æ­¥ï¼ˆstepï¼‰ä»…å…è®¸æ‰§è¡Œ 1 ä¸ªç¦»æ•£åŠ¨ä½œï¼Œéš¾ä»¥è¡¨è¾¾å¤æ‚å†³ç­–ç»„åˆã€‚  
v5.1 å¼•å…¥ **å¤šåŠ¨ä½œé‡‡æ ·æœºåˆ¶ï¼ˆMulti-Action Samplingï¼‰**ï¼Œå…è®¸æ™ºèƒ½ä½“åœ¨å•æ­¥å†…æ‰§è¡Œ **1~5 ä¸ªäº’ä¸é‡å¤åŠ¨ä½œ**ï¼Œä»¥å¢å¼ºç­–ç•¥çµæ´»æ€§ä¸è¡¨è¾¾èƒ½åŠ›ã€‚

---

## 2. æ¶æ„å‡çº§æ¦‚è¿°

### 2.1 ä¸»è¦æ”¹åŠ¨æ¨¡å—

| æ¨¡å— | æ”¹åŠ¨å†…å®¹ |
|------|-----------|
| `env.step()` | æ¥å—åŠ¨ä½œé›†åˆï¼ˆlist / setï¼‰ï¼›å†…éƒ¨ä¾æ¬¡æ‰§è¡Œå¹¶èšåˆ reward / next_state |
| `get_action_mask()` | åŠ¨æ€æ›´æ–°æ©ç ï¼Œé˜²æ­¢é‡å¤åŠ¨ä½œ |
| `rollout buffer` | å¢åŠ  `action_set`, `logprob_sum`, `entropy_sum` å­—æ®µ |
| `V5PPOTrainer` | æ”¯æŒè”åˆ logprob è®¡ç®—ï¼ˆå­åŠ¨ä½œ logprob æ±‚å’Œï¼‰ |

---

## 3. å¤šåŠ¨ä½œé‡‡æ ·é€»è¾‘

v5.1 é‡‡ç”¨ **è‡ªå›å½’é‡‡æ · + STOP åŠ¨ä½œ** çš„æ–¹æ¡ˆï¼Œæ¯æ­¥æœ€å¤šæ‰§è¡Œ 5 ä¸ªåŠ¨ä½œï¼Œæ•°é‡åŠ¨æ€å†³å®šã€‚

### 3.1 é‡‡æ ·ä¼ªä»£ç 

```python
def sample_action_set(policy, obs, mask, max_k=5):
    """åœ¨å½“å‰çŠ¶æ€ obs ä¸‹é‡‡æ · 1~max_k ä¸ªä¸é‡å¤åŠ¨ä½œ"""
    selected, logprobs, entropies = [], [], []

    for i in range(max_k):
        logits = policy(obs)
        logits[mask == 0] = -float("inf")

        # åŠ ä¸Š STOP åŠ¨ä½œ
        stop_logit = torch.tensor([0.0])
        logits = torch.cat([logits, stop_logit])
        probs = torch.softmax(logits, dim=-1)

        action = torch.multinomial(probs, 1)
        if action == len(probs) - 1:  # STOP
            break

        logprobs.append(torch.log(probs[action]))
        entropies.append(-torch.sum(probs * torch.log(probs + 1e-8)))
        selected.append(action.item())
        mask[action] = 0  # ä¸é‡å¤

    logprob_sum = torch.stack(logprobs).sum()
    entropy_sum = torch.stack(entropies).sum()
    return selected, logprob_sum, entropy_sum
```

---

## 4. PPO å…¼å®¹æ€§æ”¹é€ 

### 4.1 è”åˆ logprob

PPO çš„æ¯”ç‡è®¡ç®—è°ƒæ•´ä¸ºï¼š

```python
ratio = torch.exp(logprob_sum_new - logprob_sum_old)
ppo_loss = -torch.min(ratio * adv, torch.clamp(ratio, 1 - eps, 1 + eps) * adv).mean()
```

### 4.2 ç†µé¡¹ä¸ä»·å€¼æŸå¤±

```python
entropy_loss = -entropy_coef * entropy_sum.mean()
value_loss = (returns - values).pow(2).mean()
loss = ppo_loss + value_coef * value_loss + entropy_loss
```

### 4.3 ä¼˜åŠ¿å‡½æ•°ä¸å›æŠ¥

ä»æŒ‰ã€Œå®æ­¥ã€è®¡ç®—ï¼šä¸€æ¬¡ç¯å¢ƒ step å†…çš„æ‰€æœ‰åŠ¨ä½œè¢«è§†ä¸ºä¸€ä¸ªæ•´ä½“ã€‚

---

## 5. ç¯å¢ƒä¸ç¼“å†²åŒºè°ƒæ•´

### 5.1 ç¯å¢ƒæ¥å£

```python
def step(self, action_set):
    total_reward = 0
    for a in action_set:
        obs, reward, done, info = self._apply_action(a)
        total_reward += reward
        if done:
            break
    return obs, total_reward, done, info
```

### 5.2 Rollout Buffer

æ–°å¢å­—æ®µï¼š

```python
buffer.add(
    obs=obs,
    action_set=selected,
    logprob_sum=logprob_sum,
    entropy_sum=entropy_sum,
    reward=reward,
    value=value
)
```

---

## 6. è®­ç»ƒç¨³å®šæ€§ä¸æ­£åˆ™åŒ–å»ºè®®

| ç­–ç•¥ | è¯´æ˜ |
|------|------|
| **åŠ¨ä½œæ•°æƒ©ç½š** | åœ¨å¥–åŠ±ä¸­åŠ å…¥ `-Î» * (k - 1)`ï¼Œé˜²æ­¢è´ªå¤š |
| **æ¢ç´¢æ­£åˆ™åŒ–** | ä¿ç•™ç†µé¡¹ï¼Œæœ‰åŠ©äºå¤šåŠ¨ä½œæƒ…å†µä¸‹çš„å¤šæ ·æ€§ |
| **æ©ç æ›´æ–°** | æ¯æ¬¡é‡‡æ ·åç«‹å³æ›´æ–° maskï¼Œç¡®ä¿æ— é‡å¤ |

---

## 7. ç‰ˆæœ¬è§„åˆ’

| é˜¶æ®µ | å†…å®¹ | çŠ¶æ€ |
|------|------|------|
| v5.1-alpha | ç¯å¢ƒæ”¯æŒ action setï¼ŒPPO æ”¯æŒè”åˆ logprob | âœ… |
| v5.1-beta | è®­ç»ƒç¨³å®šæ€§æµ‹è¯•ä¸ä¼˜åŠ¿å‡½æ•°æ ¡éªŒ | â³ |
| v5.2 | å¢åŠ åŠ¨ä½œæ•°æƒ©ç½šä¸å¥–åŠ±ä¿®æ­£æœºåˆ¶ | ğŸ§© |

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2025-10-22 13:55:37  
**ç”Ÿæˆå·¥å…·**: ChatGPTï¼ˆåŸºäº Cursor v5.0 æ¶æ„åˆ†æå»¶ä¼¸ï¼‰
