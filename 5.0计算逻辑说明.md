# v5.0 城市模拟系统计算逻辑说明

## 📋 目录
1. [系统架构概览](#系统架构概览)
2. [动作驱动设计](#动作驱动设计)
3. [成本计算详解](#成本计算详解)
4. [奖励计算详解](#奖励计算详解)
5. [声望计算详解](#声望计算详解)
6. [RL奖励计算机制](#rl奖励计算机制)
7. [预算系统](#预算系统)
8. [动作选择与评分](#动作选择与评分)
9. [配置参数详解](#配置参数详解)
10. [计算流程示例](#计算流程示例)

---

## 系统架构概览

### 动作驱动架构
```
┌─────────────────────────────────────────────────────────┐
│                    V5RewardCalculator                    │
│  计算动作经济指标 (cost/reward/prestige)                 │
│  位置: logic/v5_reward_calculator.py                     │
└────────────────────────┬────────────────────────────────┘
                         │
                         ↓
┌─────────────────────────────────────────────────────────┐
│                    V5CityEnvironment                    │
│  基于动作属性 → 计算RL的total_reward                     │
│  位置: envs/v5_0/city_env.py                            │
└─────────────────────────────────────────────────────────┘
```

### 主要组件
- **V5RewardCalculator**: 计算每个动作的经济指标
- **V5CityEnvironment**: 将经济指标转换为RL训练奖励
- **BudgetPoolManager**: 管理智能体的财务约束
- **GaussianLandPriceSystem**: 提供地价信息用于成本计算
- **动态计算器**: 地价、邻近性、河流、规模奖励计算

---

## 动作驱动设计

### 动作ID映射
```json
{
    "EDU": {
        "0": "EDU_S",    // 小型教育建筑
        "1": "EDU_M",    // 中型教育建筑
        "2": "EDU_L"     // 大型教育建筑
    },
    "IND": {
        "3": "IND_S",    // 小型工业建筑
        "4": "IND_M",    // 中型工业建筑
        "5": "IND_L"     // 大型工业建筑
    },
    "COUNCIL": {
        "6": "COUNCIL_A", // 市政A型建筑
        "7": "COUNCIL_B", // 市政B型建筑
        "8": "COUNCIL_C"  // 市政C型建筑
    }
}
```

### 动作参数结构
每个动作包含完整的经济参数：
```json
{
    "action_params": {
        "3": {
            "desc": "IND_S",
            "base_cost": 900,        // 基础建造成本
            "base_reward": 180,      // 基础月收入
            "opex": 100,             // 运营成本
            "rent": 25,              // 租金
            "prestige_base": 0.2,    // 基础声望
            "pollution": 0.6,        // 污染值
            "capacity": 1.0,         // 容量系数
            "land_price_k": 0.25,    // 地价敏感度系数
            "river_pct": 20.0,       // 河流溢价百分比
            "size_bonus": 0          // 规模奖励
        }
    }
}
```

---

## 成本计算详解

### 基础公式

**所有动作**:
```
cost = base_cost + zone_cost + land_price_cost
```

### 参数详解

#### 基础建造成本
```json
{
    "0": {"base_cost": 700},   // EDU_S
    "1": {"base_cost": 1200},  // EDU_M
    "2": {"base_cost": 1900},  // EDU_L
    "3": {"base_cost": 900},   // IND_S
    "4": {"base_cost": 1500},  // IND_M
    "5": {"base_cost": 2400},  // IND_L
    "6": {"base_cost": 570},   // COUNCIL_A
    "7": {"base_cost": 870},   // COUNCIL_B
    "8": {"base_cost": 1150}    // COUNCIL_C
}
```

#### 区位附加成本
```json
ZC = {
    "near": 200,  // 靠近枢纽区域
    "mid": 100,   // 中等距离区域
    "far": 0      // 远离枢纽区域
}
```

#### 地价成本
```
land_price_cost = land_price_norm × 1000

其中:
- land_price_norm: 0.0~1.0 (从GaussianLandPriceSystem获取)
- land_price_cost范围: 0~1000 kGBP
```

### 成本计算示例

**IND_S动作** (zone=near, land_price_norm=0.8):
```
base_cost = 900
zone_cost = 200
land_price_cost = 0.8 × 1000 = 800
total_cost = 900 + 200 + 800 = 1900 kGBP
```

---

## 奖励计算详解

### 基础公式

**所有动作**:
```
reward_base = base_reward + zone_reward + adj_bonus - opex - rent + river_premium
reward = reward_base × (1 + land_price_k × land_price_norm)
reward += proximity_bonus + size_bonus
```

### 参数详解

#### 基础月收入
```json
{
    "0": {"base_reward": 140},  // EDU_S
    "1": {"base_reward": 260},  // EDU_M
    "2": {"base_reward": 420},  // EDU_L
    "3": {"base_reward": 180},  // IND_S
    "4": {"base_reward": 320},  // IND_M
    "5": {"base_reward": 520},  // IND_L
    "6": {"base_reward": 570},  // COUNCIL_A
    "7": {"base_reward": 870},  // COUNCIL_B
    "8": {"base_reward": 1150}  // COUNCIL_C
}
```

#### 区位收入加成
```json
ZR = {
    "near": 80,  // 靠近枢纽区域
    "mid": 40,   // 中等距离区域
    "far": 0     // 远离枢纽区域
}
```

#### 运营成本
```json
{
    "0": {"opex": 70},   // EDU_S
    "1": {"opex": 120},  // EDU_M
    "2": {"opex": 190},  // EDU_L
    "3": {"opex": 100},  // IND_S
    "4": {"opex": 180},  // IND_M
    "5": {"opex": 300},  // IND_L
    "6": {"opex": 50},   // COUNCIL_A
    "7": {"opex": 80},   // COUNCIL_B
    "8": {"opex": 120}   // COUNCIL_C
}
```

#### 租金机制
```json
Rent = {
    "S": 25,    // 小型建筑租金
    "M": 45,    // 中型建筑租金
    "L": 70     // 大型建筑租金
}
```

**租金逻辑**:
- IND建筑: 支付租金 (reward -= Rent)
- EDU建筑: 获得租金 (reward += Rent)
- COUNCIL建筑: 无租金影响

#### 地价敏感度系数
```json
land_price_k = {
    "0": 0.10,  // EDU_S
    "1": 0.10,  // EDU_M
    "2": 0.10,  // EDU_L
    "3": 0.25,  // IND_S
    "4": 0.25,  // IND_M
    "5": 0.25,  // IND_L
    "6": 0.15,  // COUNCIL_A
    "7": 0.15,  // COUNCIL_B
    "8": 0.15   // COUNCIL_C
}
```

### 河流溢价机制

```
decay = 2^(-river_dist / RiverD_half_m)
BaseForPremium = base_reward + zone_reward
RawRiverPrem = BaseForPremium × river_pct/100 × decay
RiverPremium = clamp(round_nearest(RawRiverPrem), 0, RiverPremiumCap)
```

**参数**:
- `river_pct`: 动作特定值
- `RiverD_half_m`: 120米 (半衰距离)
- `RiverPremiumCap`: 10000 kGBP (封顶值)

### 邻近性奖励/惩罚

**邻近奖励** (距离 ≤ 10单位):
```
proximity_bonus = proximity_reward_val × (1.0 - min_dist / proximity_threshold)
```

**距离惩罚** (距离 > 10单位):
```
distance_penalty = (min_dist - proximity_threshold) × distance_penalty_coef
```

**参数**:
- `proximity_threshold`: 10.0
- `proximity_reward_val`: 50.0
- `distance_penalty_coef`: 2.0

### 规模奖励 (Size Bonus)

```json
size_bonus = {
    "1": 1000,  // EDU_M
    "2": 2000,  // EDU_L
    "4": 1000,  // IND_M
    "5": 2000,  // IND_L
    "7": 1000,  // COUNCIL_B
    "8": 2000   // COUNCIL_C
}
```

### 收益计算示例

**IND_S动作** (zone=near, land_price_norm=0.8, 距离河流30米):
```
base_reward = 180
zone_reward = 80
opex = 100
rent = 25
river_premium = 61

reward_base = 180 + 80 + 0 - 100 - 25 + 61 = 196
reward = 196 × (1 + 0.25 × 0.8) = 196 × 1.2 = 235
proximity_bonus = 50
size_bonus = 0
total_reward = 235 + 50 + 0 = 285 kGBP/月
```

---

## 声望计算详解

### 基础公式

**所有动作**:
```
prestige = prestige_base + zone_bonus + adj_bonus - pollution_penalty
```

### 参数详解

#### 基础声望值
```json
{
    "0": {"prestige_base": 0.2},   // EDU_S
    "1": {"prestige_base": 0.6},   // EDU_M
    "2": {"prestige_base": 1.0},  // EDU_L
    "3": {"prestige_base": 0.2},   // IND_S
    "4": {"prestige_base": 0.1},   // IND_M
    "5": {"prestige_base": -0.1},  // IND_L
    "6": {"prestige_base": 0.3},   // COUNCIL_A
    "7": {"prestige_base": 0.7},   // COUNCIL_B
    "8": {"prestige_base": 1.2}    // COUNCIL_C
}
```

#### 区位和邻接加成
- `zone_bonus`: 靠近枢纽区域时 +1.0
- `adj_bonus`: 有邻接建筑时 +1.0

#### 污染惩罚
```json
pollution = {
    "0": 0.2,   // EDU_S
    "1": 0.4,   // EDU_M
    "2": 0.6,   // EDU_L
    "3": 0.6,   // IND_S
    "4": 0.9,   // IND_M
    "5": 1.2,   // IND_L
    "6": 0.1,   // COUNCIL_A
    "7": 0.2,   // COUNCIL_B
    "8": 0.3    // COUNCIL_C
}
```

### 声望计算示例

**EDU_L动作** (near区域, 有邻接):
```
prestige_base = 1.0
zone_bonus = 1.0
adj_bonus = 1.0
pollution_penalty = 0.6
prestige = 1.0 + 1.0 + 1.0 - 0.6 = 2.4
```

---

## RL奖励计算机制

### 计算流程 (`envs/v5_0/city_env.py`)

```python
def _calculate_reward(self, agent, action):
    """计算奖励（固定NPV机制）"""
    # 1. 计算建造成本
    build_cost = float(action.cost) if action.cost is not None else 0.0
    
    if build_cost > 0:  # 有建造
        # 2. 计算未来收益（固定回报期）
        expected_lifetime = 12  # 预期生命周期(月)
        monthly_reward = float(action.reward) if action.reward is not None else 0.0
        future_income = monthly_reward * expected_lifetime
        
        # 3. NPV = 未来收益 - 成本
        npv = future_income - build_cost
        
        # 4. 进度奖励
        progress_reward = len(self.buildings) * 0.5
        
        # 5. 协作奖励（可配置）
        cooperation_bonus = self._calculate_cooperation_reward(agent, action)
        
        # 6. Budget惩罚（软约束）
        budget_penalty = self._calculate_budget_penalty(agent, action)
        
        # 7. 总奖励 = NPV + 进度 + 协作 - 惩罚
        total_reward = npv + progress_reward + cooperation_bonus - budget_penalty
    else:
        # 空序列（不建造）：无reward
        total_reward = 0.0
    
    # 8. 奖励缩放
    scaled_reward = total_reward / reward_scale  # 默认30000
    scaled_reward = np.clip(scaled_reward, -1.0, 1.0)
    
    return scaled_reward
```

### 协作奖励机制

```python
def _calculate_cooperation_reward(self, agent, action):
    cooperation_lambda = 0.2  # 协作权重
    
    cooperation_bonus = 0.0
    
    # 功能互补奖励
    if agent == 'EDU':
        cooperation_bonus += len(industrial_buildings) * 0.05
    elif agent == 'IND':
        cooperation_bonus += len(public_buildings) * 0.05
    
    # 空间协调奖励
    for other_building in all_buildings:
        distance = calculate_distance(action, other_building)
        if 5 <= distance <= 20:
            cooperation_bonus += 0.02
    
    return cooperation_lambda * cooperation_bonus
```

### 月度收益累积机制

```python
def _calculate_monthly_income(self, agent):
    """计算agent的月度收益（所有在营建筑的累加）"""
    total_income = sum([
        asset['monthly_income'] 
        for asset in self.active_assets[agent]
    ])
    return total_income

def _advance_turn(self):
    """每月开始时，为所有agent累加月度收益到budget"""
    # 【月度收益机制】每月开始时，为所有agent累加月度收益到budget
    if self.budgets is not None:
        for agent in self.rl_cfg['agents']:
            monthly_income = self._calculate_monthly_income(agent)
            self.budgets[agent] += monthly_income  # 每月真实累积收益
            
            # 记录月度收益历史
            self.monthly_income_history[agent].append(monthly_income)

def _place_building(self, agent, action):
    """放置建筑时记录为在营资产"""
    # 【月度收益机制】记录为在营资产
    asset = {
        'action_id': action.id,
        'monthly_income': float(action.reward),  # 存储月度收益
        'cost': float(action.cost),
        'built_month': self.current_month,
    }
    self.active_assets[agent].append(asset)  # 添加到在营资产列表
```

### 双重收益机制详解

系统设计了**双重收益机制**来平衡RL训练和财务约束：

#### 1. **RL奖励层面（NPV机制）**
```python
# RL看到的是长期价值
expected_lifetime = 12  # 12个月预期生命周期
future_income = monthly_reward * expected_lifetime
npv = future_income - build_cost

# 示例：IND_S动作
# npv = 285 * 12 - 1900 = 3420 - 1900 = 1520 kGBP (正收益)
```

**作用**：给RL智能体正确的"投资回报"信号，鼓励长期投资行为。

#### 2. **Budget系统层面（现金流管理）**
```python
# 每月真实累积收益
def _advance_turn(self):
    for agent in agents:
        monthly_income = sum([asset.monthly_income for asset in active_assets])
        budget[agent] += monthly_income

# 示例：IND_S动作的Budget变化
# Month 0: budget = 15000 - 1900 = 13100  # 建造成本
# Month 1: budget = 13100 + 285 = 13385   # 第1个月收益
# Month 2: budget = 13385 + 285 = 13670   # 第2个月收益
# ...
# Month 12: budget = 15000 + 1520 = 16520 # 12个月后净收益1520
```

**作用**：提供真实的现金流管理，防止过度借贷，确保财务可持续性。

#### 3. **两者协调性**
- **设计一致性**：12个月后Budget增长与NPV计算完全一致
- **功能互补**：RL负责长期策略，Budget负责短期约束
- **避免矛盾**：不存在"RL说赚钱但Budget说亏钱"的情况

---

## 预算系统

### 配置参数

```json
"budget_system": {
    "enabled": true,
    "mode": "soft_constraint",
    "initial_budgets": {
        "IND": 15000,   // 工业智能体初始预算
        "EDU": 20000,   // 教育智能体初始预算
        "COUNCIL": 20000 // 市政智能体初始预算
    },
    "debt_penalty_coef": 0.1,        // 负债惩罚系数
    "max_debt": -2000,               // 最大允许负债
    "bankruptcy_threshold": -5000,   // 破产阈值
    "bankruptcy_penalty": -100.0     // 破产惩罚
}
```

### Budget惩罚计算

```python
def _calculate_budget_penalty(self, agent, action):
    if self.budgets is None:
        return 0.0
    
    # 预估建造后的budget
    budget_after = self.budgets[agent] - action.cost
    
    budget_penalty = 0.0
    
    # 负债惩罚
    if budget_after < 0:
        debt_penalty_coef = 0.1
        budget_penalty = abs(budget_after) * debt_penalty_coef
    
    # 破产惩罚
    if budget_after < -5000:
        budget_penalty += 100.0
    
    return budget_penalty
```

### Budget更新机制

```python
def step(self, action):
    # 1. 建造时扣除成本
    if action.build:
        self.budgets[agent] -= action.cost
    
    # 2. 每月累加月度收益
    monthly_income = self._calculate_monthly_income(agent)
    self.budgets[agent] += monthly_income
    
    # 3. 记录budget历史
    self.budget_history[agent].append(self.budgets[agent])
```

---

## 动作选择与评分

### V5ActionScorer评分机制

```python
def score_actions(self, actions):
    # 1. 计算原始cost/reward/prestige
    for action in actions:
        self._calc_crp(action)
    
    # 2. 归一化各维度
    costs = [a.cost for a in actions]
    rewards = [a.reward for a in actions]
    prestiges = [a.prestige for a in actions]
    
    c_min, c_max = min(costs), max(costs)
    r_min, r_max = min(rewards), max(rewards)
    p_min, p_max = min(prestiges), max(prestiges)
    
    def norm(v, lo, hi):
        return (v - lo) / (hi - lo) if hi - lo > 1e-9 else 0.5
    
    # 3. 按智能体使用不同权重计算综合评分
    for action in actions:
        w = self.objective.get(action.agent, {"w_r": 0.5, "w_p": 0.3, "w_c": 0.2})
        
        nr = norm(action.reward, r_min, r_max)
        np = norm(action.prestige, p_min, p_max)
        nc = norm(action.cost, c_min, c_max)
        
        action.score = w["w_r"] * nr + w["w_p"] * np - w["w_c"] * nc
    
    return actions
```

### 智能体权重配置

```json
"objective": {
    "EDU": {
        "w_r": 0.45,  // reward权重
        "w_p": 0.45,  // prestige权重  
        "w_c": 0.1    // cost权重
    },
    "IND": {
        "w_r": 0.6,   // reward权重
        "w_p": 0.2,   // prestige权重
        "w_c": 0.2    // cost权重
    },
    "COUNCIL": {
        "w_r": 0.4,   // reward权重
        "w_p": 0.5,   // prestige权重
        "w_c": 0.1    // cost权重
    }
}
```

### 动作选择策略

1. **枚举阶段**: 生成所有可能的建筑动作
2. **评分阶段**: 使用V5ActionScorer计算综合评分
3. **排序阶段**: 按评分降序排列
4. **选择阶段**: RL智能体从排序后的动作中选择

---

## 配置参数详解

### 核心配置结构

```json
{
    "simulation": {
        "total_months": 30
    },
    "city": {
        "map_size": [200, 200],
        "transport_hubs": [[122, 80], [112, 121]]
    },
    "budget_system": {
        "enabled": true,
        "initial_budgets": {"IND": 15000, "EDU": 20000, "COUNCIL": 20000},
        "debt_penalty_coef": 0.1,
        "bankruptcy_threshold": -5000
    },
    "action_params": {
        "0": {"base_cost": 700, "base_reward": 140, "opex": 70, "rent": 25, "prestige_base": 0.2, "pollution": 0.2, "land_price_k": 0.10, "river_pct": 15.0, "size_bonus": 0},
        "1": {"base_cost": 1200, "base_reward": 260, "opex": 120, "rent": 45, "prestige_base": 0.6, "pollution": 0.4, "land_price_k": 0.10, "river_pct": 15.0, "size_bonus": 1000},
        "2": {"base_cost": 1900, "base_reward": 420, "opex": 190, "rent": 70, "prestige_base": 1.0, "pollution": 0.6, "land_price_k": 0.10, "river_pct": 15.0, "size_bonus": 2000},
        "3": {"base_cost": 900, "base_reward": 180, "opex": 100, "rent": 25, "prestige_base": 0.2, "pollution": 0.6, "land_price_k": 0.25, "river_pct": 20.0, "size_bonus": 0},
        "4": {"base_cost": 1500, "base_reward": 320, "opex": 180, "rent": 45, "prestige_base": 0.1, "pollution": 0.9, "land_price_k": 0.25, "river_pct": 20.0, "size_bonus": 1000},
        "5": {"base_cost": 2400, "base_reward": 520, "opex": 300, "rent": 70, "prestige_base": -0.1, "pollution": 1.2, "land_price_k": 0.25, "river_pct": 20.0, "size_bonus": 2000},
        "6": {"base_cost": 570, "base_reward": 570, "opex": 50, "rent": 0, "prestige_base": 0.3, "pollution": 0.1, "land_price_k": 0.15, "river_pct": 10.0, "size_bonus": 0},
        "7": {"base_cost": 870, "base_reward": 870, "opex": 80, "rent": 0, "prestige_base": 0.7, "pollution": 0.2, "land_price_k": 0.15, "river_pct": 10.0, "size_bonus": 1000},
        "8": {"base_cost": 1150, "base_reward": 1150, "opex": 120, "rent": 0, "prestige_base": 1.2, "pollution": 0.3, "land_price_k": 0.15, "river_pct": 10.0, "size_bonus": 2000}
    },
    "reward_terms": {
        "enabled": true,
        "calculation_mode": "full",
        "components": {
            "land_price": true,
            "proximity": true,
            "river": true,
            "size_bonus": true
        },
        "land_price": {
            "LP_idx_min": 10,
            "LP_idx_max": 100,
            "LandPriceBase": 11.0,
            "RewardLP_k": {"IND": 0.25, "EDU": 0.10, "COUNCIL": 0.15}
        },
        "proximity": {
            "proximity_threshold": 10.0,
            "proximity_reward": 50.0,
            "distance_penalty_coef": 2.0
        },
        "river": {
            "RiverD_half_m": 120.0,
            "RiverPremiumCap": 10000.0
        }
    },
    "solver": {
        "rl": {
            "reward_scale": 30000.0,
            "reward_clip": 1.0,
            "cooperation_lambda": 0.0
        }
    }
}
```

---

## 计算流程示例

### 完整计算示例

假设要在位置(100, 80)建造一个IND_S动作 (action_id=3):

#### 1. 输入参数
- agent: "IND"
- action_id: 3
- zone: "near" (距离hub1 < 15单位)
- land_price_norm: 0.8 (地价归一化值)
- adj: 1 (有邻接建筑)
- river_dist_m: 30 (距离河流30米)

#### 2. 成本计算
```
base_cost = 900
zone_cost = 200  # near区域
land_price_cost = 0.8 × 1000 = 800
total_cost = 900 + 200 + 800 = 1900 kGBP
```

#### 3. 奖励计算
```
base_reward = 180
zone_reward = 80
opex = 100
rent = 25
river_premium = 61  # 河流溢价计算

reward_base = 180 + 80 + 0 - 100 - 25 + 61 = 196
reward = 196 × (1 + 0.25 × 0.8) = 196 × 1.2 = 235
proximity_bonus = 50  # 邻近性奖励
size_bonus = 0
total_reward = 235 + 50 + 0 = 285 kGBP/月
```

#### 4. 声望计算
```
prestige_base = 0.2
zone_bonus = 1.0
adj_bonus = 1.0
pollution_penalty = 0.6
prestige = 0.2 + 1.0 + 1.0 - 0.6 = 1.6
```

#### 5. V5ActionScorer评分
```
# 假设当前批次中:
# costs: [1000, 1500, 1900, 2500]
# rewards: [100, 200, 285, 400]  
# prestiges: [0.5, 1.0, 1.6, 2.0]

nr = (285-100)/(400-100) = 0.62
np = (1.6-0.5)/(2.0-0.5) = 0.73
nc = (1900-1000)/(2500-1000) = 0.60

score = 0.6 × 0.62 + 0.2 × 0.73 - 0.2 × 0.60 = 0.50
```

#### 6. RL奖励计算
```
# NPV计算 (12个月预期收益)
future_income = 285 × 12 = 3420 kGBP
npv = 3420 - 1900 = 1520 kGBP

progress_reward = 5 × 0.5 = 2.5
cooperation_bonus = 0 (默认禁用)
budget_penalty = 0 (假设budget充足)

total_reward = 1520 + 2.5 + 0 - 0 = 1522.5
scaled_reward = 1522.5 / 30000 = 0.051
```

#### 7. Budget更新
```
# 建造时
budget_IND = 15000 - 1900 = 13100

# 每月收益
budget_IND = 13100 + 285 = 13385
```

---

## 总结

### 系统特点
1. **动作驱动架构**: 基于action_id而非size进行参数查找
2. **完整经济建模**: 包含成本、奖励、声望的完整计算
3. **动态影响因子**: 地价、河流、邻近性、区位等动态影响
4. **双重收益机制**: RL使用NPV考虑长期价值，Budget系统管理真实现金流
5. **月度收益机制**: 建筑建成后持续产生收益，每月真实累积到Budget
6. **软约束Budget**: 允许适度负债，避免过度保守

### 关键设计理念
- **经济合理性**: 成本收益计算基于真实经济逻辑
- **可持续性**: 月度收益确保长期盈利
- **协作性**: 多智能体间的功能互补和空间协调
- **可配置性**: 丰富的参数配置支持不同场景

### 系统优势
1. **双重收益机制**: RL使用NPV考虑长期价值，Budget系统管理真实现金流
2. **合理的经济逻辑**: 12个月预期生命周期与月度收益累积机制协调一致
3. **智能体差异化**: 不同智能体有不同的权重和参数设置
4. **软约束Budget**: 允许适度负债，避免过度保守，同时防止破产

### 当前可能的问题
1. **参数调优**: expected_lifetime、reward_scale等参数可能需要根据实际训练效果调整
2. **协作机制较弱**: cooperation_lambda=0.0默认禁用，多智能体协作信号微弱
3. **训练超参数**: 学习率、clip_eps等可能需要针对具体场景优化

### 系统设计合理性
- **RL奖励机制**: 使用NPV给RL正确的"投资回报"信号，鼓励长期投资
- **Budget系统**: 每月真实累积收益，提供财务约束，防止过度借贷
- **两者协调**: 12个月后Budget增长与NPV计算一致，设计合理

这个系统为城市规划和多智能体决策提供了一个完整的计算框架，通过精细的经济建模和奖励设计来指导智能体的决策行为。核心逻辑设计合理，问题主要在于参数调优和训练配置。
