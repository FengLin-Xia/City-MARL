# v4.1 vs v5.0 训练策略对比分析

## 概述
本文档对比分析了v4.1和v5.0版本在训练阶段使用的不同策略选择方法，以及它们对训练效率和收敛性的影响。

## 策略对比

### v4.1训练策略：ε-贪婪探索
```python
# v4.1：贪婪探索（高探索性）
epsilon_roll = np.random.random()
if epsilon_roll < 0.9:  # 90%概率
    selected_idx = np.random.randint(0, num_actions)  # 完全随机选择
else:  # 10%概率
    selected_idx = dist.sample().item()  # 策略采样
```

**特点**:
- **90%随机探索**: 大量尝试无效动作
- **10%策略利用**: 少量使用训练好的策略
- **高探索性**: 容易发现新策略
- **低效率**: 大部分时间浪费在随机选择上

### v5.0训练策略：概率采样
```python
# v5.0：概率采样（策略导向）
sel = self.selector.select_action_multi(agent, candidates, cand_idx, state, 
                                       max_k=max_actions, greedy=False)  # 概率采样
```

**特点**:
- **100%策略导向**: 总是使用训练好的策略网络
- **高效率**: 选择更有希望的动作
- **学习集中**: 所有时间都在学习
- **收敛快**: 更快找到好的策略

## 性能对比

### 训练速度
| 版本 | 策略 | 探索率 | 利用率 | 训练速度 | 学习效率 |
|------|------|--------|--------|----------|----------|
| v4.1 | ε-贪婪探索 | 90% | 10% | 慢 | 低 |
| v5.0 | 概率采样 | 0% | 100% | 快 | 高 |

### 收敛性分析
| 版本 | 探索性 | 利用性 | 局部最优风险 | 全局最优发现能力 |
|------|--------|--------|--------------|------------------|
| v4.1 | 强 | 弱 | 低 | 高 |
| v5.0 | 弱 | 强 | 高 | 低 |

## 问题分析

### v5.0的潜在问题
1. **局部最优风险**: 缺乏随机探索，容易陷入局部最优解
2. **策略固化**: 容易强化已有策略，忽略新策略
3. **探索-利用失衡**: 过度偏向利用，缺乏探索

### v4.1的问题
1. **学习效率低**: 90%时间在随机选择
2. **收敛慢**: 需要更多episode才能学会
3. **资源浪费**: 大量无效动作尝试

## 改进建议

### 为v5.0添加探索机制
1. **ε-贪婪探索**: 添加10%随机探索
2. **温度采样**: 使用温度参数增加随机性
3. **课程学习**: 动态调整探索率
4. **多策略训练**: 并行训练多个策略

### 探索率建议
- **训练早期**: 高探索率（0.3）
- **训练中期**: 中等探索率（0.1）
- **训练后期**: 低探索率（0.05）

## 实现方案

### 方案1: 添加ε-贪婪探索
```python
def select_action_with_exploration(self, agent, candidates, state, epsilon=0.1):
    if random() < epsilon:  # 10%探索
        action = random.choice(candidates)
    else:  # 90%利用
        action = self.policy.sample(candidates, state)
    return action
```

### 方案2: 温度采样
```python
def temperature_sampling(self, logits, temperature=1.0):
    scaled_logits = logits / temperature
    probs = F.softmax(scaled_logits, dim=-1)
    return torch.multinomial(probs, 1)
```

### 方案3: 课程学习
```python
def curriculum_exploration(self, episode):
    if episode < 1000:
        epsilon = 0.3  # 早期高探索
    elif episode < 5000:
        epsilon = 0.1  # 中期中等探索
    else:
        epsilon = 0.05  # 后期低探索
    return epsilon
```

## 结论

v5.0通过使用更高效的策略选择方式实现了更快的训练速度，但存在陷入局部最优解的风险。需要在效率和探索性之间找到平衡，建议添加适当的探索机制来缓解局部最优问题。

## 后续计划
- [ ] 实现ε-贪婪探索机制
- [ ] 添加温度采样选项
- [ ] 实现课程学习策略
- [ ] 测试不同探索率的效果
- [ ] 对比改进前后的性能
