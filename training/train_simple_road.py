#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆè·¯å¾„è§„åˆ’è®­ç»ƒè„šæœ¬
"""

import sys
import os
import torch
import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Tuple
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from envs.simple_road_env import SimpleRoadEnv
from agents.simple_ppo import SimpleActorCritic, SimplePPO


class SimpleRoadTrainer:
    """ç®€åŒ–ç‰ˆè·¯å¾„è§„åˆ’è®­ç»ƒå™¨"""
    
    def __init__(self, 
                 dem_size: Tuple[int, int] = (100, 100),
                 max_steps: int = 200,
                 batch_size: int = 64,
                 num_episodes: int = 1000,
                 save_interval: int = 100):
        
        self.dem_size = dem_size
        self.max_steps = max_steps
        self.batch_size = batch_size
        self.num_episodes = num_episodes
        self.save_interval = save_interval
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = SimpleRoadEnv(dem_size=dem_size, max_steps=max_steps)
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.actor_critic = SimpleActorCritic()
        self.ppo = SimplePPO(self.actor_critic)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.success_rates = []
        self.training_losses = []
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.models_dir = Path("models")
        self.models_dir.mkdir(exist_ok=True)
        
        self.results_dir = Path("data/results")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
    def collect_episode(self) -> Tuple[List[Dict], List[torch.Tensor], List[torch.Tensor], List[float], List[float], bool]:
        """æ”¶é›†ä¸€ä¸ªepisodeçš„æ•°æ®"""
        obs, _ = self.env.reset()
        
        observations = []
        actions = []
        log_probs = []
        rewards = []
        values = []
        
        done = False
        truncated = False
        
        while not (done or truncated):
            # è½¬æ¢ä¸ºå¼ é‡
            obs_tensor = {
                'position': torch.FloatTensor(obs['position']).unsqueeze(0),
                'goal': torch.FloatTensor(obs['goal']).unsqueeze(0),
                'local_dem': torch.FloatTensor(obs['local_dem']).unsqueeze(0)
            }
            
            # è·å–åŠ¨ä½œ
            action, log_prob, value = self.actor_critic.get_action(obs_tensor)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, done, truncated, info = self.env.step(action.numpy().squeeze())
            
            # å­˜å‚¨æ•°æ®
            observations.append(obs)
            actions.append(action.squeeze())
            log_probs.append(log_prob.squeeze())
            rewards.append(reward)
            values.append(value.squeeze())
            
            obs = next_obs
        
        success = done and not truncated and info.get('reason') == 'reached_goal'
        
        return observations, actions, log_probs, rewards, values, success
    
    def train_episode(self, episode_num: int) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        start_time = time.time()
        
        # æ”¶é›†episodeæ•°æ®
        observations, actions, log_probs, rewards, values, success = self.collect_episode()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_reward = sum(rewards)
        episode_length = len(rewards)
        
        # è½¬æ¢ä¸ºå¼ é‡
        rewards_tensor = torch.FloatTensor(rewards)
        values_tensor = torch.FloatTensor([v.item() for v in values])
        dones_tensor = torch.zeros(len(rewards))
        dones_tensor[-1] = 1.0  # æœ€åä¸€æ­¥ç»“æŸ
        
        # è®¡ç®—GAE
        advantages, returns = self.ppo.compute_gae(rewards_tensor, values_tensor, dones_tensor)
        
        # æ›´æ–°ç­–ç•¥ï¼ˆç®€åŒ–ç‰ˆï¼Œåªä½¿ç”¨ç¬¬ä¸€ä¸ªè§‚æµ‹ï¼‰
        if len(observations) > 0:
            obs_tensor = {
                'position': torch.FloatTensor(observations[0]['position']).unsqueeze(0),
                'goal': torch.FloatTensor(observations[0]['goal']).unsqueeze(0),
                'local_dem': torch.FloatTensor(observations[0]['local_dem']).unsqueeze(0)
            }
            
            losses = self.ppo.update(
                obs_batch=[obs_tensor] * len(actions),
                actions_batch=actions,
                old_log_probs_batch=log_probs,
                advantages_batch=advantages,
                returns_batch=returns
            )
        else:
            losses = {'policy_loss': 0, 'value_loss': 0, 'entropy_loss': 0}
        
        # æ›´æ–°ç»Ÿè®¡
        self.episode_rewards.append(total_reward)
        self.episode_lengths.append(episode_length)
        self.success_rates.append(1.0 if success else 0.0)
        self.training_losses.append(losses)
        
        # è®¡ç®—è¿è¡Œæ—¶é—´
        episode_time = time.time() - start_time
        
        # è®¡ç®—å¹³å‡ç»Ÿè®¡
        avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)
        avg_length = np.mean(self.episode_lengths[-100:]) if len(self.episode_lengths) >= 100 else np.mean(self.episode_lengths)
        success_rate = np.mean(self.success_rates[-100:]) if len(self.success_rates) >= 100 else np.mean(self.success_rates)
        
        # æ‰“å°è¿›åº¦
        print(f"Episode {episode_num:4d} | "
              f"å¥–åŠ±: {total_reward:6.1f} | "
              f"æ­¥æ•°: {episode_length:3d} | "
              f"æˆåŠŸ: {'âœ…' if success else 'âŒ'} | "
              f"å¹³å‡å¥–åŠ±: {avg_reward:6.1f} | "
              f"æˆåŠŸç‡: {success_rate*100:5.1f}% | "
              f"ç”¨æ—¶: {episode_time:.1f}s")
        
        return {
            'episode': episode_num,
            'total_reward': total_reward,
            'episode_length': episode_length,
            'success': success,
            'avg_reward': avg_reward,
            'success_rate': success_rate,
            'losses': losses,
            'time': episode_time
        }
    
    def save_model(self, episode_num: int):
        """ä¿å­˜æ¨¡å‹"""
        model_path = self.models_dir / f"simple_road_ppo_{episode_num}.pth"
        torch.save({
            'episode': episode_num,
            'actor_critic_state_dict': self.actor_critic.state_dict(),
            'optimizer_state_dict': self.ppo.optimizer.state_dict(),
            'episode_rewards': self.episode_rewards,
            'success_rates': self.success_rates
        }, model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    def save_results(self):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        results = {
            'training_config': {
                'dem_size': self.dem_size,
                'max_steps': self.max_steps,
                'batch_size': self.batch_size,
                'num_episodes': self.num_episodes
            },
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'success_rates': self.success_rates,
            'training_losses': self.training_losses
        }
        
        results_path = self.results_dir / f"simple_road_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"ğŸ“Š è®­ç»ƒç»“æœå·²ä¿å­˜: {results_path}")
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒç®€åŒ–ç‰ˆè·¯å¾„è§„åˆ’æ™ºèƒ½ä½“")
        print(f"ğŸ“ DEMå°ºå¯¸: {self.dem_size}")
        print(f"â±ï¸ æœ€å¤§æ­¥æ•°: {self.max_steps}")
        print(f"ğŸ¯ ç›®æ ‡episodes: {self.num_episodes}")
        print("=" * 80)
        
        start_time = time.time()
        
        for episode in range(1, self.num_episodes + 1):
            # è®­ç»ƒä¸€ä¸ªepisode
            episode_info = self.train_episode(episode)
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if episode % self.save_interval == 0:
                self.save_model(episode)
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        
        print("=" * 80)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸ æ€»ç”¨æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
        print(f"ğŸ“ˆ æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(self.episode_rewards[-100:]):.2f}")
        print(f"ğŸ¯ æœ€ç»ˆæˆåŠŸç‡: {np.mean(self.success_rates[-100:])*100:.1f}%")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œç»“æœ
        self.save_model(self.num_episodes)
        self.save_results()
        
        print("ğŸ’¾ æœ€ç»ˆæ¨¡å‹å’Œç»“æœå·²ä¿å­˜ï¼")


if __name__ == "__main__":
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SimpleRoadTrainer(
        dem_size=(100, 100),
        max_steps=200,
        batch_size=64,
        num_episodes=500,  # å…ˆè®­ç»ƒ500ä¸ªepisodesæµ‹è¯•
        save_interval=50
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()

