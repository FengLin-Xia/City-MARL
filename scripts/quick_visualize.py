#!/usr/bin/env python3
"""
å¿«é€Ÿå¯è§†åŒ–è„šæœ¬ - ç›´æ¥æ˜¾ç¤ºæ™ºèƒ½ä½“è¡Œä¸º
"""

import sys
import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from envs.simple_road_env import SimpleRoadEnv
from agents.simple_ppo import SimpleActorCritic


def visualize_agent_behavior():
    """å¯è§†åŒ–æ™ºèƒ½ä½“è¡Œä¸º"""
    print("ğŸ¨ å¯è§†åŒ–æ™ºèƒ½ä½“è¡Œä¸º")
    
    # åˆ›å»ºç¯å¢ƒ
    env = SimpleRoadEnv()
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    actor_critic = SimpleActorCritic()
    
    # è®¾ç½®matplotlib
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå›¾å½¢
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    fig.suptitle('æ™ºèƒ½ä½“è¡Œä¸ºå¯è§†åŒ–', fontsize=16, fontweight='bold')
    
    # å·¦å›¾ï¼šè®­ç»ƒåçš„æ™ºèƒ½ä½“
    print("ğŸ¤– è¿è¡Œè®­ç»ƒåçš„æ™ºèƒ½ä½“...")
    obs, _ = env.reset()
    
    path_x = [obs['position'][0]]
    path_y = [obs['position'][1]]
    total_reward = 0
    success = False
    
    for step in range(200):
        # è·å–åŠ¨ä½œ
        obs_tensor = {
            'position': torch.FloatTensor(obs['position']).unsqueeze(0),
            'goal': torch.FloatTensor(obs['goal']).unsqueeze(0),
            'local_dem': torch.FloatTensor(obs['local_dem']).unsqueeze(0)
        }
        
        action, _, _ = actor_critic.get_action(obs_tensor)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_obs, reward, done, truncated, info = env.step(action.numpy().squeeze())
        
        # æ›´æ–°è·¯å¾„
        path_x.append(next_obs['position'][0])
        path_y.append(next_obs['position'][1])
        total_reward += reward
        
        if done and info.get('reason') == 'reached_goal':
            success = True
            break
        
        if done:
            break
        
        obs = next_obs
    
    # ç»˜åˆ¶è®­ç»ƒåçš„æ™ºèƒ½ä½“è·¯å¾„
    ax1.imshow(env.dem, cmap='terrain', origin='lower', alpha=0.7)
    
    # ç»˜åˆ¶ç†æƒ³è·¯å¾„
    ideal_x = [env.start_pos[0], env.goal_pos[0]]
    ideal_y = [env.start_pos[1], env.goal_pos[1]]
    ax1.plot(ideal_x, ideal_y, 'r--', linewidth=2, alpha=0.6, label='ç†æƒ³è·¯å¾„')
    
    # ç»˜åˆ¶æ™ºèƒ½ä½“è·¯å¾„
    ax1.plot(path_x, path_y, 'b-', linewidth=3, alpha=0.8, label='æ™ºèƒ½ä½“è·¯å¾„')
    ax1.plot(path_x[0], path_y[0], 'go', markersize=15, label='èµ·ç‚¹', markeredgecolor='black', markeredgewidth=2)
    ax1.plot(env.goal_pos[0], env.goal_pos[1], 'ro', markersize=15, label='ç»ˆç‚¹', markeredgecolor='black', markeredgewidth=2)
    ax1.plot(path_x[-1], path_y[-1], 'bo', markersize=10, label='å½“å‰ä½ç½®')
    
    ax1.set_title(f'è®­ç»ƒåçš„æ™ºèƒ½ä½“\nå¥–åŠ±: {total_reward:.1f}, æˆåŠŸ: {"âœ…" if success else "âŒ"}', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Xåæ ‡', fontsize=12)
    ax1.set_ylabel('Yåæ ‡', fontsize=12)
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # å³å›¾ï¼šéšæœºç­–ç•¥
    print("ğŸ² è¿è¡Œéšæœºç­–ç•¥...")
    obs, _ = env.reset()
    
    random_path_x = [obs['position'][0]]
    random_path_y = [obs['position'][1]]
    random_total_reward = 0
    random_success = False
    
    for step in range(200):
        # éšæœºåŠ¨ä½œ
        action = env.action_space.sample()
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_obs, reward, done, truncated, info = env.step(action)
        
        # æ›´æ–°è·¯å¾„
        random_path_x.append(next_obs['position'][0])
        random_path_y.append(next_obs['position'][1])
        random_total_reward += reward
        
        if done and info.get('reason') == 'reached_goal':
            random_success = True
            break
        
        if done:
            break
        
        obs = next_obs
    
    # ç»˜åˆ¶éšæœºç­–ç•¥è·¯å¾„
    ax2.imshow(env.dem, cmap='terrain', origin='lower', alpha=0.7)
    
    # ç»˜åˆ¶ç†æƒ³è·¯å¾„
    ax2.plot(ideal_x, ideal_y, 'r--', linewidth=2, alpha=0.6, label='ç†æƒ³è·¯å¾„')
    
    # ç»˜åˆ¶éšæœºè·¯å¾„
    ax2.plot(random_path_x, random_path_y, 'orange', linewidth=3, alpha=0.8, label='éšæœºè·¯å¾„')
    ax2.plot(random_path_x[0], random_path_y[0], 'go', markersize=15, label='èµ·ç‚¹', markeredgecolor='black', markeredgewidth=2)
    ax2.plot(env.goal_pos[0], env.goal_pos[1], 'ro', markersize=15, label='ç»ˆç‚¹', markeredgecolor='black', markeredgewidth=2)
    ax2.plot(random_path_x[-1], random_path_y[-1], 'orange', marker='o', markersize=10, label='å½“å‰ä½ç½®')
    
    ax2.set_title(f'éšæœºç­–ç•¥\nå¥–åŠ±: {random_total_reward:.1f}, æˆåŠŸ: {"âœ…" if random_success else "âŒ"}', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Xåæ ‡', fontsize=12)
    ax2.set_ylabel('Yåæ ‡', fontsize=12)
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    plt.tight_layout()
    plt.show()
    
    print(f"ğŸ“Š ç»“æœå¯¹æ¯”:")
    print(f"   è®­ç»ƒåæ™ºèƒ½ä½“: å¥–åŠ±={total_reward:.1f}, æˆåŠŸ={success}")
    print(f"   éšæœºç­–ç•¥: å¥–åŠ±={random_total_reward:.1f}, æˆåŠŸ={random_success}")


if __name__ == "__main__":
    visualize_agent_behavior()


