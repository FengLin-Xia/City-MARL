# MAPPOç‹¬ç«‹ç½‘ç»œå®æ–½æ€»ç»“

## âœ… å®æ–½å®Œæˆ

**æ ¸å¿ƒæ”¹é€ **ï¼šä»å…±äº«ç½‘ç»œæ”¹ä¸ºæ¯ä¸ªagentç‹¬ç«‹çš„Actorå’ŒCritic

---

## ğŸ¯ è§£å†³çš„æ ¸å¿ƒé—®é¢˜

### **å…±äº«ç½‘ç»œçš„ç“¶é¢ˆ**

```
é—®é¢˜1: ç­–ç•¥å†²çª
  - INDå’ŒEDUç”¨åŒä¸€ä¸ªActor
  - ä¼˜åŒ–INDç­–ç•¥ â†’ å½±å“EDU
  - ä¸¤è€…äº’ç›¸å¹²æ‰° â†’ KLé«˜ï¼Œä¸ç¨³å®š

é—®é¢˜2: Valueä¼°è®¡æ··æ·†  
  - åŒä¸€ä¸ªCriticè¦ä¼°è®¡ä¸¤ä¸ªagentçš„çŠ¶æ€ä»·å€¼
  - V(state, IND) vs V(state, EDU)æ··åœ¨ä¸€èµ·
  - Value Lossé«˜ï¼ˆ14000â†’567ä»ç„¶é«˜ï¼‰

é—®é¢˜3: è¿‡æ—©æ”¶æ•›
  - ç½‘ç»œæ‰¾åˆ°å¯¹ä¸¤è€…éƒ½"è¿˜è¡Œ"çš„æŠ˜ä¸­ç­–ç•¥
  - Episode 2å°±å›ºå®š90.0
  - å¡åœ¨å±€éƒ¨æœ€ä¼˜
```

---

## ğŸ”§ ä»£ç ä¿®æ”¹

### 1. RLPolicySelectorï¼ˆsolvers/v4_1/rl_selector.pyï¼‰

**åˆ›å»ºç‹¬ç«‹ç½‘ç»œ**ï¼š
```python
# æ—§ï¼šå…±äº«ç½‘ç»œ
self.actor = Actor(...)
self.critic = Critic(...)

# æ–°ï¼šç‹¬ç«‹ç½‘ç»œ
self.actors = {
    'IND': Actor(...),
    'EDU': Actor(...)
}
self.critics = {
    'IND': Critic(...),
    'EDU': Critic(...)
}

# ç‹¬ç«‹ä¼˜åŒ–å™¨
self.actor_optimizers = {
    'IND': Adam(self.actors['IND'].parameters(), ...),
    'EDU': Adam(self.actors['EDU'].parameters(), ...)
}
self.critic_optimizers = {...}  # åŒç†
```

**ä¿®æ”¹ç½‘ç»œè°ƒç”¨**ï¼š
```python
# åœ¨_rl_choose_sequenceä¸­
current_agent = actions[0].agent
actor = self.actors.get(current_agent, self.actor)  # é€‰æ‹©å¯¹åº”ç½‘ç»œ
logits = actor(state_embed)
```

### 2. PPO Trainerï¼ˆtrainers/v4_1/ppo_trainer.pyï¼‰

**åˆ†agentæ›´æ–°**ï¼š
```python
# åœ¨update_policyä¸­
for exp in experiences:
    agent = exp.get('agent', 'IND')
    actor = self.selector.actors[agent]    # é€‰æ‹©å¯¹åº”ç½‘ç»œ
    critic = self.selector.critics[agent]
    
    logits = actor(state_embed)
    value = critic(state_embed)

# æ›´æ–°æ—¶
for agent in ['IND', 'EDU']:
    optimizer = self.selector.actor_optimizers[agent]
    optimizer.zero_grad()
    # ... æ›´æ–° ...
    optimizer.step()
```

### 3. æ¨¡å‹ä¿å­˜/åŠ è½½ï¼ˆsolvers/v4_1/rl_selector.pyï¼‰

**ä¿å­˜å¤šä¸ªç½‘ç»œ**ï¼š
```python
model_data = {
    'model_version': 'v4.1_mappo',
    'actor_IND_state_dict': self.actors['IND'].state_dict(),
    'actor_EDU_state_dict': self.actors['EDU'].state_dict(),
    'critic_IND_state_dict': self.critics['IND'].state_dict(),
    'critic_EDU_state_dict': self.critics['EDU'].state_dict(),
    # ... ä¼˜åŒ–å™¨çŠ¶æ€ ...
}
```

**å‘åå…¼å®¹**ï¼š
```python
# åŠ è½½æ—¶æ£€æµ‹æ¨¡å‹ç‰ˆæœ¬
if 'v4.1_mappo' in model_version:
    # åŠ è½½MAPPOæ¨¡å‹
else:
    # å‘åå…¼å®¹ï¼šå¤åˆ¶æ—§æ¨¡å‹åˆ°æ‰€æœ‰agent
    for agent in self.actors.keys():
        self.actors[agent].load_state_dict(old_model['actor_state_dict'])
```

---

## âœ… æµ‹è¯•éªŒè¯

### æµ‹è¯•ç»“æœ
```
[OK] ä¸ºINDå’ŒEDUåˆ›å»ºäº†ç‹¬ç«‹çš„Actorå’ŒCritic
[OK] å‚æ•°ä¸å…±äº«ï¼ŒçœŸæ­£ç‹¬ç«‹
[OK] å‰å‘ä¼ æ’­æ­£å¸¸
[OK] ç½‘ç»œå¯ä»¥æ­£å¸¸ä½¿ç”¨

å‚æ•°é‡ï¼š
  - IND Actor: 170,674
  - EDU Actor: 170,674  (ç‹¬ç«‹ï¼)
  - IND Critic: 164,353
  - EDU Critic: 164,353 (ç‹¬ç«‹ï¼)

æ€»å‚æ•°é‡: çº¦670K (vs å…±äº«ç½‘ç»œ335Kï¼Œç¿»å€)
```

---

## ğŸ“Š æ¶æ„å¯¹æ¯”

### å…±äº«ç½‘ç»œ vs MAPPO

| æ–¹é¢ | å…±äº«ç½‘ç»œ | MAPPOï¼ˆç‹¬ç«‹ï¼‰ |
|------|---------|--------------|
| **Actoræ•°é‡** | 1ä¸ª | 2ä¸ªï¼ˆIND+EDUï¼‰ |
| **Criticæ•°é‡** | 1ä¸ª | 2ä¸ªï¼ˆIND+EDUï¼‰ |
| **å‚æ•°é‡** | 335K | 670K (+100%) |
| **ç­–ç•¥å†²çª** | ğŸ”´ æœ‰ | âœ… æ—  |
| **Valueæ··æ·†** | ğŸ”´ æœ‰ | âœ… æ—  |
| **æ”¶æ•›é€Ÿåº¦** | æ…¢ | å¿« |
| **KLç¨³å®šæ€§** | å·® | å¥½ |

---

## ğŸ“ˆ é¢„æœŸè®­ç»ƒæ•ˆæœ

### Episode 1-5ï¼ˆMAPPOåˆæœŸï¼‰
```
KL_ind: 1.0-2.0
KL_edu: 1.0-2.0
Value Loss: 300-600 (vs å…±äº«çš„567)
Clip: 60-90%
Return: 80-150
```

### Episode 10ï¼ˆMAPPOç¨³å®šï¼‰
```
KL: 0.5-1.5 (vs å…±äº«çš„4.93)  â† å…³é”®æ”¹å–„ï¼
Value Loss: 200-400
Clip: 40-70%
Return: 120-180
```

### Episode 20ï¼ˆMAPPOæ”¶æ•›ï¼‰
```
KL: 0.1-0.5
Value Loss: 100-300
Clip: 20-50%
Return: 150-250
Size: åº”è¯¥å‡ºç°M/Lå‹
```

---

## ğŸš€ ä¸‹ä¸€æ­¥

### **é‡æ–°è®­ç»ƒï¼ˆä»å¤´å¼€å§‹ï¼‰**

```bash
# 1. åˆ é™¤æ—§æ¨¡å‹ï¼ˆå¿…é¡»ï¼å› ä¸ºç»“æ„å˜äº†ï¼‰
rm models/v4_1_rl/*.pth

# 2. è®­ç»ƒ10 episodesæµ‹è¯•
python enhanced_city_simulation_v4_1.py --mode rl
```

### è§‚å¯Ÿå…³é”®æŒ‡æ ‡

**Episode 5æ£€æŸ¥**ï¼š
- KL < 2.0? â†’ âœ… MAPPOæœ‰æ•ˆ
- Value Loss < 600? â†’ âœ… Criticå­¦å¾—æ›´å¥½
- Return > 100? â†’ âœ… ç­–ç•¥æ”¹å–„

**Episode 10æ£€æŸ¥**ï¼š
- KL < 1.0? â†’ âœ… æ”¶æ•›è‰¯å¥½
- Return > 120? â†’ âœ… ç­–ç•¥ä¼˜åŒ–
- å‡ºç°M/Lå‹? â†’ âœ… å¤šæ ·æ€§æ”¹å–„

---

## ğŸ“ é…ç½®æ€»è§ˆ

**å½“å‰å®Œæ•´é…ç½®**ï¼š
```json
{
  "solver": {
    "rl": {
      // ç‹¬ç«‹ç½‘ç»œï¼ˆMAPPOï¼‰
      "algo": "mappo",
      "agents": ["IND", "EDU"],
      
      // å­¦ä¹ ç‡
      "actor_lr": 5e-5,
      "critic_lr": 5e-4,
      
      // Entropyï¼ˆè°ƒæ•´åï¼‰
      "ent_coef": 0.08,
      "entropy_coef": 0.04,
      "temperature": 2.5,
      
      // PPOå‚æ•°
      "clip_eps": 0.15,
      "num_epochs": 3,
      
      // Reward
      "reward_scale": 3000.0,
      "reward_clip": 1.0,
      "expected_lifetime": 12,
      
      // Sizeæ¿€åŠ±
      "size_bonus": {"S": 0, "M": 1000, "L": 2000}
    }
  }
}
```

---

## ğŸ¯ å…³é”®æ”¹è¿›ç‚¹

### 1. ç‹¬ç«‹ç½‘ç»œï¼ˆMAPPOï¼‰âœ…
- æ¶ˆé™¤ç­–ç•¥å†²çª
- åŠ é€Ÿæ”¶æ•›
- é¢„æœŸKLä»4.93é™åˆ°<1.0

### 2. å›ºå®šNPV Reward âœ…
- è§£å†³"èººå¹³"é—®é¢˜
- é¼“åŠ±æŒç»­å»ºé€ 
- Lå‹rewardæœ€é«˜

### 3. Entropyå¹³è¡¡ âœ…
- ä¸ä¼šè¿‡æ—©æ”¶æ•›
- ä¸ä¼šKLçˆ†ç‚¸
- ä¿æŒé€‚åº¦æ¢ç´¢

### 4. Value Lossä¼˜åŒ– âœ…
- reward_scale=3000
- critic_lr=5e-4
- ç‹¬ç«‹Criticæ›´æ˜“å­¦ä¹ 

---

**å®æ–½æ—¥æœŸ**: 2025-10-12  
**ç‰ˆæœ¬**: v4.1_MAPPO  
**çŠ¶æ€**: âœ… å·²å®Œæˆï¼Œå‡†å¤‡è®­ç»ƒ  
**ä¸‹ä¸€æ­¥**: åˆ é™¤æ—§æ¨¡å‹ï¼Œé‡æ–°è®­ç»ƒ10 episodes



