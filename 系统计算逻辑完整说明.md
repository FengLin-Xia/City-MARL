# 城市模拟系统计算逻辑完整说明

## 📋 目录
1. [系统架构概览](#系统架构概览)
2. [核心计算模块](#核心计算模块)
3. [Cost计算详解](#cost计算详解)
4. [Reward计算详解](#reward计算详解)
5. [Prestige计算详解](#prestige计算详解)
6. [RL奖励计算机制](#rl奖励计算机制)
7. [Budget系统](#budget系统)
8. [动作选择与评分](#动作选择与评分)
9. [配置参数详解](#配置参数详解)
10. [计算流程示例](#计算流程示例)

---

## 系统架构概览

### 双层设计架构
```
┌─────────────────────────────────────────────────────────┐
│                    ActionScorer                          │
│  计算原始经济指标 (cost/reward/prestige/score)           │
│  位置: logic/v4_enumeration.py                          │
└────────────────────────┬────────────────────────────────┘
                         │
                         ↓
┌─────────────────────────────────────────────────────────┐
│                    CityEnvironment                       │
│  基于action属性 → 计算RL的total_reward                   │
│  位置: envs/v4_1/city_env.py                            │
└─────────────────────────────────────────────────────────┘
```

### 主要组件
- **ActionScorer**: 计算每个建筑动作的经济指标
- **CityEnvironment**: 将经济指标转换为RL训练奖励
- **Budget系统**: 管理智能体的财务约束
- **地价系统**: 提供地价信息用于成本计算
- **河流系统**: 提供河流溢价奖励

---

## 核心计算模块

### 1. ActionScorer (`logic/v4_enumeration.py`)

**主要功能**:
- 计算每个建筑动作的cost、reward、prestige
- 归一化各维度数值
- 计算综合评分用于动作排序

**核心方法**:
```python
def _calc_crp(self, action, river_distance_provider=None, buildings=None):
    """计算cost/reward/prestige"""
    
def score_actions(self, actions, river_distance_provider=None, buildings=None):
    """归一化并计算综合评分"""
```

### 2. CityEnvironment (`envs/v4_1/city_env.py`)

**主要功能**:
- 将ActionScorer的结果转换为RL奖励
- 管理Budget系统
- 处理月度收益累积
- 计算协作奖励

**核心方法**:
```python
def _calculate_reward(self, agent, action):
    """计算RL训练奖励"""
    
def _calculate_monthly_income(self, agent):
    """计算月度收益累积"""
```

---

## Cost计算详解

### 基础公式

**IND建筑**:
```
cost = BaseCost_IND[size] + ZC[zone] + LP_value
```

**EDU建筑**:
```
cost = BaseCost_EDU[size] + ZC[zone] + LP_value
```

### 参数详解

#### 基础建造成本
```json
BaseCost_IND = {
    "S": 900,   // 小型工业建筑
    "M": 1500,  // 中型工业建筑  
    "L": 2400   // 大型工业建筑
}

BaseCost_EDU = {
    "S": 700,   // 小型教育建筑
    "M": 1200,  // 中型教育建筑
    "L": 1900   // 大型教育建筑
}
```

#### 区位附加成本
```json
ZC = {
    "near": 200,  // 靠近枢纽区域
    "mid": 100,   // 中等距离区域
    "far": 0      // 远离枢纽区域
}
```

#### 地价成本
```
LP_value = LP_idx × LandPriceBase

其中:
- LP_idx = 10~100 (基于LP_norm线性映射)
- LandPriceBase = 11.0 kGBP/点
- LP_value范围: 110~1100 kGBP
```

### 成本范围示例

| 建筑类型 | Size | 基础成本 | 区位影响 | 地价影响 | 总成本范围 |
|---------|------|---------|---------|---------|-----------|
| IND | S | 900 | +0~200 | +110~1100 | **1010~2200 kGBP** |
| IND | M | 1500 | +0~200 | +110~1100 | **1610~2800 kGBP** |
| IND | L | 2400 | +0~200 | +110~1100 | **2510~3700 kGBP** |
| EDU | S | 700 | +0~200 | +110~1100 | **810~2000 kGBP** |
| EDU | M | 1200 | +0~200 | +110~1100 | **1310~2500 kGBP** |
| EDU | L | 1900 | +0~200 | +110~1100 | **2010~3200 kGBP** |

---

## Reward计算详解

### 基础公式

**IND建筑**:
```
reward_base = RevBase_IND[size] + ZR[zone] + Adj_bonus*adj - OPEX_IND[size] - Rent[size] + river_premium
reward = reward_base * (1 + RewardLP_k * LP_norm)
```

**EDU建筑**:
```
reward_base = RevBase_EDU[size] + ZR[zone] + Adj_bonus*adj - OPEX_EDU[size] + river_premium
reward_land = Rent[size]
reward = reward_base + reward_land
```

### 参数详解

#### 基础月收入
```json
RevBase_IND = {
    "S": 180,   // 小型工业月收入
    "M": 320,   // 中型工业月收入
    "L": 520    // 大型工业月收入
}

RevBase_EDU = {
    "S": 140,   // 小型教育月收入
    "M": 260,   // 中型教育月收入
    "L": 420    // 大型教育月收入
}
```

#### 区位收入加成
```json
ZR = {
    "near": 80,  // 靠近枢纽区域
    "mid": 40,   // 中等距离区域
    "far": 0     // 远离枢纽区域
}
```

#### 运营成本
```json
OPEX_IND = {
    "S": 100,   // 小型工业运营成本
    "M": 180,   // 中型工业运营成本
    "L": 300    // 大型工业运营成本
}

OPEX_EDU = {
    "S": 70,    // 小型教育运营成本
    "M": 120,   // 中型教育运营成本
    "L": 190    // 大型教育运营成本
}
```

#### 租金机制
```json
Rent = {
    "S": 25,    // 小型建筑租金
    "M": 45,    // 中型建筑租金
    "L": 70     // 大型建筑租金
}
```

**租金逻辑**:
- IND建筑: 支付租金 (reward -= Rent[size])
- EDU建筑: 获得租金 (reward += Rent[size])

#### 地价增益系数
```json
RewardLP_k = {
    "IND": 0.25,  // 工业建筑地价增益
    "EDU": 0.10   // 教育建筑地价增益
}
```

### 河流溢价机制

```
decay = 2^(-d_eff / RiverD_half_m)
BaseForPremium = RevBase_type[size] + ZR[zone]
RawRiverPrem = BaseForPremium × RiverPmax_pct[type]/100 × decay
RiverPremium = clamp(round_nearest(RawRiverPrem), 0, RiverPremiumCap)
```

**参数**:
- `RiverPmax_pct`: IND=20%, EDU=15%
- `RiverD_half_m`: 120米 (半衰距离)
- `RiverPremiumCap`: 10000 kGBP (封顶值)

### 邻近性奖励/惩罚

**邻近奖励** (距离 ≤ 10单位):
```
proximity_bonus = proximity_reward_val × (1.0 - min_dist / proximity_threshold)
```

**距离惩罚** (距离 > 10单位):
```
distance_penalty = (min_dist - proximity_threshold) × distance_penalty_coef
```

**参数**:
- `proximity_threshold`: 10.0
- `proximity_reward_val`: 50.0
- `distance_penalty_coef`: 2.0

### 规模奖励 (Size Bonus)

```json
size_bonus = {
    "M": 1000,  // 中型建筑奖励
    "L": 2000   // 大型建筑奖励
}
```

### 收益计算示例

**IND M型建筑** (zone=near, LP_norm=0.8, 距离河流30米):
```
reward_base = 320 + 80 + 0 - 180 - 45 + 61 = 236
reward = 236 × (1 + 0.25 × 0.8) = 236 × 1.2 = 283 kGBP/月
```

**EDU L型建筑** (zone=near, LP_norm=0.8, 距离河流50米):
```
reward_base = 420 + 80 + 0 - 190 + 45 = 355
reward_land = 70
reward = 355 + 70 = 425 kGBP/月
```

---

## Prestige计算详解

### 基础公式

**EDU建筑**:
```
prestige = PrestigeBase[size] + I(zone==near) + I(adj) - β×Pollution[size]
```

**IND建筑**:
```
prestige = PrestigeBase[size] + I(zone==near) + I(adj) - 0.2×Pollution[size]
```

### 参数详解

#### 基础声望值
```json
PrestigeBase_EDU = {
    "S": 0.2,   // 小型教育声望
    "M": 0.6,   // 中型教育声望
    "L": 1.0    // 大型教育声望
}

PrestigeBase_IND = {
    "S": 0.2,   // 小型工业声望
    "M": 0.1,   // 中型工业声望
    "L": -0.1   // 大型工业声望(负值)
}
```

#### 区位和邻接加成
- `I(zone==near)`: 靠近枢纽区域时 +1.0
- `I(adj)`: 有邻接建筑时 +1.0

#### 污染惩罚
```json
Pollution_EDU = {
    "S": 0.2,
    "M": 0.4,
    "L": 0.6
}

Pollution_IND = {
    "S": 0.6,
    "M": 0.9,
    "L": 1.2
}
```

### Prestige计算示例

**EDU L型建筑** (near区域, 有邻接, β=0.25):
```
prestige = 1.0 + 1.0 + 1.0 - 0.25×0.6 = 2.85
```

**IND L型建筑** (mid区域, 无邻接):
```
prestige = -0.1 + 0 + 0 - 0.2×1.2 = -0.34
```

---

## RL奖励计算机制

### 计算流程 (`envs/v4_1/city_env.py`)

```python
def _calculate_reward(self, agent, action):
    """计算奖励（固定NPV机制）"""
    # 1. 计算建造成本
    build_cost = float(action.cost) if action.cost is not None else 0.0
    
    if build_cost > 0:  # 有建造
        # 2. 计算未来收益（固定回报期）
        expected_lifetime = 12  # 预期生命周期(月)
        monthly_reward = float(action.reward) if action.reward is not None else 0.0
        future_income = monthly_reward * expected_lifetime
        
        # 3. NPV = 未来收益 - 成本
        npv = future_income - build_cost
        
        # 4. 进度奖励
        progress_reward = len(self.buildings) * 0.5
        
        # 5. 协作奖励（可配置）
        cooperation_bonus = self._calculate_cooperation_reward(agent, action)
        
        # 6. Budget惩罚（软约束）
        budget_penalty = self._calculate_budget_penalty(agent, action)
        
        # 7. 总奖励 = NPV + 进度 + 协作 - 惩罚
        total_reward = npv + progress_reward + cooperation_bonus - budget_penalty
    else:
        # 空序列（不建造）：无reward
        total_reward = 0.0
    
    # 8. 奖励缩放
    scaled_reward = total_reward / reward_scale  # 默认30000
    scaled_reward = np.clip(scaled_reward, -1.0, 1.0)
    
    return scaled_reward
```

### 协作奖励机制

```python
def _calculate_cooperation_reward(self, agent, action):
    cooperation_lambda = 0.2  # 协作权重
    
    cooperation_bonus = 0.0
    
    # 功能互补奖励
    if agent == 'EDU':
        cooperation_bonus += len(industrial_buildings) * 0.05
    elif agent == 'IND':
        cooperation_bonus += len(public_buildings) * 0.05
    
    # 空间协调奖励
    for other_building in all_buildings:
        distance = calculate_distance(action, other_building)
        if 5 <= distance <= 20:
            cooperation_bonus += 0.02
    
    return cooperation_lambda * cooperation_bonus
```

### 月度收益累积机制

```python
def _calculate_monthly_income(self, agent):
    """计算agent的月度收益（所有在营建筑的累加）"""
    total_income = sum([
        asset['monthly_income'] 
        for asset in self.active_assets[agent]
    ])
    return total_income

def _advance_turn(self):
    """每月开始时，为所有agent累加月度收益到budget"""
    # 【月度收益机制】每月开始时，为所有agent累加月度收益到budget
    if self.budgets is not None:
        for agent in self.rl_cfg['agents']:
            monthly_income = self._calculate_monthly_income(agent)
            self.budgets[agent] += monthly_income  # 每月真实累积收益
            
            # 记录月度收益历史
            self.monthly_income_history[agent].append(monthly_income)

def _place_building(self, agent, action):
    """放置建筑时记录为在营资产"""
    # 【月度收益机制】记录为在营资产
    asset = {
        'size': action.size,
        'monthly_income': float(action.reward),  # 存储月度收益
        'cost': float(action.cost),
        'built_month': self.current_month,
    }
    self.active_assets[agent].append(asset)  # 添加到在营资产列表
```

### 双重收益机制详解

系统设计了**双重收益机制**来平衡RL训练和财务约束：

#### 1. **RL奖励层面（NPV机制）**
```python
# RL看到的是长期价值
expected_lifetime = 12  # 12个月预期生命周期
future_income = monthly_reward * expected_lifetime
npv = future_income - build_cost

# 示例：IND M型建筑
# npv = 283 * 12 - 2602 = 3396 - 2602 = 794 kGBP (正收益)
```

**作用**：给RL智能体正确的"投资回报"信号，鼓励长期投资行为。

#### 2. **Budget系统层面（现金流管理）**
```python
# 每月真实累积收益
def _advance_turn(self):
    for agent in agents:
        monthly_income = sum([asset.monthly_income for asset in active_assets])
        budget[agent] += monthly_income

# 示例：IND M型建筑的Budget变化
# Month 0: budget = 15000 - 2602 = 12398  # 建造成本
# Month 1: budget = 12398 + 283 = 12681   # 第1个月收益
# Month 2: budget = 12681 + 283 = 12964   # 第2个月收益
# ...
# Month 12: budget = 15000 + 794 = 15794  # 12个月后净收益794
```

**作用**：提供真实的现金流管理，防止过度借贷，确保财务可持续性。

#### 3. **两者协调性**
- **设计一致性**：12个月后Budget增长与NPV计算完全一致
- **功能互补**：RL负责长期策略，Budget负责短期约束
- **避免矛盾**：不存在"RL说赚钱但Budget说亏钱"的情况

---

## Budget系统

### 配置参数

```json
"budget_system": {
    "enabled": true,
    "mode": "soft_constraint",
    "initial_budgets": {
        "IND": 15000,  // 工业智能体初始预算
        "EDU": 10000   // 教育智能体初始预算
    },
    "debt_penalty_coef": 0.1,        // 负债惩罚系数
    "max_debt": -2000,               // 最大允许负债
    "bankruptcy_threshold": -5000,   // 破产阈值
    "bankruptcy_penalty": -100.0     // 破产惩罚
}
```

### Budget惩罚计算

```python
def _calculate_budget_penalty(self, agent, action):
    if self.budgets is None:
        return 0.0
    
    # 预估建造后的budget
    budget_after = self.budgets[agent] - action.cost
    
    budget_penalty = 0.0
    
    # 负债惩罚
    if budget_after < 0:
        debt_penalty_coef = 0.1
        budget_penalty = abs(budget_after) * debt_penalty_coef
    
    # 破产惩罚
    if budget_after < -5000:
        budget_penalty += 100.0
    
    return budget_penalty
```

### Budget更新机制

```python
def step(self, action):
    # 1. 建造时扣除成本
    if action.build:
        self.budgets[agent] -= action.cost
    
    # 2. 每月累加月度收益
    monthly_income = self._calculate_monthly_income(agent)
    self.budgets[agent] += monthly_income
    
    # 3. 记录budget历史
    self.budget_history[agent].append(self.budgets[agent])
```

---

## 动作选择与评分

### ActionScorer评分机制

```python
def score_actions(self, actions):
    # 1. 计算原始cost/reward/prestige
    for action in actions:
        self._calc_crp(action)
    
    # 2. 归一化各维度
    costs = [a.cost for a in actions]
    rewards = [a.reward for a in actions]
    prestiges = [a.prestige for a in actions]
    
    c_min, c_max = min(costs), max(costs)
    r_min, r_max = min(rewards), max(rewards)
    p_min, p_max = min(prestiges), max(prestiges)
    
    def norm(v, lo, hi):
        return (v - lo) / (hi - lo) if hi - lo > 1e-9 else 0.5
    
    # 3. 按智能体使用不同权重计算综合评分
    for action in actions:
        w = self.objective.get(action.agent, {"w_r": 0.5, "w_p": 0.3, "w_c": 0.2})
        
        nr = norm(action.reward, r_min, r_max)
        np = norm(action.prestige, p_min, p_max)
        nc = norm(action.cost, c_min, c_max)
        
        action.score = w["w_r"] * nr + w["w_p"] * np - w["w_c"] * nc
    
    return actions
```

### 智能体权重配置

```json
"objective": {
    "EDU": {
        "w_r": 0.45,  // reward权重
        "w_p": 0.45,  // prestige权重  
        "w_c": 0.1    // cost权重
    },
    "IND": {
        "w_r": 0.6,   // reward权重
        "w_p": 0.2,   // prestige权重
        "w_c": 0.2    // cost权重
    }
}
```

### 动作选择策略

1. **枚举阶段**: 生成所有可能的建筑动作
2. **评分阶段**: 使用ActionScorer计算综合评分
3. **排序阶段**: 按评分降序排列
4. **选择阶段**: RL智能体从排序后的动作中选择

---

## 配置参数详解

### 核心配置结构

```json
{
    "simulation": {
        "total_months": 30
    },
    "city": {
        "map_size": [200, 200],
        "transport_hubs": [[122, 80], [112, 121]]
    },
    "budget_system": {
        "enabled": true,
        "initial_budgets": {"IND": 15000, "EDU": 10000},
        "debt_penalty_coef": 0.1,
        "bankruptcy_threshold": -5000
    },
    "growth_v4_1": {
        "enumeration": {
            "objective": {
                "EDU": {"w_r": 0.45, "w_p": 0.45, "w_c": 0.1},
                "IND": {"w_r": 0.6, "w_p": 0.2, "w_c": 0.2}
            }
        },
        "evaluation": {
            "BaseCost_IND": {"S": 900, "M": 1500, "L": 2400},
            "BaseCost_EDU": {"S": 700, "M": 1200, "L": 1900},
            "RevBase_IND": {"S": 180, "M": 320, "L": 520},
            "RevBase_EDU": {"S": 140, "M": 260, "L": 420},
            "LandPriceBase": 11.0,
            "RiverPmax_pct": {"IND": 20, "EDU": 15},
            "RiverD_half_m": 120,
            "size_bonus": {"M": 1000, "L": 2000}
        }
    },
    "solver": {
        "rl": {
            "reward_scale": 30000.0,
            "reward_clip": 1.0,
            "cooperation_lambda": 0.0
        }
    }
}
```

---

## 计算流程示例

### 完整计算示例

假设要在位置(100, 80)建造一个IND M型建筑:

#### 1. 输入参数
- agent: "IND"
- size: "M"
- zone: "near" (距离hub1 < 15单位)
- LP_norm: 0.8 (地价归一化值)
- adj: 1 (有邻接建筑)
- river_dist_m: 30 (距离河流30米)

#### 2. Cost计算
```
LP_idx = 10 + (100-10) * 0.8 = 82
LP_value = 82 * 11 = 902 kGBP
cost = 1500 + 200 + 902 = 2602 kGBP
```

#### 3. Reward计算
```
reward_base = 320 + 80 + 0 - 180 - 45 + 61 = 236
reward = 236 * (1 + 0.25 * 0.8) = 283 kGBP/月
```

#### 4. Prestige计算
```
prestige = 0.1 + 1.0 + 1.0 - 0.2 * 0.9 = 1.82
```

#### 5. ActionScorer评分
```
# 假设当前批次中:
# costs: [1000, 2000, 2602, 3000]
# rewards: [100, 200, 283, 400]  
# prestiges: [0.5, 1.0, 1.82, 2.0]

nr = (283-100)/(400-100) = 0.61
np = (1.82-0.5)/(2.0-0.5) = 0.88
nc = (2602-1000)/(3000-1000) = 0.80

score = 0.6 * 0.61 + 0.2 * 0.88 - 0.2 * 0.80 = 0.47
```

#### 6. RL奖励计算
```
# NPV计算 (12个月预期收益)
future_income = 283 * 12 = 3396 kGBP
npv = 3396 - 2602 = 794 kGBP

progress_reward = 5 * 0.5 = 2.5
cooperation_bonus = 0 (默认禁用)
budget_penalty = 0 (假设budget充足)

total_reward = 794 + 2.5 + 0 - 0 = 796.5
scaled_reward = 796.5 / 30000 = 0.027
```

#### 7. Budget更新
```
# 建造时
budget_IND = 15000 - 2602 = 12398

# 每月收益
budget_IND = 12398 + 283 = 12681
```

---

## 总结

### 系统特点
1. **双层架构**: ActionScorer负责经济计算，Environment负责RL奖励转换
2. **多维度评估**: 综合考虑cost、reward、prestige三个维度
3. **智能体差异化**: EDU重视prestige，IND重视reward
4. **双重收益机制**: RL使用NPV考虑长期价值，Budget系统管理真实现金流
5. **月度收益机制**: 建筑建成后持续产生收益，每月真实累积到Budget
6. **软约束Budget**: 允许适度负债，避免过度保守

### 关键设计理念
- **经济合理性**: 成本收益计算基于真实经济逻辑
- **可持续性**: 月度收益确保长期盈利
- **协作性**: 多智能体间的功能互补和空间协调
- **可配置性**: 丰富的参数配置支持不同场景

### 系统优势
1. **双重收益机制**: RL使用NPV考虑长期价值，Budget系统管理真实现金流
2. **合理的经济逻辑**: 12个月预期生命周期与月度收益累积机制协调一致
3. **智能体差异化**: EDU重视prestige，IND重视reward，体现不同角色定位
4. **软约束Budget**: 允许适度负债，避免过度保守，同时防止破产

### 当前可能的问题
1. **参数调优**: expected_lifetime、reward_scale等参数可能需要根据实际训练效果调整
2. **协作机制较弱**: cooperation_lambda=0.0默认禁用，多智能体协作信号微弱
3. **训练超参数**: 学习率、clip_eps等可能需要针对具体场景优化

### 系统设计合理性
- **RL奖励机制**: 使用NPV给RL正确的"投资回报"信号，鼓励长期投资
- **Budget系统**: 每月真实累积收益，提供财务约束，防止过度借贷
- **两者协调**: 12个月后Budget增长与NPV计算一致，设计合理

这个系统为城市规划和多智能体决策提供了一个完整的计算框架，通过精细的经济建模和奖励设计来指导智能体的决策行为。核心逻辑设计合理，问题主要在于参数调优和训练配置。
