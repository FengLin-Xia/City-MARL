先做 3 个快速判定（很快、很有信息量）

把熵奖励关掉 5 个更新再看 KL

ent_coef = 0（暂时）

预期：若只是熵在“压回均匀”，关掉后 KL 应该从 ~0 升到 0.001–0.01。

若仍≈0 → 不是熵的锅。

看“logits 的起伏”有没有增长
在 update 前后打印同一批 local_logits.std(dim=-1).mean()：

print("logits_std(mean)", local_logits.std(dim=-1).mean().item())


若长期接近 0：策略头输出几乎常数（均匀）；

提升 lr 后仍不涨 → 梯度没把 logits 打开。

“反事实灵敏度”小实验（动作是否影响回报）
对同一个 state_embed，固定随机种子，枚举 5 个不同动作，各 rollout 一次，比较总回报差值：

若 Δreturn ≈ 0（各动作回报几乎一样）→ 动作对奖励无因果，策略正确地“学不动”；

若 Δreturn 明显 → 是梯度太弱/正则太强的问题。

两条分支的处理
A. 如果 Δreturn ≈ 0（动作几乎不影响奖励）

这是规则/奖励设计问题，策略“无从可学”。做法：

在选锚点那一刻加局部 shaping（立即反馈）
例如把你已有的 action.score / action.cost 变成一小段即时奖励：

r_t += α * (收益预估 - 成本)   （α 先取 0.1~0.5）


或者把“后续月度收益的估计值”（critic 或启发式）折现加一点到当前步。

保证预算/约束真的影响可行动作的集合
如果 budget 仅做日志，不改变 action_subset，那选啥都一样，当然学不动。要么通过 mask 限制可行动作，要么把预算变化体现在 shaped reward 里。

把“租金/收益率”信号做成逐月变化（你之前谈过）
如果收益是稀疏/常数，策略没梯度；让收益随选择产生可见差别（哪怕小一些也行）。

B. 如果 Δreturn 明显（动作能改变奖励）

那就是梯度太弱。直接上“点火配置”把 KL 拉进健康区（0.01–0.03）：

actor lr: 5e-4 → 1e-3（先观察 1–2 个更新；KL>0.05 再降回）

ent_coef: 0（先关）

n_epochs: 4 → 8~10

max_grad_norm: 0.5 → 1.0

临时把优势放大 ×2 验证链路（确认 KL 会随之上升）：

adv = (adv - adv.mean()) / (adv.std() + 1e-8)
adv = 2.0 * adv


自适应 KL 目标（防飞）：

target_kl = 0.02
if approx_kl < 0.2*target_kl:
    for g in actor_optim.param_groups: g['lr'] *= 1.5
elif approx_kl > 2.0*target_kl:
    for g in actor_optim.param_groups: g['lr'] *= 0.5

另外两处容易被忽略但会“让 KL=0”的点

state_embed 是否几乎相同？
你现在把 state_embed 缓存并回放；如果编码函数几乎不依赖具体动作/状态，actor 的输入在一个小邻域里，softmax 很难被拉开。
→ 打印几条样本的 state_embed.std() 检查是否有辨识度；必要时对 state_embed 做 running-norm/加区分特征。

value_loss 很大（700+），会不会把训练牵着走
确认 actor/critic 分开优化；短期可以把 vf_coef 降低（例如 0.25 → 0.1），让 policy 梯度更显著。

一次性小 checklist

 暂时 ent_coef=0，看 KL 是否抬头

 打印 local_logits.std(mean)（前后对比）

 做 5 动作反事实 Δreturn 测试

 若 Δreturn≈0：做 shaping（把 action.score/cost 带一点进即时奖励，并/或通过 mask 让预算影响动作集）

 若 Δreturn>0：按“点火配置”把 KL 拉进 0.01–0.03

 降低 vf_coef，确认 actor/critic 各自 optimizer