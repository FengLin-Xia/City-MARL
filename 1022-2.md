# v5.1 多动作采样机制（Multi-Action Sampling Logic）

**生成时间**: 2025-10-22  
**版本**: v5.1.0  
**分析范围**: PPO 框架下的多动作采样与训练兼容性

---

## 1. 背景

v5.0 框架中每个时间步（step）仅允许执行 1 个离散动作，难以表达复杂决策组合。  
v5.1 引入 **多动作采样机制（Multi-Action Sampling）**，允许智能体在单步内执行 **1~5 个互不重复动作**，以增强策略灵活性与表达能力。

---

## 2. 架构升级概述

### 2.1 主要改动模块

| 模块 | 改动内容 |
|------|-----------|
| `env.step()` | 接受动作集合（list / set）；内部依次执行并聚合 reward / next_state |
| `get_action_mask()` | 动态更新掩码，防止重复动作 |
| `rollout buffer` | 增加 `action_set`, `logprob_sum`, `entropy_sum` 字段 |
| `V5PPOTrainer` | 支持联合 logprob 计算（子动作 logprob 求和） |

---

## 3. 多动作采样逻辑

v5.1 采用 **自回归采样 + STOP 动作** 的方案，每步最多执行 5 个动作，数量动态决定。

### 3.1 采样伪代码

```python
def sample_action_set(policy, obs, mask, max_k=5):
    """在当前状态 obs 下采样 1~max_k 个不重复动作"""
    selected, logprobs, entropies = [], [], []

    for i in range(max_k):
        logits = policy(obs)
        logits[mask == 0] = -float("inf")

        # 加上 STOP 动作
        stop_logit = torch.tensor([0.0])
        logits = torch.cat([logits, stop_logit])
        probs = torch.softmax(logits, dim=-1)

        action = torch.multinomial(probs, 1)
        if action == len(probs) - 1:  # STOP
            break

        logprobs.append(torch.log(probs[action]))
        entropies.append(-torch.sum(probs * torch.log(probs + 1e-8)))
        selected.append(action.item())
        mask[action] = 0  # 不重复

    logprob_sum = torch.stack(logprobs).sum()
    entropy_sum = torch.stack(entropies).sum()
    return selected, logprob_sum, entropy_sum
```

---

## 4. PPO 兼容性改造

### 4.1 联合 logprob

PPO 的比率计算调整为：

```python
ratio = torch.exp(logprob_sum_new - logprob_sum_old)
ppo_loss = -torch.min(ratio * adv, torch.clamp(ratio, 1 - eps, 1 + eps) * adv).mean()
```

### 4.2 熵项与价值损失

```python
entropy_loss = -entropy_coef * entropy_sum.mean()
value_loss = (returns - values).pow(2).mean()
loss = ppo_loss + value_coef * value_loss + entropy_loss
```

### 4.3 优势函数与回报

仍按「宏步」计算：一次环境 step 内的所有动作被视为一个整体。

---

## 5. 环境与缓冲区调整

### 5.1 环境接口

```python
def step(self, action_set):
    total_reward = 0
    for a in action_set:
        obs, reward, done, info = self._apply_action(a)
        total_reward += reward
        if done:
            break
    return obs, total_reward, done, info
```

### 5.2 Rollout Buffer

新增字段：

```python
buffer.add(
    obs=obs,
    action_set=selected,
    logprob_sum=logprob_sum,
    entropy_sum=entropy_sum,
    reward=reward,
    value=value
)
```

---

## 6. 训练稳定性与正则化建议

| 策略 | 说明 |
|------|------|
| **动作数惩罚** | 在奖励中加入 `-λ * (k - 1)`，防止贪多 |
| **探索正则化** | 保留熵项，有助于多动作情况下的多样性 |
| **掩码更新** | 每次采样后立即更新 mask，确保无重复 |

---

## 7. 版本规划

| 阶段 | 内容 | 状态 |
|------|------|------|
| v5.1-alpha | 环境支持 action set，PPO 支持联合 logprob | ✅ |
| v5.1-beta | 训练稳定性测试与优势函数校验 | ⏳ |
| v5.2 | 增加动作数惩罚与奖励修正机制 | 🧩 |

---

**报告生成时间**: 2025-10-22 13:55:37  
**生成工具**: ChatGPT（基于 Cursor v5.0 架构分析延伸）
