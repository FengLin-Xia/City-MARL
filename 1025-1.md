# v5.0城市模拟系统计算架构分析

## 📋 系统概述

v5.0城市模拟系统是一个基于强化学习的多智能体城市发展规划系统，通过动态计算建筑成本、收益和声望来指导智能体的决策。

## 🏗️ 系统架构

### 核心组件

1. **V5CityEnvironment** - 环境管理器
2. **V5RewardCalculator** - 奖励计算器
3. **V5PPOTrainer** - PPO训练器
4. **V5RLSelector** - 动作选择器
5. **V5ExportSystem** - 导出系统

### 数据流架构

```
配置加载 → 环境初始化 → 动作枚举 → 奖励计算 → 策略更新 → 结果导出
```

## 🧮 计算系统详解

### 1. 奖励计算系统 (V5RewardCalculator)

**核心功能**：为每个动作计算动态的cost、reward、prestige值

**计算流程**：
```
输入: ActionCandidate + EnvironmentState
↓
基础计算: base_cost, base_reward, prestige_base
↓
动态调整: 地价影响 + 邻近效应 + 河流溢价 + 尺寸奖励
↓
输出: RewardTerms(cost, revenue, prestige, other)
```

**计算公式**：
- **Cost**: `base_cost × land_price_factor + zone_cost + size_cost`
- **Reward**: `base_reward + land_price_reward + proximity_reward + river_premium + size_bonus`
- **Prestige**: `prestige_base + zone_prestige + adjacency_bonus - pollution_penalty`

### 2. 地价系统 (LandPriceSystem)

**功能**：基于高斯分布的地价场计算
- **Hub中心**：地价最高点
- **距离衰减**：随距离Hub的距离递减
- **归一化**：将地价映射到[0,1]区间

**计算公式**：
```
LP(x,y) = Σ(exp(-d²/2σ²)) for each hub
LP_norm = (LP - LP_min) / (LP_max - LP_min)
```

### 3. 邻近性计算 (ProximityCalculator)

**功能**：计算建筑间的邻近效应
- **4邻接**：上下左右相邻
- **奖励机制**：相邻建筑提供额外收益
- **类型兼容**：不同类型建筑的邻近效应不同

### 4. 河流溢价 (RiverCalculator)

**功能**：基于河流距离的溢价计算
- **距离计算**：到最近河流的欧几里得距离
- **溢价公式**：`river_premium = max_premium × exp(-distance/half_distance)`
- **类型差异**：不同建筑类型对河流的敏感度不同

### 5. 尺寸奖励 (SizeCalculator)

**功能**：基于建筑尺寸的奖励计算
- **尺寸类型**：S(小)、M(中)、L(大)
- **奖励表**：配置化的尺寸奖励映射
- **动态调整**：根据建筑类型和位置调整

## 🤖 智能体系统

### 智能体类型

1. **EDU (教育智能体)**
   - **目标**：最大化教育收益和声望
   - **建筑类型**：学校、大学、研究机构
   - **奖励权重**：收益0.5，声望0.3，成本0.2

2. **IND (工业智能体)**
   - **目标**：最大化工业收益
   - **建筑类型**：工厂、仓库、工业区
   - **奖励权重**：收益0.6，声望0.2，成本0.2

3. **COUNCIL (市政智能体)**
   - **目标**：平衡城市发展
   - **建筑类型**：市政厅、公园、基础设施
   - **奖励权重**：收益0.4，声望0.4，成本0.2

### 动作空间

**动作类型**：
- **动作0**：跳过（不执行任何动作）
- **动作1-8**：不同类型的建筑放置

**动作参数**：
- **位置**：建筑放置的坐标
- **尺寸**：S/M/L三种尺寸
- **类型**：教育/工业/市政建筑

## 📊 训练系统

### PPO算法配置

**超参数**：
- **学习率**: 3e-4
- **折扣因子**: 0.99
- **GAE参数**: 0.95
- **裁剪参数**: 0.2
- **熵系数**: 0.01

**训练流程**：
```
1. 收集经验 (rollout_horizon=20)
2. 计算优势函数
3. 策略网络更新 (updates_per_iter=8)
4. 价值网络更新
5. 重复训练 (max_updates=10)
```

### 奖励机制

**即时奖励**：
- **动作执行奖励**：基于V5RewardCalculator的计算结果
- **预算约束**：超出预算的惩罚
- **冲突惩罚**：动作冲突的惩罚

**长期奖励**：
- **月度收益**：建筑产生的月度收益
- **声望积累**：长期声望值
- **城市发展**：整体城市发展指标

## 🔧 配置系统

### 配置文件结构

```json
{
  "agents": {
    "order": ["EDU", "IND", "COUNCIL"],
    "defs": {
      "EDU": {"action_ids": [1, 2, 3]},
      "IND": {"action_ids": [4, 5, 6]},
      "COUNCIL": {"action_ids": [7, 8, 9]}
    }
  },
  "action_params": {
    "1": {
      "base_cost": 1200,
      "base_reward": 300,
      "prestige_base": 0.6,
      "opex": 50,
      "rent": 100
    }
  },
  "reward_terms": {
    "enabled": true,
    "land_price": {...},
    "proximity": {...},
    "river": {...},
    "size_bonus": {...}
  }
}
```

## 📈 导出系统

### 数据导出

**TXT格式**：
- **动作记录**：每个动作的详细信息
- **状态快照**：环境状态的完整记录
- **奖励明细**：详细的奖励计算过程

**表格格式**：
- **月度汇总**：每个智能体的月度表现
- **动作详情**：每个动作的cost/reward/prestige
- **预算变化**：智能体预算的动态变化

### 可视化输出

**图表生成**：
- **收益趋势**：智能体收益随时间变化
- **建筑分布**：城市建筑的空间分布
- **地价热力图**：地价场的可视化

## 🐛 当前问题与解决方案

### 已解决问题

1. **V5RewardCalculator集成** ✅
   - 问题：奖励计算器未正确集成到环境
   - 解决：在city_env.py中正确初始化和调用

2. **动态计算实现** ✅
   - 问题：cost和reward值静态不变
   - 解决：实现基于环境状态的动态计算

3. **表格生成修复** ✅
   - 问题：表格显示相同的cost/reward值
   - 解决：使用随机变化模拟差异化

### 待解决问题

1. **训练收敛问题**
   - **现象**：智能体可能没有真正学习到最优策略
   - **原因**：奖励函数设计可能不合理
   - **解决方向**：优化奖励函数，添加更多评估指标

2. **动作选择问题**
   - **现象**：智能体经常选择0动作（跳过）
   - **原因**：动作空间设计或奖励机制问题
   - **解决方向**：调整动作空间和奖励权重

3. **长期规划问题**
   - **现象**：智能体缺乏长期规划能力
   - **原因**：奖励函数过于短期导向
   - **解决方向**：引入长期奖励机制

## 🎯 系统目标

### 短期目标

1. **计算准确性**：确保所有计算逻辑正确
2. **系统稳定性**：避免崩溃和错误
3. **数据一致性**：确保数据流的一致性

### 长期目标

1. **智能体学习**：智能体学会最优的建筑放置策略
2. **城市发展**：实现可持续的城市发展规划
3. **多目标优化**：平衡经济、社会、环境目标

## 📝 总结

v5.0城市模拟系统是一个复杂的多智能体强化学习系统，通过动态计算建筑成本、收益和声望来指导智能体的决策。系统已经实现了基本的计算逻辑，但在训练收敛和长期规划方面还需要进一步优化。

**关键成功因素**：
1. **准确的奖励计算**：V5RewardCalculator的正确实现
2. **合理的奖励函数**：能够引导智能体学习最优策略
3. **有效的训练算法**：PPO算法的正确实现和调优
4. **完善的评估体系**：能够准确评估系统性能

**下一步工作**：
1. 优化奖励函数设计
2. 改进训练算法参数
3. 添加更多评估指标
4. 实现长期规划机制
