#!/usr/bin/env python3
"""
IDEç«¯è®­ç»ƒè„šæœ¬ - ä½¿ç”¨FlaskæœåŠ¡å™¨ä¸Šä¼ çš„åœ°å½¢è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒ
"""

import requests
import json
import numpy as np
import time
from envs.terrain_road_env import TerrainRoadEnvironment
from agents.terrain_policy import TerrainPolicyNetwork
from training.train_terrain_road import PPOAgent
import torch

def get_terrain_from_flask(flask_url="http://localhost:5000"):
    """ä»FlaskæœåŠ¡å™¨è·å–åœ°å½¢æ•°æ®"""
    try:
        print("ğŸŒ ä»FlaskæœåŠ¡å™¨è·å–åœ°å½¢æ•°æ®...")
        
        response = requests.get(f"{flask_url}/get_terrain")
        
        if response.status_code == 200:
            result = response.json()
            terrain_data = result.get('terrain_data')
            
            if terrain_data:
                print("âœ… æˆåŠŸè·å–åœ°å½¢æ•°æ®!")
                return terrain_data
            else:
                print("âŒ æœåŠ¡å™¨ä¸Šæ²¡æœ‰åœ°å½¢æ•°æ®")
                return None
        else:
            print(f"âŒ è·å–åœ°å½¢æ•°æ®å¤±è´¥: {response.status_code}")
            return None
            
    except requests.exceptions.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ°FlaskæœåŠ¡å™¨")
        print("è¯·ç¡®ä¿æœåŠ¡å™¨å·²å¯åŠ¨: python main.py")
        return None
    except Exception as e:
        print(f"âŒ è·å–åœ°å½¢æ•°æ®å‡ºé”™: {e}")
        return None

def create_terrain_file(terrain_data, filename="uploaded_terrain.json"):
    """å°†åœ°å½¢æ•°æ®ä¿å­˜ä¸ºæ–‡ä»¶"""
    try:
        with open(filename, 'w') as f:
            json.dump(terrain_data, f, indent=2)
        print(f"ğŸ’¾ åœ°å½¢æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        return filename
    except Exception as e:
        print(f"âŒ ä¿å­˜åœ°å½¢æ–‡ä»¶å¤±è´¥: {e}")
        return None

def train_with_uploaded_terrain(terrain_data, training_config=None):
    """ä½¿ç”¨ä¸Šä¼ çš„åœ°å½¢æ•°æ®è¿›è¡Œè®­ç»ƒ"""
    print("ğŸ¯ å¼€å§‹ä½¿ç”¨ä¸Šä¼ çš„åœ°å½¢è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒ...")
    
    # é»˜è®¤è®­ç»ƒé…ç½®
    if training_config is None:
        training_config = {
            'total_timesteps': 10000,
            'learning_rate': 3e-4,
            'batch_size': 64,
            'n_steps': 2048,
            'n_epochs': 10,
            'gamma': 0.99,
            'gae_lambda': 0.95,
            'clip_range': 0.2,
            'ent_coef': 0.01,
            'vf_coef': 0.5,
            'max_grad_norm': 0.5
        }
    
    try:
        # åˆ›å»ºç¯å¢ƒï¼ˆä½¿ç”¨ä¸Šä¼ çš„åœ°å½¢ï¼‰
        env = TerrainRoadEnvironment(
            mesh_file=None,  # ä¸ä½¿ç”¨æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨æ•°æ®
            grid_size=tuple(terrain_data.get('grid_size', (50, 50))),
            max_steps=1000,
            render_mode=None  # è®­ç»ƒæ—¶ä¸æ¸²æŸ“
        )
        
        # æ‰‹åŠ¨è®¾ç½®åœ°å½¢æ•°æ®
        env.height_map = np.array(terrain_data['height_map'])
        env._generate_terrain_from_height()  # æ ¹æ®é«˜ç¨‹ç”Ÿæˆåœ°å½¢ç±»å‹
        env._set_agent_and_target()  # é‡æ–°è®¾ç½®æ™ºèƒ½ä½“å’Œç›®æ ‡ä½ç½®
        
        print(f"ğŸ“Š åœ°å½¢ä¿¡æ¯:")
        print(f"   ç½‘æ ¼å¤§å°: {env.grid_size}")
        print(f"   é«˜ç¨‹èŒƒå›´: {env.height_map.min():.2f} - {env.height_map.max():.2f}")
        print(f"   åœ°å½¢åˆ†å¸ƒ: {env.get_terrain_info()['terrain_distribution']}")
        
        # åˆ›å»ºç­–ç•¥ç½‘ç»œ
        policy_net = TerrainPolicyNetwork(
            grid_size=env.grid_size,
            action_space=env.action_space
        )
        
        print("ğŸ§  ç­–ç•¥ç½‘ç»œå·²åˆ›å»º")
        print(f"   ç½‘ç»œå‚æ•°æ•°é‡: {sum(p.numel() for p in policy_net.parameters())}")
        
        # å¼€å§‹è®­ç»ƒ
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        print("=" * 50)
        
        # ç®€å•çš„è®­ç»ƒå¾ªç¯ç¤ºä¾‹
        optimizer = torch.optim.Adam(policy_net.parameters(), lr=training_config['learning_rate'])
        
        episode_rewards = []
        for episode in range(100):  # è®­ç»ƒ100ä¸ªepisode
            obs, _ = env.reset()
            episode_reward = 0
            done = False
            
            while not done:
                # è·å–åŠ¨ä½œ
                obs_tensor = {k: torch.FloatTensor(v).unsqueeze(0) for k, v in obs.items()}
                action_logits, _ = policy_net(obs_tensor)
                action_probs = torch.softmax(action_logits, dim=-1)
                action = torch.multinomial(action_probs, 1).item()
                
                # æ‰§è¡ŒåŠ¨ä½œ
                obs, reward, done, truncated, info = env.step(action)
                episode_reward += reward
                
                if truncated:
                    done = True
            
            episode_rewards.append(episode_reward)
            
            if (episode + 1) % 10 == 0:
                avg_reward = np.mean(episode_rewards[-10:])
                print(f"Episode {episode + 1}/100, å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ¯ æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(episode_rewards[-10:]):.2f}")
        
        return env, policy_net, episode_rewards
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return None, None, None

def test_trained_agent(env, policy_net, num_episodes=5):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“"""
    print(f"\nğŸ§ª æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“ ({num_episodes} ä¸ªepisode)...")
    
    test_rewards = []
    success_count = 0
    
    for episode in range(num_episodes):
        obs, _ = env.reset()
        episode_reward = 0
        done = False
        steps = 0
        
        while not done and steps < env.max_steps:
            # è·å–åŠ¨ä½œ
            obs_tensor = torch.FloatTensor(obs['height_map']).unsqueeze(0)
            action_logits, _ = policy_net({'height_map': obs_tensor})
            action_probs = torch.softmax(action_logits, dim=-1)
            action = torch.argmax(action_probs, dim=1).item()
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, truncated, info = env.step(action)
            episode_reward += reward
            steps += 1
            
            if truncated:
                done = True
        
        test_rewards.append(episode_reward)
        if done and steps < env.max_steps:
            success_count += 1
        
        print(f"Episode {episode + 1}: å¥–åŠ±={episode_reward:.2f}, æ­¥æ•°={steps}")
    
    success_rate = success_count / num_episodes
    avg_reward = np.mean(test_rewards)
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    print(f"   æˆåŠŸç‡: {success_rate:.2%}")
    
    return test_rewards, success_rate

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ IDEç«¯åœ°å½¢å¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
    print("=" * 50)
    
    # 1. ä»FlaskæœåŠ¡å™¨è·å–åœ°å½¢æ•°æ®
    terrain_data = get_terrain_from_flask()
    if not terrain_data:
        print("âŒ æ— æ³•è·å–åœ°å½¢æ•°æ®ï¼Œé€€å‡ºè®­ç»ƒ")
        return
    
    # 2. ä¿å­˜åœ°å½¢æ•°æ®åˆ°æ–‡ä»¶
    terrain_file = create_terrain_file(terrain_data)
    if not terrain_file:
        print("âŒ æ— æ³•ä¿å­˜åœ°å½¢æ–‡ä»¶ï¼Œé€€å‡ºè®­ç»ƒ")
        return
    
    # 3. ä½¿ç”¨åœ°å½¢æ•°æ®è¿›è¡Œè®­ç»ƒ
    env, policy_net, training_rewards = train_with_uploaded_terrain(terrain_data)
    if env is None:
        print("âŒ è®­ç»ƒå¤±è´¥")
        return
    
    # 4. æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    test_rewards, success_rate = test_trained_agent(env, policy_net)
    
    # 5. ä¿å­˜è®­ç»ƒç»“æœ
    results = {
        'terrain_info': terrain_data,
        'training_rewards': training_rewards,
        'test_rewards': test_rewards,
        'success_rate': success_rate,
        'timestamp': time.time()
    }
    
    with open('training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: training_results.json")
    print("âœ… è®­ç»ƒæµç¨‹å®Œæˆ!")

if __name__ == "__main__":
    main()
