你该做的（按影响力排序）

确保最后一层是“线性直出 logits”，没有激活/归一化

去掉 tanh/sigmoid/ReLU/LayerNorm/BatchNorm/Dropout 等紧跟在 最后 Linear 后的层。

特别是 BN/LayerNorm 放在最后会把方差压回到 ≈0。

重初始化最后一层（把 logits 方差拉起来）

# 以 actor 最后一层 Linear 为例：actor.last
torch.nn.init.orthogonal_(actor.last.weight, gain=0.5)  # 先用 0.5；太尖再降
torch.nn.init.zeros_(actor.last.bias)


目标：让 loc.std 从 3e-05 提到 ~1e-2 ~ 1e-1 量级。

临时加“温度缩放”让策略先动起来（验证用，后续可撤）

tau = 0.5   # 先 0.5；若 still flat，试 0.25
local_logits = local_logits / tau


预期：KL 立刻抬到 1e-3~1e-2，clip_fraction > 0。

关闭会抹平方差的正则

训练若用 weight decay，对 actor 暂设为 0 或 ≤1e-5；

关闭 Dropout（或把 p→0）；

BatchNorm 在 batch=1 时不要用在 actor 路径（会退化）。

输入缩放到合理范围（避免前层饱和）

你现在 embed.std≈52，对含 tanh/sigmoid 的网络会饱和。

给 state_embed 做 running normalization：

state_embed = (state_embed - m) / (s + 1e-5)
state_embed = state_embed.clamp(-5, 5)


或者把第一层的初始化/权重尺度调大一些以适配该范围，但更推荐归一化输入。

快速“探针”验证（加这行，看是否对症）

重初始化 + 温度后，再打印一次同批数据的探针：

print(f"[probe] embed.std={std_embed:.4g} | loc.std={local_before.std().item():.4g} | KL_before={kl_before:.3g}")


健康迹象：

loc.std 从 3e-05 → ≥1e-3；

KL_before 从 1e-05 → ~1e-3~1e-2；

训练日志出现 clip_fraction ~ 0.1~0.3，entropy 开始缓慢下降。

常见“隐藏杀手”再核一遍

最后一层后面无任何激活/BN/LN（真的没有）。

local_logits 确实来自 actor(state_embed)，没有 .detach()。

没有把 critic 的正则 绑到 actor 的参数组。

小 batch 训练时，不要让 BN 参与 actor（eval 模式也可能有偏移问题）。