温度/重初始化没有真正生效在“new_logp 的那条计算路径”上（改对了别处；或采样/更新两端都同样缩放把效果抵消了）。

actor 头（最后一层）对局部 logits 的影响近乎 0（最后层未被优化、被正则/归一化抹平，或根本没连到你用的 local_logits）。

你统计的 KL/ratio 是“更新前”的（或前后复用了同一个 dist/logits），所以永远≈0。

下面给你一套“能一锤定音”的极短诊断 + 修正。照做就能定位到点子上。

A. 先确认“温度/after-KL”真的走到了计算路径

把这 6 行插在你创建 dist_new = Categorical(logits=...) 前后（就那一处！）：

# --- 在构造 dist_new 之前 ---
print("[chk] local_logits.std(before) =", local_logits.std().item())

# 温度缩放（如果你要用）
tau = 0.5  # 先 0.5，看到KL动就可以再调
local_logits = local_logits / tau
print("[chk] tau =", tau, " local_logits.std(after) =", local_logits.std().item())

dist_new = torch.distributions.Categorical(logits=local_logits)
new_logp = dist_new.log_prob(action_idx_tensor)

# --- 在 step() 之后，重新前向一次，打印 AFTER KL ---
with torch.no_grad():
    logits_after = actor(state_embed)
    local_after  = logits_after[0, :K]  # 或 subset_ids
    dist_after   = torch.distributions.Categorical(logits=local_after / tau)  # 注意与上面一致
    newlp_after  = dist_after.log_prob(action_idx_tensor)
    kl_after     = (old_logp - newlp_after).mean().item()
print("[chk] KL_after =", kl_after)


你应该看到：

local_logits.std(before) 明显 > 0（哪怕只有 1e-4）；

after 比 before 更大（有 τ 缩放）；

KL_after 不是 0（通常 1e-3~1e-2）。
如果这里仍是 0，基本可以断定：温度没用在 new_logp 的这条路径（改错地方了）。

B. 检查“最后一层是否真的在被优化、且影响到 local_logits”

最后一层梯度/更新是否非零（逐层看）

# backward 之后、step 前
for name, p in actor.named_parameters():
    if "last" in name or "policy_head" in name:
        g = (p.grad.norm().item() if p.grad is not None else 0.0)
        print(f"[grad] {name}: grad_norm={g:.3e}")

# 记录 step 前后的权重差（只看最后层）
last_params_before = {name: p.detach().clone()
                      for name, p in actor.named_parameters()
                      if "last" in name or "policy_head" in name}
actor_optimizer.step()
diff_sum = 0.0
for name, p in actor.named_parameters():
    if name in last_params_before:
        diff = (p.detach() - last_params_before[name]).abs().sum().item()
        diff_sum += diff
print(f"[upd] last_layer_param_diff={diff_sum:.6f}")


若 grad_norm=0 或 param_diff≈0 → 最后一层没被优化器绑定/被 requires_grad=False/梯度没传到头；
→ 检查 optimizer.param_groups 是否包含最后一层，p.requires_grad == True。

local logits 前后是否真的在变

with torch.no_grad():
    l1 = actor(state_embed)[0, :K]
# … backward+step …
with torch.no_grad():
    l2 = actor(state_embed)[0, :K]
print("[probe] Δlocal_logits_L2 =", (l2 - l1).pow(2).mean().sqrt().item())


若 Δlocal_logits_L2 ≈ 0 → 参数在动但没打到这段 logits（常见：最后层后面加了 BN/LN/Dropout，或你索引的 :K 不是头的输出；请保证“最后一层线性直出 logits，无激活/归一化”）。

C. 两个“立竿见影”的修正（你可以直接上）
C1. 确保最后一层无激活/无归一化，并重新初始化
# 最后一层 Linear，不要接 ReLU/tanh/sigmoid/LayerNorm/BatchNorm/Dropout
torch.nn.init.orthogonal_(actor.last.weight, gain=0.5)  # 把 gain 提高一点，拉大方差
torch.nn.init.zeros_(actor.last.bias)


目标：把你之前 loc.std≈3e-05 拉到 ≥1e-3 量级。

C2. 在 new_logp 构造处固定使用温度 tau<1

（放在上面的 A 段位置，不要放别处！）

tau = 0.5  # 先 0.5；若 KL 还是极小，试 0.25
local_logits = local_logits / tau
dist_new = torch.distributions.Categorical(logits=local_logits)

D. 你提供的调试脚本里还有两个“可能没生效”的点

debug_kl_fix.py 写配置用的路径是 "configs/city_config_v4_1.json"，而你最开始的文件在根目录 city_config_v4_1.json。如果运行时加载的不是 configs/… 这份，ent_coef=0、batch 改动都不会生效。请确认训练入口加载的到底是哪个路径（或在日志里打印出使用的配置路径）。

debug_kl_fix

脚本里对 ppo_trainer.py 的 字符串替换 是脆弱的：如果目标代码和你写的锚点不完全一致（空格/缩进/注释不同），插入可能没成功。建议直接在你实际创建 dist_new 的那几行插入上面的 [chk] 打印和 tau 缩放，肉眼确认。

E. 最后一个“必杀定位”：手动扰动参数看 KL 是否跳起
# step 前，手动抖动 actor 参数（只为诊断，不反传）
old = [p.detach().clone() for p in actor.parameters()]
with torch.no_grad():
    for p in actor.parameters():
        p.add_(0.01 * torch.randn_like(p))

# 重新前向→算 KL_after
with torch.no_grad():
    logits_jit = actor(state_embed)
    local_jit  = logits_jit[0, :K]
    dist_jit   = torch.distributions.Categorical(logits=local_jit / tau)
    kl_jit = (old_logp - dist_jit.log_prob(action_idx_tensor)).mean().item()
print("[probe] KL after param jitter =", kl_jit)

# 还原
for p, q in zip(actor.parameters(), old):
    p.copy_(q)


如果 KL 仍≈0：你的 KL 计算路径用到的仍不是正确的 logits（或前后复用了同一 dist）。

如果 KL 跳到 1e-3~1e-2：链路没问题，只是“最后一层太平/温度太高”。按 C1+C2 处理即可。

TL;DR

你现在的问题不是“数据/超参”，而是new_logp 的那条路径没有吃到“尖一点的 logits”。

按 A（打印 std/tau/KL_after） + B（最后层 grad/Δlocal） 两步，定位 100%；

然后用 C1（最后层直连+重初始化） + C2（在 dist_new 处加 τ），KL 和 clip 就会起来。