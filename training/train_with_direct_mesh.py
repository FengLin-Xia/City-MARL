#!/usr/bin/env python3
"""
ä½¿ç”¨ç›´æ¥Meshå¤„ç†ç»“æœçš„è®­ç»ƒè„šæœ¬
"""

import numpy as np
import json
import os
import torch
import torch.nn as nn
import torch.optim as optim
from envs.terrain_grid_nav_env import TerrainGridNavEnv
from agents.ppo_terrain_agent import TerrainPPOAgent
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class DirectMeshTrainer:
    """ç›´æ¥Meshåœ°å½¢è®­ç»ƒå™¨"""
    
    def __init__(self, terrain_file: str = "data/terrain/terrain_direct_mesh_fixed.json"):
        self.terrain_file = terrain_file
        self.terrain_data = None
        self.env = None
        self.agent = None
        self.training_stats = {
            'episode_rewards': [],
            'success_rates': [],
            'episode_lengths': [],
            'total_episodes': 0,
            'total_success': 0,
            'final_success_rate': 0.0,
            'final_avg_reward': 0.0,
            'final_avg_length': 0.0,
            'start_point': None,
            'goal_point': None,
            'terrain_file': terrain_file
        }
        
    def load_terrain_data(self):
        """åŠ è½½åœ°å½¢æ•°æ®"""
        if not os.path.exists(self.terrain_file):
            print(f"âŒ åœ°å½¢æ–‡ä»¶ä¸å­˜åœ¨: {self.terrain_file}")
            return False
        
        with open(self.terrain_file, 'r') as f:
            self.terrain_data = json.load(f)
        
        print(f"âœ… æˆåŠŸåŠ è½½åœ°å½¢æ•°æ®")
        print(f"   ç½‘æ ¼å°ºå¯¸: {self.terrain_data['grid_size']}")
        print(f"   æœ‰æ•ˆç‚¹æ•°: {self.terrain_data['valid_points_count']}")
        print(f"   è¦†ç›–ç‡: {self.terrain_data['coverage_percentage']:.1f}%")
        
        return True
    
    def find_land_points(self, height_threshold: float = 0.0) -> tuple:
        """åœ¨é™†åœ°ä¸Šæ‰¾åˆ°åˆé€‚çš„èµ·å§‹ç‚¹å’Œç»ˆç‚¹"""
        height_map = np.array(self.terrain_data['height_map'])
        mask = np.array(self.terrain_data['mask'])
        
        # æ‰¾åˆ°æ‰€æœ‰æœ‰æ•ˆçš„é™†åœ°ç‚¹
        valid_indices = np.where((mask) & (height_map > height_threshold))
        
        if len(valid_indices[0]) < 2:
            print("âŒ æ²¡æœ‰è¶³å¤Ÿçš„é™†åœ°ç‚¹")
            return None, None
        
        # éšæœºé€‰æ‹©ä¸¤ä¸ªä¸åŒçš„ç‚¹
        indices = np.random.choice(len(valid_indices[0]), 2, replace=False)
        
        start_idx = (valid_indices[0][indices[0]], valid_indices[1][indices[0]])
        goal_idx = (valid_indices[0][indices[1]], valid_indices[1][indices[1]])
        
        start_height = height_map[start_idx]
        goal_height = height_map[goal_idx]
        
        print(f"âœ… æ‰¾åˆ°èµ·å§‹ç‚¹å’Œç»ˆç‚¹")
        print(f"   èµ·å§‹ç‚¹: {start_idx}, é«˜ç¨‹: {start_height:.2f}")
        print(f"   ç»ˆç‚¹: {goal_idx}, é«˜ç¨‹: {goal_height:.2f}")
        
        return start_idx, goal_idx
    
    def create_environment(self, start_point: tuple, goal_point: tuple):
        """åˆ›å»ºç¯å¢ƒ"""
        height_map = np.array(self.terrain_data['height_map'])
        mask = np.array(self.terrain_data['mask'])
        
        # åˆ›å»ºè‡ªå®šä¹‰åœ°å½¢æ•°æ® - åªä¼ é€’é«˜ç¨‹å›¾
        custom_terrain = height_map
        
        grid_size = self.terrain_data['grid_size']
        self.env = TerrainGridNavEnv(
            H=grid_size[0],
            W=grid_size[1],
            max_steps=400,
            custom_terrain=custom_terrain,
            fixed_start=start_point,
            fixed_goal=goal_point,
            slope_penalty_weight=0.0,  # æš‚æ—¶ç§»é™¤åœ°å½¢æƒ©ç½š
            height_penalty_weight=0.0
        )
        
        print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        print(f"   ç½‘æ ¼å°ºå¯¸: {grid_size}")
        print(f"   æœ€å¤§æ­¥æ•°: {self.env.max_steps}")
    
    def create_agent(self):
        """åˆ›å»ºæ™ºèƒ½ä½“"""
        # è®¡ç®—çŠ¶æ€ç»´åº¦ï¼ˆåŸºç¡€çŠ¶æ€ç‰¹å¾ï¼‰
        # position(2) + goal(2) + distance_to_goal(1) + current_height(1) + 
        # goal_height(1) + height_difference(1) + current_slope(1) + action_mask(4) = 13
        state_dim = 13
        action_dim = self.env.action_space.n
        
        self.agent = TerrainPPOAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=128,
            lr=1e-3,
            gamma=0.99,
            gae_lambda=0.95,
            clip_ratio=0.2,
            target_kl=0.01,
            train_pi_iters=80,
            train_v_iters=80,
            lam=0.97,
            max_grad_norm=0.5
        )
        
        print(f"âœ… æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
        print(f"   çŠ¶æ€ç»´åº¦: {state_dim}")
        print(f"   åŠ¨ä½œç»´åº¦: {action_dim}")
    
    def train(self, num_episodes: int = 1000, eval_interval: int = 50):
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œæ€»episodes: {num_episodes}")
        print("=" * 50)
        
        for episode in range(1, num_episodes + 1):
            # æ”¶é›†ä¸€ä¸ªepisodeçš„æ•°æ®
            states, actions, rewards, values, log_probs, dones, path, success = self.agent.collect_episode(self.env)
            
            # æ›´æ–°æ™ºèƒ½ä½“
            if len(states) > 0:
                self.agent.update(states, actions, rewards, values, log_probs, dones)
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            episode_reward = rewards.sum().item()
            episode_length = len(rewards)
            episode_success = success
            
            self.training_stats['episode_rewards'].append(episode_reward)
            self.training_stats['episode_lengths'].append(episode_length)
            self.training_stats['total_episodes'] += 1
            
            if episode_success:
                self.training_stats['total_success'] += 1
            
            # è®¡ç®—å½“å‰æˆåŠŸç‡
            current_success_rate = self.training_stats['total_success'] / self.training_stats['total_episodes']
            self.training_stats['success_rates'].append(current_success_rate)
            
            # å®šæœŸè¾“å‡ºè®­ç»ƒçŠ¶æ€
            if episode % eval_interval == 0:
                recent_rewards = self.training_stats['episode_rewards'][-eval_interval:]
                recent_lengths = self.training_stats['episode_lengths'][-eval_interval:]
                recent_successes = sum(1 for r in recent_rewards if r > 0)
                
                avg_reward = np.mean(recent_rewards)
                avg_length = np.mean(recent_lengths)
                recent_success_rate = recent_successes / eval_interval
                
                print(f"Episode {episode:4d} | "
                      f"æˆåŠŸç‡: {current_success_rate:.1%} | "
                      f"æœ€è¿‘æˆåŠŸç‡: {recent_success_rate:.1%} | "
                      f"å¹³å‡å¥–åŠ±: {avg_reward:6.1f} | "
                      f"å¹³å‡é•¿åº¦: {avg_length:4.1f}")
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        self.training_stats['final_success_rate'] = current_success_rate
        self.training_stats['final_avg_reward'] = np.mean(self.training_stats['episode_rewards'])
        self.training_stats['final_avg_length'] = np.mean(self.training_stats['episode_lengths'])
        
        print("\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"   æ€»episodes: {self.training_stats['total_episodes']}")
        print(f"   æˆåŠŸæ¬¡æ•°: {self.training_stats['total_success']}")
        print(f"   æœ€ç»ˆæˆåŠŸç‡: {self.training_stats['final_success_rate']:.1%}")
        print(f"   å¹³å‡å¥–åŠ±: {self.training_stats['final_avg_reward']:.2f}")
        print(f"   å¹³å‡è·¯å¾„é•¿åº¦: {self.training_stats['final_avg_length']:.1f}")
    
    def save_training_data(self, output_file: str):
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        def convert_numpy_types(obj):
            """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            elif isinstance(obj, tuple):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj
        
        # è½¬æ¢æ‰€æœ‰numpyç±»å‹
        training_data = convert_numpy_types(self.training_stats)
        
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w') as f:
            json.dump(training_data, f, indent=2)
        
        print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    def visualize_training(self, save_path: str = None):
        """å¯è§†åŒ–è®­ç»ƒç»“æœ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('ç›´æ¥Meshåœ°å½¢è®­ç»ƒç»“æœ', fontsize=16)
        
        episodes = list(range(1, len(self.training_stats['episode_rewards']) + 1))
        
        # 1. æˆåŠŸç‡å˜åŒ–
        axes[0, 0].plot(episodes, self.training_stats['success_rates'], 'b-', linewidth=2)
        axes[0, 0].set_title('æˆåŠŸç‡å˜åŒ–')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('æˆåŠŸç‡')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_ylim(0, 1)
        
        # 2. å¥–åŠ±å˜åŒ–
        window_size = min(50, len(self.training_stats['episode_rewards']) // 10)
        if window_size > 1:
            moving_avg = np.convolve(self.training_stats['episode_rewards'], 
                                   np.ones(window_size)/window_size, mode='valid')
            moving_avg_episodes = episodes[window_size-1:]
            axes[0, 1].plot(moving_avg_episodes, moving_avg, 'r-', linewidth=2, 
                           label=f'ç§»åŠ¨å¹³å‡({window_size})')
        
        axes[0, 1].plot(episodes, self.training_stats['episode_rewards'], 'gray', alpha=0.3, linewidth=0.5)
        axes[0, 1].set_title('å¥–åŠ±å˜åŒ–')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('æ€»å¥–åŠ±')
        axes[0, 1].grid(True, alpha=0.3)
        if window_size > 1:
            axes[0, 1].legend()
        
        # 3. è·¯å¾„é•¿åº¦å˜åŒ–
        if window_size > 1:
            moving_avg_length = np.convolve(self.training_stats['episode_lengths'], 
                                          np.ones(window_size)/window_size, mode='valid')
            axes[1, 0].plot(moving_avg_episodes, moving_avg_length, 'g-', linewidth=2, 
                           label=f'ç§»åŠ¨å¹³å‡({window_size})')
        
        axes[1, 0].plot(episodes, self.training_stats['episode_lengths'], 'gray', alpha=0.3, linewidth=0.5)
        axes[1, 0].set_title('è·¯å¾„é•¿åº¦å˜åŒ–')
        axes[1, 0].set_xlabel('Episode')
        axes[1, 0].set_ylabel('è·¯å¾„é•¿åº¦')
        axes[1, 0].grid(True, alpha=0.3)
        if window_size > 1:
            axes[1, 0].legend()
        
        # 4. å¥–åŠ±åˆ†å¸ƒ
        axes[1, 1].hist(self.training_stats['episode_rewards'], bins=30, alpha=0.7, 
                       color='skyblue', edgecolor='black')
        axes[1, 1].axvline(np.mean(self.training_stats['episode_rewards']), color='red', 
                          linestyle='--', linewidth=2, 
                          label=f'å¹³å‡å€¼: {np.mean(self.training_stats["episode_rewards"]):.2f}')
        axes[1, 1].set_title('å¥–åŠ±åˆ†å¸ƒ')
        axes[1, 1].set_xlabel('æ€»å¥–åŠ±')
        axes[1, 1].set_ylabel('é¢‘æ¬¡')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"è®­ç»ƒç»“æœå›¾å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DirectMeshTrainer()
    
    # åŠ è½½åœ°å½¢æ•°æ®
    if not trainer.load_terrain_data():
        return
    
    # æ‰¾åˆ°èµ·å§‹ç‚¹å’Œç»ˆç‚¹
    start_point, goal_point = trainer.find_land_points(height_threshold=0.0)
    if start_point is None or goal_point is None:
        return
    
    # ä¿å­˜èµ·å§‹ç‚¹å’Œç»ˆç‚¹
    trainer.training_stats['start_point'] = list(start_point)
    trainer.training_stats['goal_point'] = list(goal_point)
    
    # åˆ›å»ºç¯å¢ƒ
    trainer.create_environment(start_point, goal_point)
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    trainer.create_agent()
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(num_episodes=1000, eval_interval=50)
    
    # ä¿å­˜è®­ç»ƒæ•°æ®
    trainer.save_training_data("training_data/direct_mesh_training_stats.json")
    
    # å¯è§†åŒ–è®­ç»ƒç»“æœ
    trainer.visualize_training("visualization_output/direct_mesh_training_results.png")


if __name__ == "__main__":
    main()
