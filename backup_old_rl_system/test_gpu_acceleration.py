#!/usr/bin/env python3
"""
GPUåŠ é€Ÿå¼ºåŒ–å­¦ä¹ æµ‹è¯•
éªŒè¯ä»ç¯å¢ƒåˆ°è®­ç»ƒçš„å®Œæ•´GPUåŠ é€Ÿæµç¨‹
"""

import sys
import os
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from envs.terrain_road_env import TerrainRoadEnvironment
from agents.terrain_policy import TerrainPolicyNetwork

def test_device_setup():
    """æµ‹è¯•è®¾å¤‡è®¾ç½®"""
    print("=== è®¾å¤‡è®¾ç½®æµ‹è¯• ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
    print(f"å½“å‰GPU: {torch.cuda.current_device()}")
    print(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
    print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    return device

def test_environment_gpu(device):
    """æµ‹è¯•ç¯å¢ƒGPUåŒ–"""
    print("\n=== ç¯å¢ƒGPUåŒ–æµ‹è¯• ===")
    
    # æŸ¥æ‰¾åœ°å½¢æ•°æ®
    terrain_dir = Path("data/terrain")
    terrain_files = list(terrain_dir.glob("terrain_continuity_boundary_*.json"))
    
    if not terrain_files:
        print("âŒ æœªæ‰¾åˆ°åœ°å½¢æ•°æ®æ–‡ä»¶")
        return None
    
    latest_file = max(terrain_files, key=lambda x: x.stat().st_mtime)
    print(f"ä½¿ç”¨åœ°å½¢æ–‡ä»¶: {latest_file}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = TerrainRoadEnvironment(mesh_file=str(latest_file))
    print(f"ç¯å¢ƒç½‘æ ¼å°ºå¯¸: {env.grid_size}")
    
    # æµ‹è¯•è§‚å¯Ÿæ•°æ®GPUåŒ–
    obs, _ = env.reset()
    print(f"è§‚å¯Ÿç©ºé—´é”®: {list(obs.keys())}")
    
    # å°†è§‚å¯Ÿæ•°æ®ç§»åˆ°GPU
    gpu_obs = {}
    for key, value in obs.items():
        if isinstance(value, np.ndarray):
            gpu_obs[key] = torch.from_numpy(value).to(device)
            print(f"{key}: {value.shape} -> GPU: {gpu_obs[key].device}")
        else:
            gpu_obs[key] = value
    
    # æµ‹è¯•GPUæ•°æ®æ“ä½œ
    height_map_gpu = gpu_obs['height_map']
    print(f"GPUé«˜ç¨‹å›¾ç»Ÿè®¡: æœ€å°å€¼={height_map_gpu.min().item():.2f}, æœ€å¤§å€¼={height_map_gpu.max().item():.2f}")
    
    return env, gpu_obs

def test_network_gpu(device, env):
    """æµ‹è¯•ç½‘ç»œGPUåŒ–"""
    print("\n=== ç½‘ç»œGPUåŒ–æµ‹è¯• ===")
    
    # åˆ›å»ºç­–ç•¥ç½‘ç»œ
    policy_net = TerrainPolicyNetwork(grid_size=env.grid_size, action_space=env.action_space)
    policy_net = policy_net.to(device)
    print(f"ç­–ç•¥ç½‘ç»œå·²ç§»åˆ°: {next(policy_net.parameters()).device}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    obs, _ = env.reset()
    obs_tensor = {}
    for key, value in obs.items():
        if isinstance(value, np.ndarray):
            obs_tensor[key] = torch.from_numpy(value).unsqueeze(0).to(device)  # æ·»åŠ batchç»´åº¦
        else:
            obs_tensor[key] = torch.tensor([value]).to(device)
    
    print("æµ‹è¯•å‰å‘ä¼ æ’­...")
    with torch.no_grad():
        action_probs, value = policy_net(obs_tensor)
    
    print(f"åŠ¨ä½œæ¦‚ç‡å½¢çŠ¶: {action_probs.shape}")
    print(f"ä»·å€¼ä¼°è®¡: {value.item():.4f}")
    print(f"åŠ¨ä½œæ¦‚ç‡è®¾å¤‡: {action_probs.device}")
    print(f"ä»·å€¼ä¼°è®¡è®¾å¤‡: {value.device}")
    
    return policy_net

def test_training_step(device, env, policy_net):
    """æµ‹è¯•è®­ç»ƒæ­¥éª¤"""
    print("\n=== è®­ç»ƒæ­¥éª¤æµ‹è¯• ===")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)
    
    # æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒæ­¥éª¤
    obs, _ = env.reset()
    
    # å‡†å¤‡æ•°æ®
    obs_tensor = {}
    for key, value in obs.items():
        if isinstance(value, np.ndarray):
            obs_tensor[key] = torch.from_numpy(value).unsqueeze(0).to(device)
        else:
            obs_tensor[key] = torch.tensor([value]).to(device)
    
    # å‰å‘ä¼ æ’­
    action_logits, value = policy_net(obs_tensor)
    
    # é‡‡æ ·åŠ¨ä½œ
    action_probs = torch.softmax(action_logits, dim=-1)
    action_dist = torch.distributions.Categorical(action_probs)
    action = action_dist.sample()
    
    # æ‰§è¡ŒåŠ¨ä½œ
    next_obs, reward, done, truncated, info = env.step(action.item())
    
    # è®¡ç®—æŸå¤±ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    loss = -action_dist.log_prob(action) * torch.tensor(reward, device=device)  # ç®€å•çš„ç­–ç•¥æ¢¯åº¦æŸå¤±
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    print(f"åŠ¨ä½œ: {action.item()}")
    print(f"å¥–åŠ±: {reward:.4f}")
    print(f"æŸå¤±: {loss.item():.4f}")
    print(f"æŸå¤±è®¾å¤‡: {loss.device}")
    print("âœ… è®­ç»ƒæ­¥éª¤å®Œæˆ")

def test_performance_comparison(device, env, policy_net):
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("\n=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")
    
    # æµ‹è¯•GPUæ€§èƒ½
    print("æµ‹è¯•GPUæ€§èƒ½...")
    start_time = time.time()
    
    for _ in range(10):
        obs, _ = env.reset()
        obs_tensor = {}
        for key, value in obs.items():
            if isinstance(value, np.ndarray):
                obs_tensor[key] = torch.from_numpy(value).unsqueeze(0).to(device)
            else:
                obs_tensor[key] = torch.tensor([value]).to(device)
        
        with torch.no_grad():
            action_probs, value = policy_net(obs_tensor)
    
    gpu_time = time.time() - start_time
    print(f"GPU 10æ¬¡å‰å‘ä¼ æ’­è€—æ—¶: {gpu_time:.4f}ç§’")
    
    # æµ‹è¯•CPUæ€§èƒ½ï¼ˆå¦‚æœå¯èƒ½ï¼‰
    if device.type == 'cuda':
        print("æµ‹è¯•CPUæ€§èƒ½...")
        policy_net_cpu = policy_net.cpu()
        start_time = time.time()
        
        for _ in range(10):
            obs, _ = env.reset()
            obs_tensor = {}
            for key, value in obs.items():
                if isinstance(value, np.ndarray):
                    obs_tensor[key] = torch.from_numpy(value).unsqueeze(0)
                else:
                    obs_tensor[key] = torch.tensor([value])
            
            with torch.no_grad():
                action_probs, value = policy_net_cpu(obs_tensor)
        
        cpu_time = time.time() - start_time
        print(f"CPU 10æ¬¡å‰å‘ä¼ æ’­è€—æ—¶: {cpu_time:.4f}ç§’")
        print(f"GPUåŠ é€Ÿæ¯”: {cpu_time/gpu_time:.2f}x")
        
        # ç§»å›GPU
        policy_net = policy_net_cpu.to(device)

def test_memory_usage(device):
    """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
    print("\n=== å†…å­˜ä½¿ç”¨æµ‹è¯• ===")
    
    if device.type == 'cuda':
        print(f"GPUæ€»å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"GPUå·²ç”¨å†…å­˜: {torch.cuda.memory_allocated(0) / 1024**3:.3f} GB")
        print(f"GPUç¼“å­˜å†…å­˜: {torch.cuda.memory_reserved(0) / 1024**3:.3f} GB")
        
        # æ¸…ç†ç¼“å­˜
        torch.cuda.empty_cache()
        print("å·²æ¸…ç†GPUç¼“å­˜")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ GPUåŠ é€Ÿå¼ºåŒ–å­¦ä¹ æµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    # 1. è®¾å¤‡è®¾ç½®æµ‹è¯•
    device = test_device_setup()
    
    # 2. ç¯å¢ƒGPUåŒ–æµ‹è¯•
    result = test_environment_gpu(device)
    if result is None:
        print("âŒ ç¯å¢ƒæµ‹è¯•å¤±è´¥ï¼Œé€€å‡º")
        return
    
    env, gpu_obs = result
    
    # 3. ç½‘ç»œGPUåŒ–æµ‹è¯•
    policy_net = test_network_gpu(device, env)
    
    # 4. è®­ç»ƒæ­¥éª¤æµ‹è¯•
    test_training_step(device, env, policy_net)
    
    # 5. æ€§èƒ½å¯¹æ¯”æµ‹è¯•
    test_performance_comparison(device, env, policy_net)
    
    # 6. å†…å­˜ä½¿ç”¨æµ‹è¯•
    test_memory_usage(device)
    
    print("\n" + "=" * 50)
    print("âœ… GPUåŠ é€Ÿæµ‹è¯•å®Œæˆï¼")
    print("ç°åœ¨å¯ä»¥å¼€å§‹GPUåŠ é€Ÿçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒäº†ï¼")

if __name__ == "__main__":
    main()
