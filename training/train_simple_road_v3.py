#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆè·¯å¾„è§„åˆ’è®­ç»ƒè„šæœ¬ V3
æœ€ç®€å•çš„å®ç°ï¼Œä½¿ç”¨åŸºæœ¬ç­–ç•¥æ¢¯åº¦
"""

import sys
import os
import torch
import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Tuple
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from envs.simple_road_env import SimpleRoadEnv
from agents.simple_ppo import SimpleActorCritic


class SimpleRoadTrainerV3:
    """ç®€åŒ–ç‰ˆè·¯å¾„è§„åˆ’è®­ç»ƒå™¨ V3"""
    
    def __init__(self, 
                 dem_size: Tuple[int, int] = (100, 100),
                 max_steps: int = 200,
                 num_episodes: int = 200,
                 save_interval: int = 50):
        
        self.dem_size = dem_size
        self.max_steps = max_steps
        self.num_episodes = num_episodes
        self.save_interval = save_interval
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = SimpleRoadEnv(dem_size=dem_size, max_steps=max_steps)
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.actor_critic = SimpleActorCritic()
        self.optimizer = torch.optim.Adam(self.actor_critic.parameters(), lr=1e-3)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.success_rates = []
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.models_dir = Path("models")
        self.models_dir.mkdir(exist_ok=True)
        
        self.results_dir = Path("data/results")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
    def train_episode(self, episode_num: int) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepisodeï¼ˆæœ€ç®€å•ç‰ˆï¼‰"""
        start_time = time.time()
        
        # é‡ç½®ç¯å¢ƒ
        obs, _ = self.env.reset()
        
        total_reward = 0
        episode_length = 0
        success = False
        
        # å­˜å‚¨episodeæ•°æ®
        log_probs = []
        rewards = []
        
        while episode_length < self.max_steps:
            # è½¬æ¢ä¸ºå¼ é‡
            obs_tensor = {
                'position': torch.FloatTensor(obs['position']).unsqueeze(0),
                'goal': torch.FloatTensor(obs['goal']).unsqueeze(0),
                'local_dem': torch.FloatTensor(obs['local_dem']).unsqueeze(0)
            }
            
            # è·å–åŠ¨ä½œ
            action, log_prob, value = self.actor_critic.get_action(obs_tensor)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, done, truncated, info = self.env.step(action.numpy().squeeze())
            
            # å­˜å‚¨æ•°æ®
            log_probs.append(log_prob)
            rewards.append(reward)
            
            # ç´¯ç§¯å¥–åŠ±
            total_reward += reward
            episode_length += 1
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            if done and not truncated and info.get('reason') == 'reached_goal':
                success = True
                break
            
            # æ£€æŸ¥æ˜¯å¦å¤±è´¥
            if done:
                break
            
            obs = next_obs
        
        # ç®€å•çš„ç­–ç•¥æ¢¯åº¦æ›´æ–°
        if len(log_probs) > 0:
            # è®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±
            log_probs_tensor = torch.stack(log_probs)
            rewards_tensor = torch.FloatTensor(rewards)
            
            # æ ‡å‡†åŒ–å¥–åŠ±
            if len(rewards) > 1:
                rewards_tensor = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)
            
            # ç­–ç•¥æ¢¯åº¦æŸå¤±
            loss = -(log_probs_tensor * rewards_tensor).mean()
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)
            self.optimizer.step()
        
        # æ›´æ–°ç»Ÿè®¡
        self.episode_rewards.append(total_reward)
        self.episode_lengths.append(episode_length)
        self.success_rates.append(1.0 if success else 0.0)
        
        # è®¡ç®—è¿è¡Œæ—¶é—´
        episode_time = time.time() - start_time
        
        # è®¡ç®—å¹³å‡ç»Ÿè®¡
        avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)
        avg_length = np.mean(self.episode_lengths[-100:]) if len(self.episode_lengths) >= 100 else np.mean(self.episode_lengths)
        success_rate = np.mean(self.success_rates[-100:]) if len(self.success_rates) >= 100 else np.mean(self.success_rates)
        
        # æ‰“å°è¿›åº¦
        print(f"Episode {episode_num:4d} | "
              f"å¥–åŠ±: {total_reward:6.1f} | "
              f"æ­¥æ•°: {episode_length:3d} | "
              f"æˆåŠŸ: {'âœ…' if success else 'âŒ'} | "
              f"å¹³å‡å¥–åŠ±: {avg_reward:6.1f} | "
              f"æˆåŠŸç‡: {success_rate*100:5.1f}% | "
              f"ç”¨æ—¶: {episode_time:.1f}s")
        
        return {
            'episode': episode_num,
            'total_reward': total_reward,
            'episode_length': episode_length,
            'success': success,
            'avg_reward': avg_reward,
            'success_rate': success_rate,
            'time': episode_time
        }
    
    def save_model(self, episode_num: int):
        """ä¿å­˜æ¨¡å‹"""
        model_path = self.models_dir / f"simple_road_v3_{episode_num}.pth"
        torch.save({
            'episode': episode_num,
            'actor_critic_state_dict': self.actor_critic.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'episode_rewards': self.episode_rewards,
            'success_rates': self.success_rates
        }, model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    def save_results(self):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        results = {
            'training_config': {
                'dem_size': self.dem_size,
                'max_steps': self.max_steps,
                'num_episodes': self.num_episodes
            },
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'success_rates': self.success_rates
        }
        
        results_path = self.results_dir / f"simple_road_v3_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"ğŸ“Š è®­ç»ƒç»“æœå·²ä¿å­˜: {results_path}")
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒç®€åŒ–ç‰ˆè·¯å¾„è§„åˆ’æ™ºèƒ½ä½“ V3")
        print(f"ğŸ“ DEMå°ºå¯¸: {self.dem_size}")
        print(f"â±ï¸ æœ€å¤§æ­¥æ•°: {self.max_steps}")
        print(f"ğŸ¯ ç›®æ ‡episodes: {self.num_episodes}")
        print("=" * 80)
        
        start_time = time.time()
        
        for episode in range(1, self.num_episodes + 1):
            # è®­ç»ƒä¸€ä¸ªepisode
            episode_info = self.train_episode(episode)
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if episode % self.save_interval == 0:
                self.save_model(episode)
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        
        print("=" * 80)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸ æ€»ç”¨æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
        print(f"ğŸ“ˆ æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(self.episode_rewards[-100:]):.2f}")
        print(f"ğŸ¯ æœ€ç»ˆæˆåŠŸç‡: {np.mean(self.success_rates[-100:])*100:.1f}%")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œç»“æœ
        self.save_model(self.num_episodes)
        self.save_results()
        
        print("ğŸ’¾ æœ€ç»ˆæ¨¡å‹å’Œç»“æœå·²ä¿å­˜ï¼")


if __name__ == "__main__":
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SimpleRoadTrainerV3(
        dem_size=(100, 100),
        max_steps=200,
        num_episodes=100,  # å…ˆè®­ç»ƒ100ä¸ªepisodesæµ‹è¯•
        save_interval=25
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()


