# PPOè®­ç»ƒBugä¿®å¤æ€»ç»“

**æ—¥æœŸï¼š** 2025-10-09  
**çŠ¶æ€ï¼š** âœ… å·²å®Œæˆå¹¶æµ‹è¯•  

---

## ğŸ› **å‘ç°çš„Bug**

### **Bug #1: num_actionsä¸ä¸€è‡´** ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥

**ä½ç½®ï¼š** `trainers/v4_1/ppo_trainer.py`

**é—®é¢˜ï¼š**
```python
# æ”¶é›†ç»éªŒæ—¶ï¼ˆç¬¬198è¡Œï¼‰ï¼š
num_actions = min(5, max_actions)  # ç¡¬ç¼–ç 5

# æ›´æ–°ç­–ç•¥æ—¶ï¼ˆç¬¬346è¡Œï¼‰ï¼š
num_actions = min(len(exp['available_actions']), max_actions)  # ä½¿ç”¨çœŸå®æ•°é‡
```

**å½±å“ï¼š**
- æ”¶é›†æ—¶ç”¨5ä¸ªåŠ¨ä½œè®¡ç®—log_prob
- æ›´æ–°æ—¶ç”¨15ä¸ªåŠ¨ä½œé‡æ–°è®¡ç®—
- æ¦‚ç‡åˆ†å¸ƒå®Œå…¨ä¸åŒ
- **å¯¼è‡´clipç‡99%ï¼**

**ä¿®å¤ï¼š**
```python
# 1. ä¿®æ”¹å‡½æ•°ç­¾åï¼Œæ¥å—num_actionså‚æ•°
def _get_action_log_prob(self, sequence, state, num_actions):
    num_actions = min(num_actions, self.selector.max_actions)
    valid_logits = logits[0, :num_actions]
    # ...

# 2. è°ƒç”¨æ—¶ä¼ å…¥çœŸå®æ•°é‡
old_log_prob = self._get_action_log_prob(selected_seq, state, len(actions))

# 3. ä¿å­˜åˆ°ç»éªŒä¸­
experience['num_actions'] = len(actions)

# 4. æ›´æ–°æ—¶ä½¿ç”¨ä¿å­˜çš„æ•°é‡
num_actions = exp.get('num_actions', ...)
```

---

### **Bug #2: Value Lossçˆ†ç‚¸** ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥

**ä½ç½®ï¼š** `envs/v4_1/city_env.py`

**é—®é¢˜ï¼š**
```python
# Budgetæƒ©ç½šå¯¼è‡´rewardæ³¢åŠ¨å·¨å¤§ï¼š
æ­£å¸¸: reward = +775, scaled = +7.75
è´Ÿå€º-2000: reward = -225, scaled = -2.25
è´Ÿå€º-5000: reward = -2080, scaled = -20.8  â† çˆ†ç‚¸ï¼

# Valueç½‘ç»œæ— æ³•é¢„æµ‹è¿™ç§æ³¢åŠ¨
# value_loss = 3203ï¼ˆæ­£å¸¸åº”è¯¥<100ï¼‰
```

**ä¿®å¤ï¼š**
```python
# 1. å¢å¼ºç¼©æ”¾
scaled_reward = total_reward / 200.0  # ä»100æ”¹ä¸º200

# 2. æ·»åŠ clipping
scaled_reward = np.clip(scaled_reward, -10.0, 10.0)

# 3. é™ä½Budgetæƒ©ç½šç³»æ•°
debt_penalty_coef: 0.5 â†’ 0.3
```

---

### **Bug #3: KLæ•£åº¦ä¸ºè´Ÿ** ğŸ”¥

**ä½ç½®ï¼š** `trainers/v4_1/ppo_trainer.py`

**é—®é¢˜ï¼š**
```python
# å½“å‰å…¬å¼ï¼š
kl_div = (old_log_probs - current_log_probs).mean()

# è¿™ä¸æ˜¯çœŸæ­£çš„KLæ•£åº¦ï¼Œåªæ˜¯ç®€åŒ–è¿‘ä¼¼
# å½“new_prob > old_probæ—¶ï¼Œä¼šå‡ºç°è´Ÿæ•°
```

**ä¿®å¤ï¼š**
```python
# ä½¿ç”¨æ­£ç¡®çš„KLè¿‘ä¼¼å…¬å¼ï¼š
# KL(old||new) â‰ˆ E[(ratio - 1) - log(ratio)]
kl_div = ((ratio - 1.0) - torch.log(ratio + 1e-8)).mean()

# è¿™ä¸ªå…¬å¼ä¿è¯KL â‰¥ 0
```

---

## âœ… **ä¿®å¤éªŒè¯**

### **æµ‹è¯•ç»“æœï¼š**
```
[PASS] num_actionsä¸€è‡´æ€§ä¿®å¤æˆåŠŸ
  - æ”¶é›†æ—¶: num_actions=7
  - æ›´æ–°æ—¶: num_actions=7
  - å®Œå…¨ä¸€è‡´ï¼

[PASS] RewardèŒƒå›´æ§åˆ¶
  - æœ€æç«¯æƒ…å†µ: -3.625
  - åœ¨[-10, +10]èŒƒå›´å†…

[PASS] KLæ•£åº¦ä¸ºæ­£æ•°
  - æ–°å…¬å¼è®¡ç®—: 0.0169
  - ç¬¦åˆç†è®ºé¢„æœŸ
```

---

## ğŸ“Š **é¢„æœŸæ”¹è¿›**

| æŒ‡æ ‡ | ä¿®å¤å‰ | ä¿®å¤åï¼ˆé¢„æœŸï¼‰ | æ”¹å–„ |
|-----|--------|--------------|------|
| **value_loss** | 3203.81 | <500 | -85%+ |
| **clip_fraction** | 0.9936 (99%) | <0.3 (30%) | -70%+ |
| **KLæ•£åº¦** | -1.88 (è´Ÿæ•°) | >0 (æ­£æ•°) | âœ“ |
| **è®­ç»ƒç¨³å®šæ€§** | å·® | å¥½ | âœ“âœ“âœ“ |

---

## ğŸ”§ **ä¿®æ”¹çš„æ–‡ä»¶**

### **1. trainers/v4_1/ppo_trainer.py**

**ä¿®æ”¹1:** å‡½æ•°ç­¾åï¼ˆç¬¬179è¡Œï¼‰
```python
- def _get_action_log_prob(self, sequence, state):
+ def _get_action_log_prob(self, sequence, state, num_actions):
```

**ä¿®æ”¹2:** ç§»é™¤ç¡¬ç¼–ç ï¼ˆç¬¬204è¡Œï¼‰
```python
- num_actions = min(5, self.selector.max_actions)
+ num_actions = min(num_actions, self.selector.max_actions)
```

**ä¿®æ”¹3:** è°ƒç”¨å¤„ä¼ å…¥çœŸå®æ•°é‡ï¼ˆç¬¬149è¡Œï¼‰
```python
- old_log_prob = self._get_action_log_prob(selected_sequence, state)
+ old_log_prob = self._get_action_log_prob(selected_sequence, state, len(actions))
```

**ä¿®æ”¹4:** ä¿å­˜num_actionsï¼ˆç¬¬158è¡Œï¼‰
```python
experience = {
    ...
+   'num_actions': len(actions)
}
```

**ä¿®æ”¹5:** æ›´æ–°æ—¶ä½¿ç”¨ä¿å­˜çš„å€¼ï¼ˆç¬¬352è¡Œï¼‰
```python
- num_actions = min(len(exp.get('available_actions', [])), ...)
+ num_actions = exp.get('num_actions', len(exp.get('available_actions', [])))
```

**ä¿®æ”¹6:** KLæ•£åº¦è®¡ç®—ï¼ˆç¬¬419è¡Œï¼‰
```python
- kl_div = (old_log_probs - current_log_probs).mean()
+ kl_div = ((ratio - 1.0) - torch.log(ratio + 1e-8)).mean()
```

**ä¿®æ”¹7:** è®¾å¤‡ä¸€è‡´æ€§ï¼ˆç¬¬210è¡Œï¼‰
```python
- log_prob = dist.log_prob(torch.tensor(valid_action_idx))
+ log_prob = dist.log_prob(torch.tensor(valid_action_idx).to(self.device))
```

### **2. envs/v4_1/city_env.py**

**ä¿®æ”¹1:** Reward clippingï¼ˆç¬¬415è¡Œï¼‰
```python
scaled_reward = total_reward / 200.0
+ scaled_reward = np.clip(scaled_reward, -10.0, 10.0)
```

### **3. configs/city_config_v4_1.json**

**ä¿®æ”¹1:** é™ä½å­¦ä¹ ç‡
```python
- "lr": 1e-3
+ "lr": 3e-4
```

**ä¿®æ”¹2:** é™ä½Budgetæƒ©ç½š
```python
- "debt_penalty_coef": 0.5
+ "debt_penalty_coef": 0.3
```

---

## ğŸš€ **ä¸‹ä¸€æ­¥ï¼šé‡æ–°è®­ç»ƒ**

### **æ¸…ç†æ—§æ¨¡å‹ï¼š**
```bash
rm -rf models/v4_1_rl/*.pth
rm -rf models/v4_1_rl/training_results_*.json
```

### **å¯åŠ¨æ–°è®­ç»ƒï¼š**
```bash
python enhanced_city_simulation_v4_1.py --mode rl
```

### **è§‚å¯ŸæŒ‡æ ‡ï¼š**
```
ç¬¬1ä¸ªupdateåº”è¯¥çœ‹åˆ°:
  âœ“ value_loss < 500 (è€Œä¸æ˜¯3203)
  âœ“ clip_fraction < 0.5 (è€Œä¸æ˜¯0.99)
  âœ“ KL > 0 (è€Œä¸æ˜¯è´Ÿæ•°)

å¦‚æœçœ‹åˆ°è¿™äº›æ”¹è¿› â†’ bugä¿®å¤æˆåŠŸï¼
```

---

## ğŸ“‹ **ä¿®å¤æ€»ç»“**

âœ… **num_actionsä¸€è‡´æ€§** - æ”¶é›†å’Œæ›´æ–°ä½¿ç”¨ç›¸åŒæ•°é‡  
âœ… **Reward clipping** - é˜²æ­¢æç«¯å€¼  
âœ… **KLæ•£åº¦ä¿®æ­£** - ä½¿ç”¨æ­£ç¡®å…¬å¼  
âœ… **è®¾å¤‡ä¸€è‡´æ€§** - tensoræ”¾åœ¨æ­£ç¡®çš„device  
âœ… **æµ‹è¯•éªŒè¯** - æ‰€æœ‰æµ‹è¯•é€šè¿‡  

**ä¿®å¤å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼** ğŸ‰

---

**ä¿®å¤è€…ï¼š** AI Assistant  
**æµ‹è¯•è€…ï¼š** Fenglin  
**æœ€åæ›´æ–°ï¼š** 2025-10-09




