#!/usr/bin/env python3
"""
‰ΩøÁî®Áõ¥Êé•MeshÂ§ÑÁêÜÁªìÊûúÁöÑËÆ≠ÁªÉËÑöÊú¨
"""

import numpy as np
import json
import os
import torch
import torch.nn as nn
import torch.optim as optim
from envs.terrain_grid_nav_env import TerrainGridNavEnv
from agents.ppo_terrain_agent import TerrainPPOAgent
import matplotlib.pyplot as plt

# ËÆæÁΩÆ‰∏≠ÊñáÂ≠ó‰Ωì
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class DirectMeshTrainer:
    """Áõ¥Êé•MeshÂú∞ÂΩ¢ËÆ≠ÁªÉÂô®"""
    
    def __init__(self, terrain_file: str = "data/terrain/terrain_direct_mesh_fixed.json"):
        self.terrain_file = terrain_file
        self.terrain_data = None
        self.env = None
        self.agent = None
        self.training_stats = {
            'episode_rewards': [],
            'success_rates': [],
            'episode_lengths': [],
            'total_episodes': 0,
            'total_success': 0,
            'final_success_rate': 0.0,
            'final_avg_reward': 0.0,
            'final_avg_length': 0.0,
            'start_point': None,
            'goal_point': None,
            'terrain_file': terrain_file
        }
        
    def load_terrain_data(self):
        """Âä†ËΩΩÂú∞ÂΩ¢Êï∞ÊçÆ"""
        if not os.path.exists(self.terrain_file):
            print(f"‚ùå Âú∞ÂΩ¢Êñá‰ª∂‰∏çÂ≠òÂú®: {self.terrain_file}")
            return False
        
        with open(self.terrain_file, 'r') as f:
            self.terrain_data = json.load(f)
        
        print(f"‚úÖ ÊàêÂäüÂä†ËΩΩÂú∞ÂΩ¢Êï∞ÊçÆ")
        print(f"   ÁΩëÊ†ºÂ∞∫ÂØ∏: {self.terrain_data['grid_size']}")
        print(f"   ÊúâÊïàÁÇπÊï∞: {self.terrain_data['valid_points_count']}")
        print(f"   Ë¶ÜÁõñÁéá: {self.terrain_data['coverage_percentage']:.1f}%")
        
        return True
    
    def find_land_points(self, height_threshold: float = 0.0) -> tuple:
        """Âú®ÈôÜÂú∞‰∏äÊâæÂà∞ÂêàÈÄÇÁöÑËµ∑ÂßãÁÇπÂíåÁªàÁÇπ"""
        height_map = np.array(self.terrain_data['height_map'])
        mask = np.array(self.terrain_data['mask'])
        
        # ÊâæÂà∞ÊâÄÊúâÊúâÊïàÁöÑÈôÜÂú∞ÁÇπ
        valid_indices = np.where((mask) & (height_map > height_threshold))
        
        if len(valid_indices[0]) < 2:
            print("‚ùå Ê≤°ÊúâË∂≥Â§üÁöÑÈôÜÂú∞ÁÇπ")
            return None, None
        
        # ÈöèÊú∫ÈÄâÊã©‰∏§‰∏™‰∏çÂêåÁöÑÁÇπ
        indices = np.random.choice(len(valid_indices[0]), 2, replace=False)
        
        start_idx = (valid_indices[0][indices[0]], valid_indices[1][indices[0]])
        goal_idx = (valid_indices[0][indices[1]], valid_indices[1][indices[1]])
        
        start_height = height_map[start_idx]
        goal_height = height_map[goal_idx]
        
        print(f"‚úÖ ÊâæÂà∞Ëµ∑ÂßãÁÇπÂíåÁªàÁÇπ")
        print(f"   Ëµ∑ÂßãÁÇπ: {start_idx}, È´òÁ®ã: {start_height:.2f}")
        print(f"   ÁªàÁÇπ: {goal_idx}, È´òÁ®ã: {goal_height:.2f}")
        
        return start_idx, goal_idx
    
    def create_environment(self, start_point: tuple, goal_point: tuple):
        """ÂàõÂª∫ÁéØÂ¢É"""
        height_map = np.array(self.terrain_data['height_map'])
        mask = np.array(self.terrain_data['mask'])
        
        # ÂàõÂª∫Ëá™ÂÆö‰πâÂú∞ÂΩ¢Êï∞ÊçÆ - Âè™‰º†ÈÄíÈ´òÁ®ãÂõæ
        custom_terrain = height_map
        
        grid_size = self.terrain_data['grid_size']
        self.env = TerrainGridNavEnv(
            H=grid_size[0],
            W=grid_size[1],
            max_steps=400,
            custom_terrain=custom_terrain,
            fixed_start=start_point,
            fixed_goal=goal_point,
            slope_penalty_weight=0.0,  # ÊöÇÊó∂ÁßªÈô§Âú∞ÂΩ¢ÊÉ©ÁΩö
            height_penalty_weight=0.0
        )
        
        print(f"‚úÖ ÁéØÂ¢ÉÂàõÂª∫ÊàêÂäü")
        print(f"   ÁΩëÊ†ºÂ∞∫ÂØ∏: {grid_size}")
        print(f"   ÊúÄÂ§ßÊ≠•Êï∞: {self.env.max_steps}")
    
    def create_agent(self):
        """ÂàõÂª∫Êô∫ËÉΩ‰Ωì"""
        # ËÆ°ÁÆóÁä∂ÊÄÅÁª¥Â∫¶ÔºàÂü∫Á°ÄÁä∂ÊÄÅÁâπÂæÅÔºâ
        # position(2) + goal(2) + distance_to_goal(1) + current_height(1) + 
        # goal_height(1) + height_difference(1) + current_slope(1) + action_mask(4) = 13
        state_dim = 13
        action_dim = self.env.action_space.n
        
        self.agent = TerrainPPOAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=128,
            lr=1e-3,
            gamma=0.99,
            gae_lambda=0.95,
            clip_ratio=0.2,
            target_kl=0.01,
            train_pi_iters=80,
            train_v_iters=80,
            lam=0.97,
            max_grad_norm=0.5
        )
        
        print(f"‚úÖ Êô∫ËÉΩ‰ΩìÂàõÂª∫ÊàêÂäü")
        print(f"   Áä∂ÊÄÅÁª¥Â∫¶: {state_dim}")
        print(f"   Âä®‰ΩúÁª¥Â∫¶: {action_dim}")
    
    def train(self, num_episodes: int = 1000, eval_interval: int = 50):
        """ËÆ≠ÁªÉÊô∫ËÉΩ‰Ωì"""
        print(f"üöÄ ÂºÄÂßãËÆ≠ÁªÉÔºåÊÄªepisodes: {num_episodes}")
        print("=" * 50)
        
        for episode in range(1, num_episodes + 1):
            # Êî∂ÈõÜ‰∏Ä‰∏™episodeÁöÑÊï∞ÊçÆ
            states, actions, rewards, values, log_probs, dones, path, success = self.agent.collect_episode(self.env)
            
            # Êõ¥Êñ∞Êô∫ËÉΩ‰Ωì
            if len(states) > 0:
                self.agent.update(states, actions, rewards, values, log_probs, dones)
            
            # ËÆ∞ÂΩïÁªüËÆ°‰ø°ÊÅØ
            episode_reward = rewards.sum().item()
            episode_length = len(rewards)
            episode_success = success
            
            self.training_stats['episode_rewards'].append(episode_reward)
            self.training_stats['episode_lengths'].append(episode_length)
            self.training_stats['total_episodes'] += 1
            
            if episode_success:
                self.training_stats['total_success'] += 1
            
            # ËÆ°ÁÆóÂΩìÂâçÊàêÂäüÁéá
            current_success_rate = self.training_stats['total_success'] / self.training_stats['total_episodes']
            self.training_stats['success_rates'].append(current_success_rate)
            
            # ÂÆöÊúüËæìÂá∫ËÆ≠ÁªÉÁä∂ÊÄÅ
            if episode % eval_interval == 0:
                recent_rewards = self.training_stats['episode_rewards'][-eval_interval:]
                recent_lengths = self.training_stats['episode_lengths'][-eval_interval:]
                recent_successes = sum(1 for r in recent_rewards if r > 0)
                
                avg_reward = np.mean(recent_rewards)
                avg_length = np.mean(recent_lengths)
                recent_success_rate = recent_successes / eval_interval
                
                print(f"Episode {episode:4d} | "
                      f"ÊàêÂäüÁéá: {current_success_rate:.1%} | "
                      f"ÊúÄËøëÊàêÂäüÁéá: {recent_success_rate:.1%} | "
                      f"Âπ≥ÂùáÂ•ñÂä±: {avg_reward:6.1f} | "
                      f"Âπ≥ÂùáÈïøÂ∫¶: {avg_length:4.1f}")
        
        # ËÆ°ÁÆóÊúÄÁªàÁªüËÆ°
        self.training_stats['final_success_rate'] = current_success_rate
        self.training_stats['final_avg_reward'] = np.mean(self.training_stats['episode_rewards'])
        self.training_stats['final_avg_length'] = np.mean(self.training_stats['episode_lengths'])
        
        print("\n‚úÖ ËÆ≠ÁªÉÂÆåÊàê!")
        print(f"   ÊÄªepisodes: {self.training_stats['total_episodes']}")
        print(f"   ÊàêÂäüÊ¨°Êï∞: {self.training_stats['total_success']}")
        print(f"   ÊúÄÁªàÊàêÂäüÁéá: {self.training_stats['final_success_rate']:.1%}")
        print(f"   Âπ≥ÂùáÂ•ñÂä±: {self.training_stats['final_avg_reward']:.2f}")
        print(f"   Âπ≥ÂùáË∑ØÂæÑÈïøÂ∫¶: {self.training_stats['final_avg_length']:.1f}")
    
    def save_training_data(self, output_file: str):
        """‰øùÂ≠òËÆ≠ÁªÉÊï∞ÊçÆ"""
        def convert_numpy_types(obj):
            """ÈÄíÂΩíËΩ¨Êç¢numpyÁ±ªÂûã‰∏∫PythonÂéüÁîüÁ±ªÂûã"""
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            elif isinstance(obj, tuple):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj
        
        # ËΩ¨Êç¢ÊâÄÊúânumpyÁ±ªÂûã
        training_data = convert_numpy_types(self.training_stats)
        
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w') as f:
            json.dump(training_data, f, indent=2)
        
        print(f"‚úÖ ËÆ≠ÁªÉÊï∞ÊçÆÂ∑≤‰øùÂ≠òÂà∞: {output_file}")
    
    def visualize_training(self, save_path: str = None):
        """ÂèØËßÜÂåñËÆ≠ÁªÉÁªìÊûú"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Áõ¥Êé•MeshÂú∞ÂΩ¢ËÆ≠ÁªÉÁªìÊûú', fontsize=16)
        
        episodes = list(range(1, len(self.training_stats['episode_rewards']) + 1))
        
        # 1. ÊàêÂäüÁéáÂèòÂåñ
        axes[0, 0].plot(episodes, self.training_stats['success_rates'], 'b-', linewidth=2)
        axes[0, 0].set_title('ÊàêÂäüÁéáÂèòÂåñ')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('ÊàêÂäüÁéá')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_ylim(0, 1)
        
        # 2. Â•ñÂä±ÂèòÂåñ
        window_size = min(50, len(self.training_stats['episode_rewards']) // 10)
        if window_size > 1:
            moving_avg = np.convolve(self.training_stats['episode_rewards'], 
                                   np.ones(window_size)/window_size, mode='valid')
            moving_avg_episodes = episodes[window_size-1:]
            axes[0, 1].plot(moving_avg_episodes, moving_avg, 'r-', linewidth=2, 
                           label=f'ÁßªÂä®Âπ≥Âùá({window_size})')
        
        axes[0, 1].plot(episodes, self.training_stats['episode_rewards'], 'gray', alpha=0.3, linewidth=0.5)
        axes[0, 1].set_title('Â•ñÂä±ÂèòÂåñ')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('ÊÄªÂ•ñÂä±')
        axes[0, 1].grid(True, alpha=0.3)
        if window_size > 1:
            axes[0, 1].legend()
        
        # 3. Ë∑ØÂæÑÈïøÂ∫¶ÂèòÂåñ
        if window_size > 1:
            moving_avg_length = np.convolve(self.training_stats['episode_lengths'], 
                                          np.ones(window_size)/window_size, mode='valid')
            axes[1, 0].plot(moving_avg_episodes, moving_avg_length, 'g-', linewidth=2, 
                           label=f'ÁßªÂä®Âπ≥Âùá({window_size})')
        
        axes[1, 0].plot(episodes, self.training_stats['episode_lengths'], 'gray', alpha=0.3, linewidth=0.5)
        axes[1, 0].set_title('Ë∑ØÂæÑÈïøÂ∫¶ÂèòÂåñ')
        axes[1, 0].set_xlabel('Episode')
        axes[1, 0].set_ylabel('Ë∑ØÂæÑÈïøÂ∫¶')
        axes[1, 0].grid(True, alpha=0.3)
        if window_size > 1:
            axes[1, 0].legend()
        
        # 4. Â•ñÂä±ÂàÜÂ∏É
        axes[1, 1].hist(self.training_stats['episode_rewards'], bins=30, alpha=0.7, 
                       color='skyblue', edgecolor='black')
        axes[1, 1].axvline(np.mean(self.training_stats['episode_rewards']), color='red', 
                          linestyle='--', linewidth=2, 
                          label=f'Âπ≥ÂùáÂÄº: {np.mean(self.training_stats["episode_rewards"]):.2f}')
        axes[1, 1].set_title('Â•ñÂä±ÂàÜÂ∏É')
        axes[1, 1].set_xlabel('ÊÄªÂ•ñÂä±')
        axes[1, 1].set_ylabel('È¢ëÊ¨°')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ËÆ≠ÁªÉÁªìÊûúÂõæÂ∑≤‰øùÂ≠òÂà∞: {save_path}")
        
        plt.show()


def main():
    """‰∏ªÂáΩÊï∞"""
    # ÂàõÂª∫ËÆ≠ÁªÉÂô®
    trainer = DirectMeshTrainer()
    
    # Âä†ËΩΩÂú∞ÂΩ¢Êï∞ÊçÆ
    if not trainer.load_terrain_data():
        return
    
    # ÊâæÂà∞Ëµ∑ÂßãÁÇπÂíåÁªàÁÇπ
    start_point, goal_point = trainer.find_land_points(height_threshold=0.0)
    if start_point is None or goal_point is None:
        return
    
    # ‰øùÂ≠òËµ∑ÂßãÁÇπÂíåÁªàÁÇπ
    trainer.training_stats['start_point'] = list(start_point)
    trainer.training_stats['goal_point'] = list(goal_point)
    
    # ÂàõÂª∫ÁéØÂ¢É
    trainer.create_environment(start_point, goal_point)
    
    # ÂàõÂª∫Êô∫ËÉΩ‰Ωì
    trainer.create_agent()
    
    # ÂºÄÂßãËÆ≠ÁªÉ
    trainer.train(num_episodes=1000, eval_interval=50)
    
    # ‰øùÂ≠òËÆ≠ÁªÉÊï∞ÊçÆ
    trainer.save_training_data("training_data/direct_mesh_training_stats.json")
    
    # ÂèØËßÜÂåñËÆ≠ÁªÉÁªìÊûú
    trainer.visualize_training("visualization_output/direct_mesh_training_results.png")


if __name__ == "__main__":
    main()
