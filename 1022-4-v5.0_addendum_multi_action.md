
# v5.0 增补文档（Addendum）— 点×动作类型 + 一步多动作（兼容方案）

**生成时间**：2025-10-22  
**版本**：v5.0-addendum-1  
**适用范围**：在 **保持 v5.0 主干不变** 的前提下，引入“点×动作类型（point×type）”与“同一 step 内 1～5 个动作”的采样与执行能力。

> 本增补为**向后兼容**改动：不开启则走 v5.0 原路径；开启后仅影响“动作选择/执行”的局部逻辑，PPO 主流程保持不变。

---

## 0. 开关与配置

```json
"multi_action": {
  "enabled": true,                  // 关闭即回到 v5.0 行为
  "max_actions_per_step": 5,        // 同一 step 内最多 1~5 个
  "mode": "two_stage",              // two_stage | flat（推荐 two_stage）
  "candidate_topP": 128,            // 点候选裁剪上限（性能优化）
  "dup_policy": "no_repeat_point",  // 去重策略：点不重复/类型不重复/both
  "stop_bias": 0.0,                 // STOP 先验偏置（可为正以鼓励早停）
  "penalty_k": 0.0                  // 每多选一个的轻惩罚，稳定数量
}
```

---

## 1. 设计目标与兼容性

- **动作语义升级**：原子动作从 `int action_id` 扩展为 `(point_id, action_type)`；
- **一步多动作**：同一环境 step 内可自回归选择 **1～5** 个互不重复的 `(point, type)`；
- **PPO 兼容**：把“联合动作”的对数概率记为**子选择对数概率之和**，优势/回报仍按“宏步”计算；
- **完全可回滚**：`multi_action.enabled=false` 时使用 v5.0 既有实现（单一离散 ID + Sequence）。

---

## 2. 数据结构最小改动

### 2.1 原子动作（新增）
```python
@dataclass
class AtomicAction:
    point: int   # point_id（候选点索引/ID）
    atype: int   # action_type（在该点可用类型的局部/全局索引）
```

### 2.2 Sequence（扩展，向后兼容）
```python
@dataclass
class Sequence:
    agent: str
    actions: List[AtomicAction]  # 由原 int 扩展为 AtomicAction
    meta: Dict[str, Any] = field(default_factory=dict)
```

> 兼容层：若读到旧的 `int action_id`，在 selector 内部通过候选快照映射为默认 `(point, type)`。

---

## 3. 候选与两层掩码

### 3.1 候选索引（新增辅助）
```python
class CandidateIndex:
    points: List[int]                # 当前可用点列表，长度 P
    types_per_point: List[List[int]] # 每个点可用的类型列表，长度 U(p)
```

### 3.2 掩码
- **点级 mask**：`point_mask: Tensor[P]`（0/1），控制“可被选的点”；
- **类型级 mask**：`type_masks[p]: Tensor[U(p)]`（0/1），控制“在点 p 上可选的类型”；
- 每选定一个 `(p,u)` 后，**即时**更新预算/槽位/互斥，刷新两层 mask，避免重复与非法组合。

---

## 4. 策略网络（共享编码器 + 三个 head）

保持 v5.0 编码器与价值头不变，仅在 **actor** 侧增加：

```python
class Policy(nn.Module):
    def __init__(self, encoder, hid_dim, P_max, U_max, point_embed_dim=16):
        super().__init__()
        self.encoder = encoder
        self.point_head = nn.Linear(hid_dim, P_max)     # 选点
        self.type_head  = nn.Linear(hid_dim + point_embed_dim, U_max)  # 选类型
        self.stop_head  = nn.Linear(hid_dim, 1)         # STOP
        self.point_embed = nn.Embedding(P_max, point_embed_dim)

    def forward_point(self, feat):            # 返回 [B, P_max]
        return self.point_head(feat)

    def forward_type(self, feat, point_idx):  # 返回 [B, U_max]
        pe = self.point_embed(point_idx)      # [B, E]
        return self.type_head(torch.cat([feat, pe], dim=-1))

    def forward_stop(self, feat):             # 返回 [B, 1]
        return self.stop_head(feat)
```

> **性能要点**：编码器每步只前向一次；点/类型/STOP 仅为小头，显著降低多子步开销。

---

## 5. 采样逻辑（自回归 + STOP）

```python
def sample_action_set(policy, obs, cand_idx, point_mask, type_masks, max_k=5):
    feat = encoder(obs)  # 一次
    selected, logprobs, entropies = [], [], []

    for _ in range(max_k):
        # 选点
        p_logits = policy.forward_point(feat)                    # [P_max]
        p_logits = mask_logits(p_logits, point_mask, P=len(cand_idx.points))
        p_probs  = torch.softmax(p_logits, -1)

        # 合并 STOP（简单归一化示意）
        stop_logit = policy.forward_stop(feat).squeeze(-1)       # [1]
        stop_prob  = torch.sigmoid(stop_logit)
        probs_cat  = torch.cat([p_probs, stop_prob[None]], dim=0)
        probs_cat  = probs_cat / probs_cat.sum()

        idx = torch.multinomial(probs_cat, 1).item()
        if idx == len(p_probs):  # STOP
            break
        p_idx = idx

        # 在点 p_idx 上选类型
        t_logits = policy.forward_type(feat, torch.tensor([p_idx], device=feat.device))
        t_logits = mask_logits(t_logits, type_masks[p_idx], U=len(cand_idx.types_per_point[p_idx]))
        t_probs  = torch.softmax(t_logits, -1)
        t_idx    = torch.multinomial(t_probs, 1).item()

        # 记录联合概率项与熵
        logprobs += [torch.log(p_probs[p_idx] + 1e-8), torch.log(t_probs[t_idx] + 1e-8)]
        entropies += [-(p_probs * (p_probs+1e-8).log()).sum(), -(t_probs * (t_probs+1e-8).log()).sum()]

        selected.append(AtomicAction(point=p_idx, atype=t_idx))

        # 更新掩码（去重 + 约束联动）
        point_mask[p_idx] = 0
        update_masks_after_choice(p_idx, t_idx, point_mask, type_masks, cand_idx)

        if point_mask.sum() == 0:
            break

    logprob_sum = torch.stack(logprobs).sum() if logprobs else torch.tensor(0.0, device=feat.device)
    entropy_sum = torch.stack(entropies).sum() if entropies else torch.tensor(0.0, device=feat.device)
    return selected, logprob_sum, entropy_sum
```

> 注：也可把 STOP 并入点分布同一 softmax 的最后一维，实践中更稳定。

---

## 6. 环境执行（宏步内多手）

```python
def step(self, agent: str, sequence: Sequence):
    total_reward, done, info = 0.0, False, {}
    if sequence and sequence.actions:
        for aa in sequence.actions:
            r, terms, done = self._execute_point_type(agent, aa.point, aa.atype)
            total_reward += r
            info.update(terms)
            if done:
                break
    next_obs = self._get_obs()
    return next_obs, total_reward, done, info
```

- 每执行一个子动作即刻更新：**预算、槽位占用、互斥/依赖**；
- 奖励聚合：默认相加，必要时可引入非线性权重（例如边际递减/顺序依赖）。

---

## 7. PPO 侧改动（极小）

- **联合 logprob**：将子选择（点与类型）的 `logprob` 相加（必要时加上 STOP 的 logprob）；
- **比率**：`ratio = exp(logprob_sum_new - logprob_sum_old)`；
- **熵**：子步熵相加；
- **GAE/价值**：仍按“宏步”计算，不展开子步作为时间维；
- **Buffer 追加字段**：`action_set_t`（pad 到 6=最多 5+STOP）、`logprob_sum_t`、`entropy_sum_t`、`seq_mask_t`。

---

## 8. 迁移与最小 Diff

1. **Sequence**：`List[int] → List[AtomicAction]`（在 selector 内做兼容映射）；  
2. **Env**：新增 `_execute_point_type(point, atype)`，内部可通过候选快照映射回旧 `action_id`；  
3. **Selector/Policy**：加 `CandidateIndex` 与两层 mask 构建；用 `sample_action_set` 替换单次采样；  
4. **PPO**：`logprob → logprob_sum`；其余不变；  
5. **开关**：配置项 `multi_action.enabled` 控制新旧路径。

---

## 9. 测试清单

- **掩码正确性**：非法点/类型必为 -inf；重复点被禁用；  
- **序列合法性**：同一 step 内最多 K=5；dup_policy 生效；  
- **STOP 行为**：候选耗尽/预算不足可自然 STOP；  
- **性能基线**：记录单步时延；开启 `candidate_topP`、AMP 与 `torch.compile` 逐项优化；  
- **回滚一致性**：`enabled=false` 与 v5.0 指标对齐。

---

## 10. 性能与调参建议

- **共享编码器**、小头（point/type/stop）；  
- **候选裁剪（Top-P）** 限制点 softmax 尺寸；  
- **GPU 掩码原位操作**，避免 CPU-GPU 往返；  
- **轻微惩罚** 控制过多选择：`r' = r - penalty_k * max(0, k-1)`；  
- **课程式上限**：从 `max_actions_per_step=3` 起步稳定后增至 5。

---

**说明**：本增补不改变 v5.0 的总体训练/评估流程，仅增强动作表达与同拍多手能力。若后续需要一次前向选 K（Gumbel-TopK / Plackett-Luce），可在本方案之上替换“自回归采样”模块而不影响其余组件。
